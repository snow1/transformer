Training Progress: 	Epoch 1 [0/8883 (0.00%)]		Loss: 1.65029
Training Progress: 	Epoch 1 [320/8883 (3.60%)]		Loss: 1.68599
Training Progress: 	Epoch 1 [640/8883 (7.19%)]		Loss: 1.46664
Training Progress: 	Epoch 1 [960/8883 (10.79%)]		Loss: 1.56269
Training Progress: 	Epoch 1 [1280/8883 (14.39%)]		Loss: 1.50588
Training Progress: 	Epoch 1 [1600/8883 (17.99%)]		Loss: 1.67049
Training Progress: 	Epoch 1 [1920/8883 (21.58%)]		Loss: 1.44089
Training Progress: 	Epoch 1 [2240/8883 (25.18%)]		Loss: 1.57890
Training Progress: 	Epoch 1 [2560/8883 (28.78%)]		Loss: 1.49102
Training Progress: 	Epoch 1 [2880/8883 (32.37%)]		Loss: 1.59067
Training Progress: 	Epoch 1 [3200/8883 (35.97%)]		Loss: 1.52617
Training Progress: 	Epoch 1 [3520/8883 (39.57%)]		Loss: 1.43644
Training Progress: 	Epoch 1 [3840/8883 (43.17%)]		Loss: 1.48197
Training Progress: 	Epoch 1 [4160/8883 (46.76%)]		Loss: 1.60653
Training Progress: 	Epoch 1 [4480/8883 (50.36%)]		Loss: 1.43721
Training Progress: 	Epoch 1 [4800/8883 (53.96%)]		Loss: 1.42475
Training Progress: 	Epoch 1 [5120/8883 (57.55%)]		Loss: 1.32257
Training Progress: 	Epoch 1 [5440/8883 (61.15%)]		Loss: 1.41085
Training Progress: 	Epoch 1 [5760/8883 (64.75%)]		Loss: 1.31634
Training Progress: 	Epoch 1 [6080/8883 (68.35%)]		Loss: 1.66319
Training Progress: 	Epoch 1 [6400/8883 (71.94%)]		Loss: 1.54915
Training Progress: 	Epoch 1 [6720/8883 (75.54%)]		Loss: 1.43624
Training Progress: 	Epoch 1 [7040/8883 (79.14%)]		Loss: 1.38012
Training Progress: 	Epoch 1 [7360/8883 (82.73%)]		Loss: 1.37746
Training Progress: 	Epoch 1 [7680/8883 (86.33%)]		Loss: 1.39229
Training Progress: 	Epoch 1 [8000/8883 (89.93%)]		Loss: 1.24197
Training Progress: 	Epoch 1 [8320/8883 (93.53%)]		Loss: 1.25074
Training Progress: 	Epoch 1 [8640/8883 (97.12%)]		Loss: 1.28239
	Train loss: 0.03945, Accuracy: 3797/8883 (42.00%)
	Validation loss: 0.00073, Accuracy: 825/1692 (48.00%)
	Test loss: 0.00071, Accuracy: 770/1772 (43.00%)

Training Progress: 	Epoch 2 [0/8883 (0.00%)]		Loss: 1.29285
Training Progress: 	Epoch 2 [320/8883 (3.60%)]		Loss: 1.27398
Training Progress: 	Epoch 2 [640/8883 (7.19%)]		Loss: 1.36208
Training Progress: 	Epoch 2 [960/8883 (10.79%)]		Loss: 1.22837
Training Progress: 	Epoch 2 [1280/8883 (14.39%)]		Loss: 1.26533
Training Progress: 	Epoch 2 [1600/8883 (17.99%)]		Loss: 1.14450
Training Progress: 	Epoch 2 [1920/8883 (21.58%)]		Loss: 1.25836
Training Progress: 	Epoch 2 [2240/8883 (25.18%)]		Loss: 1.16746
Training Progress: 	Epoch 2 [2560/8883 (28.78%)]		Loss: 1.42504
Training Progress: 	Epoch 2 [2880/8883 (32.37%)]		Loss: 1.23225
Training Progress: 	Epoch 2 [3200/8883 (35.97%)]		Loss: 1.40114
Training Progress: 	Epoch 2 [3520/8883 (39.57%)]		Loss: 1.27325
Training Progress: 	Epoch 2 [3840/8883 (43.17%)]		Loss: 1.38411
Training Progress: 	Epoch 2 [4160/8883 (46.76%)]		Loss: 1.53744
Training Progress: 	Epoch 2 [4480/8883 (50.36%)]		Loss: 1.22818
Training Progress: 	Epoch 2 [4800/8883 (53.96%)]		Loss: 1.23718
Training Progress: 	Epoch 2 [5120/8883 (57.55%)]		Loss: 1.34197
Training Progress: 	Epoch 2 [5440/8883 (61.15%)]		Loss: 1.33467
Training Progress: 	Epoch 2 [5760/8883 (64.75%)]		Loss: 1.10748
Training Progress: 	Epoch 2 [6080/8883 (68.35%)]		Loss: 1.33007
Training Progress: 	Epoch 2 [6400/8883 (71.94%)]		Loss: 1.34182
Training Progress: 	Epoch 2 [6720/8883 (75.54%)]		Loss: 1.30479
Training Progress: 	Epoch 2 [7040/8883 (79.14%)]		Loss: 1.24327
Training Progress: 	Epoch 2 [7360/8883 (82.73%)]		Loss: 1.43526
Training Progress: 	Epoch 2 [7680/8883 (86.33%)]		Loss: 1.20421
Training Progress: 	Epoch 2 [8000/8883 (89.93%)]		Loss: 1.26216
Training Progress: 	Epoch 2 [8320/8883 (93.53%)]		Loss: 1.05700
Training Progress: 	Epoch 2 [8640/8883 (97.12%)]		Loss: 1.31543
	Train loss: 0.03684, Accuracy: 4132/8883 (46.00%)
	Validation loss: 0.00068, Accuracy: 873/1692 (51.00%)
	Test loss: 0.00066, Accuracy: 825/1772 (46.00%)

Training Progress: 	Epoch 3 [0/8883 (0.00%)]		Loss: 1.20024
Training Progress: 	Epoch 3 [320/8883 (3.60%)]		Loss: 1.23043
Training Progress: 	Epoch 3 [640/8883 (7.19%)]		Loss: 1.20684
Training Progress: 	Epoch 3 [960/8883 (10.79%)]		Loss: 1.15843
Training Progress: 	Epoch 3 [1280/8883 (14.39%)]		Loss: 1.15203
Training Progress: 	Epoch 3 [1600/8883 (17.99%)]		Loss: 1.23057
Training Progress: 	Epoch 3 [1920/8883 (21.58%)]		Loss: 1.04047
Training Progress: 	Epoch 3 [2240/8883 (25.18%)]		Loss: 1.18495
Training Progress: 	Epoch 3 [2560/8883 (28.78%)]		Loss: 1.33986
Training Progress: 	Epoch 3 [2880/8883 (32.37%)]		Loss: 1.26532
Training Progress: 	Epoch 3 [3200/8883 (35.97%)]		Loss: 1.30697
Training Progress: 	Epoch 3 [3520/8883 (39.57%)]		Loss: 1.26743
Training Progress: 	Epoch 3 [3840/8883 (43.17%)]		Loss: 1.35495
Training Progress: 	Epoch 3 [4160/8883 (46.76%)]		Loss: 1.34847
Training Progress: 	Epoch 3 [4480/8883 (50.36%)]		Loss: 1.14874
Training Progress: 	Epoch 3 [4800/8883 (53.96%)]		Loss: 1.28225
Training Progress: 	Epoch 3 [5120/8883 (57.55%)]		Loss: 1.25997
Training Progress: 	Epoch 3 [5440/8883 (61.15%)]		Loss: 1.31254
Training Progress: 	Epoch 3 [5760/8883 (64.75%)]		Loss: 1.13482
Training Progress: 	Epoch 3 [6080/8883 (68.35%)]		Loss: 1.22735
Training Progress: 	Epoch 3 [6400/8883 (71.94%)]		Loss: 1.10357
Training Progress: 	Epoch 3 [6720/8883 (75.54%)]		Loss: 1.28012
Training Progress: 	Epoch 3 [7040/8883 (79.14%)]		Loss: 1.19300
Training Progress: 	Epoch 3 [7360/8883 (82.73%)]		Loss: 1.31598
Training Progress: 	Epoch 3 [7680/8883 (86.33%)]		Loss: 1.09879
Training Progress: 	Epoch 3 [8000/8883 (89.93%)]		Loss: 1.32168
Training Progress: 	Epoch 3 [8320/8883 (93.53%)]		Loss: 1.09036
Training Progress: 	Epoch 3 [8640/8883 (97.12%)]		Loss: 1.22051
	Train loss: 0.03533, Accuracy: 4442/8883 (50.00%)
	Validation loss: 0.00063, Accuracy: 947/1692 (55.00%)
	Test loss: 0.00064, Accuracy: 874/1772 (49.00%)

Training Progress: 	Epoch 4 [0/8883 (0.00%)]		Loss: 1.24132
Training Progress: 	Epoch 4 [320/8883 (3.60%)]		Loss: 1.15709
Training Progress: 	Epoch 4 [640/8883 (7.19%)]		Loss: 1.13303
Training Progress: 	Epoch 4 [960/8883 (10.79%)]		Loss: 1.17214
Training Progress: 	Epoch 4 [1280/8883 (14.39%)]		Loss: 1.19746
Training Progress: 	Epoch 4 [1600/8883 (17.99%)]		Loss: 1.07572
Training Progress: 	Epoch 4 [1920/8883 (21.58%)]		Loss: 1.10195
Training Progress: 	Epoch 4 [2240/8883 (25.18%)]		Loss: 1.01024
Training Progress: 	Epoch 4 [2560/8883 (28.78%)]		Loss: 1.29132
Training Progress: 	Epoch 4 [2880/8883 (32.37%)]		Loss: 1.21148
Training Progress: 	Epoch 4 [3200/8883 (35.97%)]		Loss: 1.38416
Training Progress: 	Epoch 4 [3520/8883 (39.57%)]		Loss: 1.32873
Training Progress: 	Epoch 4 [3840/8883 (43.17%)]		Loss: 1.25840
Training Progress: 	Epoch 4 [4160/8883 (46.76%)]		Loss: 1.07635
Training Progress: 	Epoch 4 [4480/8883 (50.36%)]		Loss: 1.03552
Training Progress: 	Epoch 4 [4800/8883 (53.96%)]		Loss: 1.23394
Training Progress: 	Epoch 4 [5120/8883 (57.55%)]		Loss: 1.35385
Training Progress: 	Epoch 4 [5440/8883 (61.15%)]		Loss: 1.14962
Training Progress: 	Epoch 4 [5760/8883 (64.75%)]		Loss: 1.06472
Training Progress: 	Epoch 4 [6080/8883 (68.35%)]		Loss: 1.10785
Training Progress: 	Epoch 4 [6400/8883 (71.94%)]		Loss: 1.06250
Training Progress: 	Epoch 4 [6720/8883 (75.54%)]		Loss: 1.13591
Training Progress: 	Epoch 4 [7040/8883 (79.14%)]		Loss: 1.00499
Training Progress: 	Epoch 4 [7360/8883 (82.73%)]		Loss: 1.15068
Training Progress: 	Epoch 4 [7680/8883 (86.33%)]		Loss: 0.96870
Training Progress: 	Epoch 4 [8000/8883 (89.93%)]		Loss: 1.04665
Training Progress: 	Epoch 4 [8320/8883 (93.53%)]		Loss: 1.05191
Training Progress: 	Epoch 4 [8640/8883 (97.12%)]		Loss: 1.37728
	Train loss: 0.03348, Accuracy: 4754/8883 (53.00%)
	Validation loss: 0.00059, Accuracy: 1020/1692 (60.00%)
	Test loss: 0.00061, Accuracy: 931/1772 (52.00%)

Training Progress: 	Epoch 5 [0/8883 (0.00%)]		Loss: 1.05357
Training Progress: 	Epoch 5 [320/8883 (3.60%)]		Loss: 1.07102
Training Progress: 	Epoch 5 [640/8883 (7.19%)]		Loss: 1.02085
Training Progress: 	Epoch 5 [960/8883 (10.79%)]		Loss: 1.33584
Training Progress: 	Epoch 5 [1280/8883 (14.39%)]		Loss: 1.04826
Training Progress: 	Epoch 5 [1600/8883 (17.99%)]		Loss: 1.00970
Training Progress: 	Epoch 5 [1920/8883 (21.58%)]		Loss: 1.02223
Training Progress: 	Epoch 5 [2240/8883 (25.18%)]		Loss: 1.18928
Training Progress: 	Epoch 5 [2560/8883 (28.78%)]		Loss: 1.16394
Training Progress: 	Epoch 5 [2880/8883 (32.37%)]		Loss: 1.10341
Training Progress: 	Epoch 5 [3200/8883 (35.97%)]		Loss: 1.33387
Training Progress: 	Epoch 5 [3520/8883 (39.57%)]		Loss: 1.18658
Training Progress: 	Epoch 5 [3840/8883 (43.17%)]		Loss: 1.29155
Training Progress: 	Epoch 5 [4160/8883 (46.76%)]		Loss: 1.19504
Training Progress: 	Epoch 5 [4480/8883 (50.36%)]		Loss: 1.02439
Training Progress: 	Epoch 5 [4800/8883 (53.96%)]		Loss: 1.16354
Training Progress: 	Epoch 5 [5120/8883 (57.55%)]		Loss: 1.31042
Training Progress: 	Epoch 5 [5440/8883 (61.15%)]		Loss: 1.25251
Training Progress: 	Epoch 5 [5760/8883 (64.75%)]		Loss: 1.22860
Training Progress: 	Epoch 5 [6080/8883 (68.35%)]		Loss: 1.05840
Training Progress: 	Epoch 5 [6400/8883 (71.94%)]		Loss: 1.06054
Training Progress: 	Epoch 5 [6720/8883 (75.54%)]		Loss: 1.03263
Training Progress: 	Epoch 5 [7040/8883 (79.14%)]		Loss: 1.09937
Training Progress: 	Epoch 5 [7360/8883 (82.73%)]		Loss: 1.08119
Training Progress: 	Epoch 5 [7680/8883 (86.33%)]		Loss: 0.90160
Training Progress: 	Epoch 5 [8000/8883 (89.93%)]		Loss: 1.16731
Training Progress: 	Epoch 5 [8320/8883 (93.53%)]		Loss: 1.07722
Training Progress: 	Epoch 5 [8640/8883 (97.12%)]		Loss: 1.19078
	Train loss: 0.03285, Accuracy: 4744/8883 (53.00%)
	Validation loss: 0.00060, Accuracy: 993/1692 (58.00%)
	Test loss: 0.00062, Accuracy: 923/1772 (52.00%)

Training Progress: 	Epoch 6 [0/8883 (0.00%)]		Loss: 1.18072
Training Progress: 	Epoch 6 [320/8883 (3.60%)]		Loss: 1.06104
Training Progress: 	Epoch 6 [640/8883 (7.19%)]		Loss: 0.98414
Training Progress: 	Epoch 6 [960/8883 (10.79%)]		Loss: 1.18059
Training Progress: 	Epoch 6 [1280/8883 (14.39%)]		Loss: 1.20902
Training Progress: 	Epoch 6 [1600/8883 (17.99%)]		Loss: 1.12808
Training Progress: 	Epoch 6 [1920/8883 (21.58%)]		Loss: 1.12005
Training Progress: 	Epoch 6 [2240/8883 (25.18%)]		Loss: 1.14894
Training Progress: 	Epoch 6 [2560/8883 (28.78%)]		Loss: 0.95833
Training Progress: 	Epoch 6 [2880/8883 (32.37%)]		Loss: 1.05964
Training Progress: 	Epoch 6 [3200/8883 (35.97%)]		Loss: 1.32947
Training Progress: 	Epoch 6 [3520/8883 (39.57%)]		Loss: 1.30314
Training Progress: 	Epoch 6 [3840/8883 (43.17%)]		Loss: 1.01618
Training Progress: 	Epoch 6 [4160/8883 (46.76%)]		Loss: 1.26946
Training Progress: 	Epoch 6 [4480/8883 (50.36%)]		Loss: 0.93200
Training Progress: 	Epoch 6 [4800/8883 (53.96%)]		Loss: 1.31983
Training Progress: 	Epoch 6 [5120/8883 (57.55%)]		Loss: 1.25848
Training Progress: 	Epoch 6 [5440/8883 (61.15%)]		Loss: 1.26379
Training Progress: 	Epoch 6 [5760/8883 (64.75%)]		Loss: 1.07651
Training Progress: 	Epoch 6 [6080/8883 (68.35%)]		Loss: 1.04764
Training Progress: 	Epoch 6 [6400/8883 (71.94%)]		Loss: 0.98409
Training Progress: 	Epoch 6 [6720/8883 (75.54%)]		Loss: 1.15887
Training Progress: 	Epoch 6 [7040/8883 (79.14%)]		Loss: 1.19808
Training Progress: 	Epoch 6 [7360/8883 (82.73%)]		Loss: 1.11033
Training Progress: 	Epoch 6 [7680/8883 (86.33%)]		Loss: 0.83643
Training Progress: 	Epoch 6 [8000/8883 (89.93%)]		Loss: 1.06089
Training Progress: 	Epoch 6 [8320/8883 (93.53%)]		Loss: 0.93617
Training Progress: 	Epoch 6 [8640/8883 (97.12%)]		Loss: 1.12006
	Train loss: 0.03114, Accuracy: 5082/8883 (57.00%)
	Validation loss: 0.00055, Accuracy: 1082/1692 (63.00%)
	Test loss: 0.00059, Accuracy: 979/1772 (55.00%)

Training Progress: 	Epoch 7 [0/8883 (0.00%)]		Loss: 0.92691
Training Progress: 	Epoch 7 [320/8883 (3.60%)]		Loss: 0.97251
Training Progress: 	Epoch 7 [640/8883 (7.19%)]		Loss: 1.05227
Training Progress: 	Epoch 7 [960/8883 (10.79%)]		Loss: 1.19267
Training Progress: 	Epoch 7 [1280/8883 (14.39%)]		Loss: 1.07167
Training Progress: 	Epoch 7 [1600/8883 (17.99%)]		Loss: 1.05426
Training Progress: 	Epoch 7 [1920/8883 (21.58%)]		Loss: 0.87980
Training Progress: 	Epoch 7 [2240/8883 (25.18%)]		Loss: 1.02094
Training Progress: 	Epoch 7 [2560/8883 (28.78%)]		Loss: 1.11978
Training Progress: 	Epoch 7 [2880/8883 (32.37%)]		Loss: 1.14722
Training Progress: 	Epoch 7 [3200/8883 (35.97%)]		Loss: 1.27127
Training Progress: 	Epoch 7 [3520/8883 (39.57%)]		Loss: 1.28734
Training Progress: 	Epoch 7 [3840/8883 (43.17%)]		Loss: 1.35956
Training Progress: 	Epoch 7 [4160/8883 (46.76%)]		Loss: 1.16473
Training Progress: 	Epoch 7 [4480/8883 (50.36%)]		Loss: 0.98905
Training Progress: 	Epoch 7 [4800/8883 (53.96%)]		Loss: 0.96217
Training Progress: 	Epoch 7 [5120/8883 (57.55%)]		Loss: 1.19371
Training Progress: 	Epoch 7 [5440/8883 (61.15%)]		Loss: 1.07116
Training Progress: 	Epoch 7 [5760/8883 (64.75%)]		Loss: 0.96043
Training Progress: 	Epoch 7 [6080/8883 (68.35%)]		Loss: 1.01333
Training Progress: 	Epoch 7 [6400/8883 (71.94%)]		Loss: 1.06405
Training Progress: 	Epoch 7 [6720/8883 (75.54%)]		Loss: 1.06008
Training Progress: 	Epoch 7 [7040/8883 (79.14%)]		Loss: 1.22583
Training Progress: 	Epoch 7 [7360/8883 (82.73%)]		Loss: 1.09883
Training Progress: 	Epoch 7 [7680/8883 (86.33%)]		Loss: 0.95374
Training Progress: 	Epoch 7 [8000/8883 (89.93%)]		Loss: 1.07485
Training Progress: 	Epoch 7 [8320/8883 (93.53%)]		Loss: 1.11285
Training Progress: 	Epoch 7 [8640/8883 (97.12%)]		Loss: 1.02676
	Train loss: 0.03059, Accuracy: 5042/8883 (56.00%)
	Validation loss: 0.00054, Accuracy: 1093/1692 (64.00%)
	Test loss: 0.00059, Accuracy: 964/1772 (54.00%)

Training Progress: 	Epoch 8 [0/8883 (0.00%)]		Loss: 1.00187
Training Progress: 	Epoch 8 [320/8883 (3.60%)]		Loss: 0.98550
Training Progress: 	Epoch 8 [640/8883 (7.19%)]		Loss: 1.00607
Training Progress: 	Epoch 8 [960/8883 (10.79%)]		Loss: 1.17871
Training Progress: 	Epoch 8 [1280/8883 (14.39%)]		Loss: 0.98774
Training Progress: 	Epoch 8 [1600/8883 (17.99%)]		Loss: 0.88242
Training Progress: 	Epoch 8 [1920/8883 (21.58%)]		Loss: 1.00325
Training Progress: 	Epoch 8 [2240/8883 (25.18%)]		Loss: 0.82835
Training Progress: 	Epoch 8 [2560/8883 (28.78%)]		Loss: 1.05999
Training Progress: 	Epoch 8 [2880/8883 (32.37%)]		Loss: 1.03210
Training Progress: 	Epoch 8 [3200/8883 (35.97%)]		Loss: 1.05345
Training Progress: 	Epoch 8 [3520/8883 (39.57%)]		Loss: 1.32458
Training Progress: 	Epoch 8 [3840/8883 (43.17%)]		Loss: 1.02470
Training Progress: 	Epoch 8 [4160/8883 (46.76%)]		Loss: 1.12603
Training Progress: 	Epoch 8 [4480/8883 (50.36%)]		Loss: 0.84147
Training Progress: 	Epoch 8 [4800/8883 (53.96%)]		Loss: 1.08869
Training Progress: 	Epoch 8 [5120/8883 (57.55%)]		Loss: 1.17850
Training Progress: 	Epoch 8 [5440/8883 (61.15%)]		Loss: 1.26689
Training Progress: 	Epoch 8 [5760/8883 (64.75%)]		Loss: 1.09162
Training Progress: 	Epoch 8 [6080/8883 (68.35%)]		Loss: 0.96728
Training Progress: 	Epoch 8 [6400/8883 (71.94%)]		Loss: 1.07394
Training Progress: 	Epoch 8 [6720/8883 (75.54%)]		Loss: 1.15755
Training Progress: 	Epoch 8 [7040/8883 (79.14%)]		Loss: 0.99261
Training Progress: 	Epoch 8 [7360/8883 (82.73%)]		Loss: 1.02758
Training Progress: 	Epoch 8 [7680/8883 (86.33%)]		Loss: 0.99525
Training Progress: 	Epoch 8 [8000/8883 (89.93%)]		Loss: 1.12431
Training Progress: 	Epoch 8 [8320/8883 (93.53%)]		Loss: 1.13432
Training Progress: 	Epoch 8 [8640/8883 (97.12%)]		Loss: 1.25441
	Train loss: 0.03006, Accuracy: 5161/8883 (58.00%)
	Validation loss: 0.00053, Accuracy: 1094/1692 (64.00%)
	Test loss: 0.00059, Accuracy: 987/1772 (55.00%)

Training Progress: 	Epoch 9 [0/8883 (0.00%)]		Loss: 1.08398
Training Progress: 	Epoch 9 [320/8883 (3.60%)]		Loss: 0.85158
Training Progress: 	Epoch 9 [640/8883 (7.19%)]		Loss: 0.87309
Training Progress: 	Epoch 9 [960/8883 (10.79%)]		Loss: 1.08777
Training Progress: 	Epoch 9 [1280/8883 (14.39%)]		Loss: 1.09677
Training Progress: 	Epoch 9 [1600/8883 (17.99%)]		Loss: 0.84044
Training Progress: 	Epoch 9 [1920/8883 (21.58%)]		Loss: 1.01573
Training Progress: 	Epoch 9 [2240/8883 (25.18%)]		Loss: 1.05182
Training Progress: 	Epoch 9 [2560/8883 (28.78%)]		Loss: 1.06742
Training Progress: 	Epoch 9 [2880/8883 (32.37%)]		Loss: 0.98380
Training Progress: 	Epoch 9 [3200/8883 (35.97%)]		Loss: 0.97803
Training Progress: 	Epoch 9 [3520/8883 (39.57%)]		Loss: 1.28849
Training Progress: 	Epoch 9 [3840/8883 (43.17%)]		Loss: 1.15965
Training Progress: 	Epoch 9 [4160/8883 (46.76%)]		Loss: 1.09672
Training Progress: 	Epoch 9 [4480/8883 (50.36%)]		Loss: 1.05040
Training Progress: 	Epoch 9 [4800/8883 (53.96%)]		Loss: 1.24077
Training Progress: 	Epoch 9 [5120/8883 (57.55%)]		Loss: 1.11539
Training Progress: 	Epoch 9 [5440/8883 (61.15%)]		Loss: 1.04403
Training Progress: 	Epoch 9 [5760/8883 (64.75%)]		Loss: 0.99431
Training Progress: 	Epoch 9 [6080/8883 (68.35%)]		Loss: 0.78399
Training Progress: 	Epoch 9 [6400/8883 (71.94%)]		Loss: 1.02036
Training Progress: 	Epoch 9 [6720/8883 (75.54%)]		Loss: 1.18321
Training Progress: 	Epoch 9 [7040/8883 (79.14%)]		Loss: 1.05007
Training Progress: 	Epoch 9 [7360/8883 (82.73%)]		Loss: 0.95355
Training Progress: 	Epoch 9 [7680/8883 (86.33%)]		Loss: 0.87961
Training Progress: 	Epoch 9 [8000/8883 (89.93%)]		Loss: 1.11242
Training Progress: 	Epoch 9 [8320/8883 (93.53%)]		Loss: 0.93612
Training Progress: 	Epoch 9 [8640/8883 (97.12%)]		Loss: 1.11541
	Train loss: 0.02891, Accuracy: 5308/8883 (59.00%)
	Validation loss: 0.00050, Accuracy: 1140/1692 (67.00%)
	Test loss: 0.00057, Accuracy: 972/1772 (54.00%)

Training Progress: 	Epoch 10 [0/8883 (0.00%)]		Loss: 1.16415
Training Progress: 	Epoch 10 [320/8883 (3.60%)]		Loss: 1.01888
Training Progress: 	Epoch 10 [640/8883 (7.19%)]		Loss: 0.84423
Training Progress: 	Epoch 10 [960/8883 (10.79%)]		Loss: 1.22730
Training Progress: 	Epoch 10 [1280/8883 (14.39%)]		Loss: 1.14357
Training Progress: 	Epoch 10 [1600/8883 (17.99%)]		Loss: 0.98608
Training Progress: 	Epoch 10 [1920/8883 (21.58%)]		Loss: 0.91530
Training Progress: 	Epoch 10 [2240/8883 (25.18%)]		Loss: 0.95585
Training Progress: 	Epoch 10 [2560/8883 (28.78%)]		Loss: 1.04581
Training Progress: 	Epoch 10 [2880/8883 (32.37%)]		Loss: 1.16481
Training Progress: 	Epoch 10 [3200/8883 (35.97%)]		Loss: 1.10790
Training Progress: 	Epoch 10 [3520/8883 (39.57%)]		Loss: 1.23086
Training Progress: 	Epoch 10 [3840/8883 (43.17%)]		Loss: 0.92700
Training Progress: 	Epoch 10 [4160/8883 (46.76%)]		Loss: 1.01668
Training Progress: 	Epoch 10 [4480/8883 (50.36%)]		Loss: 1.00384
Training Progress: 	Epoch 10 [4800/8883 (53.96%)]		Loss: 1.25714
Training Progress: 	Epoch 10 [5120/8883 (57.55%)]		Loss: 1.03256
Training Progress: 	Epoch 10 [5440/8883 (61.15%)]		Loss: 1.18901
Training Progress: 	Epoch 10 [5760/8883 (64.75%)]		Loss: 0.89479
Training Progress: 	Epoch 10 [6080/8883 (68.35%)]		Loss: 0.96414
Training Progress: 	Epoch 10 [6400/8883 (71.94%)]		Loss: 0.92337
Training Progress: 	Epoch 10 [6720/8883 (75.54%)]		Loss: 1.10740
Training Progress: 	Epoch 10 [7040/8883 (79.14%)]		Loss: 1.01660
Training Progress: 	Epoch 10 [7360/8883 (82.73%)]		Loss: 0.92573
Training Progress: 	Epoch 10 [7680/8883 (86.33%)]		Loss: 0.88806
Training Progress: 	Epoch 10 [8000/8883 (89.93%)]		Loss: 1.05318
Training Progress: 	Epoch 10 [8320/8883 (93.53%)]		Loss: 0.96554
Training Progress: 	Epoch 10 [8640/8883 (97.12%)]		Loss: 1.01947
	Train loss: 0.02765, Accuracy: 5524/8883 (62.00%)
	Validation loss: 0.00047, Accuracy: 1209/1692 (71.00%)
	Test loss: 0.00055, Accuracy: 1055/1772 (59.00%)

Training Progress: 	Epoch 11 [0/8883 (0.00%)]		Loss: 1.16158
Training Progress: 	Epoch 11 [320/8883 (3.60%)]		Loss: 0.96112
Training Progress: 	Epoch 11 [640/8883 (7.19%)]		Loss: 0.92060
Training Progress: 	Epoch 11 [960/8883 (10.79%)]		Loss: 1.05057
Training Progress: 	Epoch 11 [1280/8883 (14.39%)]		Loss: 1.09649
Training Progress: 	Epoch 11 [1600/8883 (17.99%)]		Loss: 0.89756
Training Progress: 	Epoch 11 [1920/8883 (21.58%)]		Loss: 1.00493
Training Progress: 	Epoch 11 [2240/8883 (25.18%)]		Loss: 0.92991
Training Progress: 	Epoch 11 [2560/8883 (28.78%)]		Loss: 1.00247
Training Progress: 	Epoch 11 [2880/8883 (32.37%)]		Loss: 1.12900
Training Progress: 	Epoch 11 [3200/8883 (35.97%)]		Loss: 0.99628
Training Progress: 	Epoch 11 [3520/8883 (39.57%)]		Loss: 1.13015
Training Progress: 	Epoch 11 [3840/8883 (43.17%)]		Loss: 1.13576
Training Progress: 	Epoch 11 [4160/8883 (46.76%)]		Loss: 0.86511
Training Progress: 	Epoch 11 [4480/8883 (50.36%)]		Loss: 0.88278
Training Progress: 	Epoch 11 [4800/8883 (53.96%)]		Loss: 0.86930
Training Progress: 	Epoch 11 [5120/8883 (57.55%)]		Loss: 1.19086
Training Progress: 	Epoch 11 [5440/8883 (61.15%)]		Loss: 0.99328
Training Progress: 	Epoch 11 [5760/8883 (64.75%)]		Loss: 0.92272
Training Progress: 	Epoch 11 [6080/8883 (68.35%)]		Loss: 0.95693
Training Progress: 	Epoch 11 [6400/8883 (71.94%)]		Loss: 1.10535
Training Progress: 	Epoch 11 [6720/8883 (75.54%)]		Loss: 1.14313
Training Progress: 	Epoch 11 [7040/8883 (79.14%)]		Loss: 1.02132
Training Progress: 	Epoch 11 [7360/8883 (82.73%)]		Loss: 1.02733
Training Progress: 	Epoch 11 [7680/8883 (86.33%)]		Loss: 0.73188
Training Progress: 	Epoch 11 [8000/8883 (89.93%)]		Loss: 0.90213
Training Progress: 	Epoch 11 [8320/8883 (93.53%)]		Loss: 0.87669
Training Progress: 	Epoch 11 [8640/8883 (97.12%)]		Loss: 1.26562
	Train loss: 0.02610, Accuracy: 5670/8883 (63.00%)
	Validation loss: 0.00044, Accuracy: 1213/1692 (71.00%)
	Test loss: 0.00053, Accuracy: 1055/1772 (59.00%)

Training Progress: 	Epoch 12 [0/8883 (0.00%)]		Loss: 0.99327
Training Progress: 	Epoch 12 [320/8883 (3.60%)]		Loss: 0.90423
Training Progress: 	Epoch 12 [640/8883 (7.19%)]		Loss: 0.85664
Training Progress: 	Epoch 12 [960/8883 (10.79%)]		Loss: 1.24090
Training Progress: 	Epoch 12 [1280/8883 (14.39%)]		Loss: 1.07197
Training Progress: 	Epoch 12 [1600/8883 (17.99%)]		Loss: 0.73200
Training Progress: 	Epoch 12 [1920/8883 (21.58%)]		Loss: 0.78279
Training Progress: 	Epoch 12 [2240/8883 (25.18%)]		Loss: 1.00808
Training Progress: 	Epoch 12 [2560/8883 (28.78%)]		Loss: 0.92882
Training Progress: 	Epoch 12 [2880/8883 (32.37%)]		Loss: 0.93157
Training Progress: 	Epoch 12 [3200/8883 (35.97%)]		Loss: 1.14209
Training Progress: 	Epoch 12 [3520/8883 (39.57%)]		Loss: 1.42745
Training Progress: 	Epoch 12 [3840/8883 (43.17%)]		Loss: 1.16478
Training Progress: 	Epoch 12 [4160/8883 (46.76%)]		Loss: 0.97033
Training Progress: 	Epoch 12 [4480/8883 (50.36%)]		Loss: 0.84109
Training Progress: 	Epoch 12 [4800/8883 (53.96%)]		Loss: 1.06892
Training Progress: 	Epoch 12 [5120/8883 (57.55%)]		Loss: 0.99556
Training Progress: 	Epoch 12 [5440/8883 (61.15%)]		Loss: 1.17830
Training Progress: 	Epoch 12 [5760/8883 (64.75%)]		Loss: 0.97726
Training Progress: 	Epoch 12 [6080/8883 (68.35%)]		Loss: 0.87123
Training Progress: 	Epoch 12 [6400/8883 (71.94%)]		Loss: 0.94984
Training Progress: 	Epoch 12 [6720/8883 (75.54%)]		Loss: 1.17382
Training Progress: 	Epoch 12 [7040/8883 (79.14%)]		Loss: 0.82132
Training Progress: 	Epoch 12 [7360/8883 (82.73%)]		Loss: 0.98457
Training Progress: 	Epoch 12 [7680/8883 (86.33%)]		Loss: 0.78142
Training Progress: 	Epoch 12 [8000/8883 (89.93%)]		Loss: 1.21593
Training Progress: 	Epoch 12 [8320/8883 (93.53%)]		Loss: 0.93252
Training Progress: 	Epoch 12 [8640/8883 (97.12%)]		Loss: 1.20497
	Train loss: 0.02583, Accuracy: 5784/8883 (65.00%)
	Validation loss: 0.00043, Accuracy: 1232/1692 (72.00%)
	Test loss: 0.00053, Accuracy: 1092/1772 (61.00%)

Training Progress: 	Epoch 13 [0/8883 (0.00%)]		Loss: 0.85764
Training Progress: 	Epoch 13 [320/8883 (3.60%)]		Loss: 1.17890
Training Progress: 	Epoch 13 [640/8883 (7.19%)]		Loss: 0.91792
Training Progress: 	Epoch 13 [960/8883 (10.79%)]		Loss: 0.99516
Training Progress: 	Epoch 13 [1280/8883 (14.39%)]		Loss: 1.05879
Training Progress: 	Epoch 13 [1600/8883 (17.99%)]		Loss: 0.83442
Training Progress: 	Epoch 13 [1920/8883 (21.58%)]		Loss: 0.97338
Training Progress: 	Epoch 13 [2240/8883 (25.18%)]		Loss: 1.02906
Training Progress: 	Epoch 13 [2560/8883 (28.78%)]		Loss: 0.99940
Training Progress: 	Epoch 13 [2880/8883 (32.37%)]		Loss: 0.89020
Training Progress: 	Epoch 13 [3200/8883 (35.97%)]		Loss: 1.21205
Training Progress: 	Epoch 13 [3520/8883 (39.57%)]		Loss: 1.06851
Training Progress: 	Epoch 13 [3840/8883 (43.17%)]		Loss: 1.14903
Training Progress: 	Epoch 13 [4160/8883 (46.76%)]		Loss: 0.89405
Training Progress: 	Epoch 13 [4480/8883 (50.36%)]		Loss: 0.80696
Training Progress: 	Epoch 13 [4800/8883 (53.96%)]		Loss: 0.89771
Training Progress: 	Epoch 13 [5120/8883 (57.55%)]		Loss: 0.96193
Training Progress: 	Epoch 13 [5440/8883 (61.15%)]		Loss: 0.86142
Training Progress: 	Epoch 13 [5760/8883 (64.75%)]		Loss: 0.94501
Training Progress: 	Epoch 13 [6080/8883 (68.35%)]		Loss: 0.73824
Training Progress: 	Epoch 13 [6400/8883 (71.94%)]		Loss: 0.90076
Training Progress: 	Epoch 13 [6720/8883 (75.54%)]		Loss: 1.12802
Training Progress: 	Epoch 13 [7040/8883 (79.14%)]		Loss: 1.05834
Training Progress: 	Epoch 13 [7360/8883 (82.73%)]		Loss: 0.96369
Training Progress: 	Epoch 13 [7680/8883 (86.33%)]		Loss: 0.60401
Training Progress: 	Epoch 13 [8000/8883 (89.93%)]		Loss: 1.02245
Training Progress: 	Epoch 13 [8320/8883 (93.53%)]		Loss: 0.87289
Training Progress: 	Epoch 13 [8640/8883 (97.12%)]		Loss: 1.02550
	Train loss: 0.02398, Accuracy: 6037/8883 (67.00%)
	Validation loss: 0.00039, Accuracy: 1307/1692 (77.00%)
	Test loss: 0.00050, Accuracy: 1156/1772 (65.00%)

Training Progress: 	Epoch 14 [0/8883 (0.00%)]		Loss: 1.05018
Training Progress: 	Epoch 14 [320/8883 (3.60%)]		Loss: 1.06244
Training Progress: 	Epoch 14 [640/8883 (7.19%)]		Loss: 0.89847
Training Progress: 	Epoch 14 [960/8883 (10.79%)]		Loss: 1.09929
Training Progress: 	Epoch 14 [1280/8883 (14.39%)]		Loss: 1.05187
Training Progress: 	Epoch 14 [1600/8883 (17.99%)]		Loss: 0.81516
Training Progress: 	Epoch 14 [1920/8883 (21.58%)]		Loss: 0.89523
Training Progress: 	Epoch 14 [2240/8883 (25.18%)]		Loss: 0.79763
Training Progress: 	Epoch 14 [2560/8883 (28.78%)]		Loss: 0.94938
Training Progress: 	Epoch 14 [2880/8883 (32.37%)]		Loss: 0.98403
Training Progress: 	Epoch 14 [3200/8883 (35.97%)]		Loss: 1.10443
Training Progress: 	Epoch 14 [3520/8883 (39.57%)]		Loss: 1.23819
Training Progress: 	Epoch 14 [3840/8883 (43.17%)]		Loss: 1.05404
Training Progress: 	Epoch 14 [4160/8883 (46.76%)]		Loss: 1.13118
Training Progress: 	Epoch 14 [4480/8883 (50.36%)]		Loss: 0.78814
Training Progress: 	Epoch 14 [4800/8883 (53.96%)]		Loss: 0.83686
Training Progress: 	Epoch 14 [5120/8883 (57.55%)]		Loss: 0.91190
Training Progress: 	Epoch 14 [5440/8883 (61.15%)]		Loss: 0.77568
Training Progress: 	Epoch 14 [5760/8883 (64.75%)]		Loss: 0.87428
Training Progress: 	Epoch 14 [6080/8883 (68.35%)]		Loss: 0.67594
Training Progress: 	Epoch 14 [6400/8883 (71.94%)]		Loss: 0.92945
Training Progress: 	Epoch 14 [6720/8883 (75.54%)]		Loss: 1.16032
Training Progress: 	Epoch 14 [7040/8883 (79.14%)]		Loss: 0.98706
Training Progress: 	Epoch 14 [7360/8883 (82.73%)]		Loss: 0.88405
Training Progress: 	Epoch 14 [7680/8883 (86.33%)]		Loss: 0.72458
Training Progress: 	Epoch 14 [8000/8883 (89.93%)]		Loss: 0.92611
Training Progress: 	Epoch 14 [8320/8883 (93.53%)]		Loss: 0.85378
Training Progress: 	Epoch 14 [8640/8883 (97.12%)]		Loss: 0.96844
	Train loss: 0.02398, Accuracy: 5969/8883 (67.00%)
	Validation loss: 0.00039, Accuracy: 1296/1692 (76.00%)
	Test loss: 0.00052, Accuracy: 1101/1772 (62.00%)

Training Progress: 	Epoch 15 [0/8883 (0.00%)]		Loss: 1.02617
Training Progress: 	Epoch 15 [320/8883 (3.60%)]		Loss: 0.99475
Training Progress: 	Epoch 15 [640/8883 (7.19%)]		Loss: 0.60219
Training Progress: 	Epoch 15 [960/8883 (10.79%)]		Loss: 0.96935
Training Progress: 	Epoch 15 [1280/8883 (14.39%)]		Loss: 0.90675
Training Progress: 	Epoch 15 [1600/8883 (17.99%)]		Loss: 0.67626
Training Progress: 	Epoch 15 [1920/8883 (21.58%)]		Loss: 0.88138
Training Progress: 	Epoch 15 [2240/8883 (25.18%)]		Loss: 0.76790
Training Progress: 	Epoch 15 [2560/8883 (28.78%)]		Loss: 1.04448
Training Progress: 	Epoch 15 [2880/8883 (32.37%)]		Loss: 0.90541
Training Progress: 	Epoch 15 [3200/8883 (35.97%)]		Loss: 0.88826
Training Progress: 	Epoch 15 [3520/8883 (39.57%)]		Loss: 1.19383
Training Progress: 	Epoch 15 [3840/8883 (43.17%)]		Loss: 0.96300
Training Progress: 	Epoch 15 [4160/8883 (46.76%)]		Loss: 0.94237
Training Progress: 	Epoch 15 [4480/8883 (50.36%)]		Loss: 0.83796
Training Progress: 	Epoch 15 [4800/8883 (53.96%)]		Loss: 0.96195
Training Progress: 	Epoch 15 [5120/8883 (57.55%)]		Loss: 1.14696
Training Progress: 	Epoch 15 [5440/8883 (61.15%)]		Loss: 1.14915
Training Progress: 	Epoch 15 [5760/8883 (64.75%)]		Loss: 0.78309
Training Progress: 	Epoch 15 [6080/8883 (68.35%)]		Loss: 0.69986
Training Progress: 	Epoch 15 [6400/8883 (71.94%)]		Loss: 0.79103
Training Progress: 	Epoch 15 [6720/8883 (75.54%)]		Loss: 0.99764
Training Progress: 	Epoch 15 [7040/8883 (79.14%)]		Loss: 0.85260
Training Progress: 	Epoch 15 [7360/8883 (82.73%)]		Loss: 0.91721
Training Progress: 	Epoch 15 [7680/8883 (86.33%)]		Loss: 0.74683
Training Progress: 	Epoch 15 [8000/8883 (89.93%)]		Loss: 0.91992
Training Progress: 	Epoch 15 [8320/8883 (93.53%)]		Loss: 1.08582
Training Progress: 	Epoch 15 [8640/8883 (97.12%)]		Loss: 1.17550
	Train loss: 0.02323, Accuracy: 6100/8883 (68.00%)
	Validation loss: 0.00037, Accuracy: 1308/1692 (77.00%)
	Test loss: 0.00052, Accuracy: 1107/1772 (62.00%)

Training Progress: 	Epoch 16 [0/8883 (0.00%)]		Loss: 0.90383
Training Progress: 	Epoch 16 [320/8883 (3.60%)]		Loss: 1.04039
Training Progress: 	Epoch 16 [640/8883 (7.19%)]		Loss: 0.76314
Training Progress: 	Epoch 16 [960/8883 (10.79%)]		Loss: 1.02385
Training Progress: 	Epoch 16 [1280/8883 (14.39%)]		Loss: 0.81634
Training Progress: 	Epoch 16 [1600/8883 (17.99%)]		Loss: 0.70755
Training Progress: 	Epoch 16 [1920/8883 (21.58%)]		Loss: 0.78043
Training Progress: 	Epoch 16 [2240/8883 (25.18%)]		Loss: 0.82828
Training Progress: 	Epoch 16 [2560/8883 (28.78%)]		Loss: 0.87208
Training Progress: 	Epoch 16 [2880/8883 (32.37%)]		Loss: 0.96163
Training Progress: 	Epoch 16 [3200/8883 (35.97%)]		Loss: 0.92570
Training Progress: 	Epoch 16 [3520/8883 (39.57%)]		Loss: 1.20090
Training Progress: 	Epoch 16 [3840/8883 (43.17%)]		Loss: 1.07178
Training Progress: 	Epoch 16 [4160/8883 (46.76%)]		Loss: 1.01039
Training Progress: 	Epoch 16 [4480/8883 (50.36%)]		Loss: 0.71219
Training Progress: 	Epoch 16 [4800/8883 (53.96%)]		Loss: 0.76689
Training Progress: 	Epoch 16 [5120/8883 (57.55%)]		Loss: 1.02617
Training Progress: 	Epoch 16 [5440/8883 (61.15%)]		Loss: 0.98213
Training Progress: 	Epoch 16 [5760/8883 (64.75%)]		Loss: 0.82922
Training Progress: 	Epoch 16 [6080/8883 (68.35%)]		Loss: 0.73304
Training Progress: 	Epoch 16 [6400/8883 (71.94%)]		Loss: 0.81237
Training Progress: 	Epoch 16 [6720/8883 (75.54%)]		Loss: 0.92792
Training Progress: 	Epoch 16 [7040/8883 (79.14%)]		Loss: 0.82967
Training Progress: 	Epoch 16 [7360/8883 (82.73%)]		Loss: 0.85155
Training Progress: 	Epoch 16 [7680/8883 (86.33%)]		Loss: 0.78709
Training Progress: 	Epoch 16 [8000/8883 (89.93%)]		Loss: 0.80620
Training Progress: 	Epoch 16 [8320/8883 (93.53%)]		Loss: 1.01006
Training Progress: 	Epoch 16 [8640/8883 (97.12%)]		Loss: 1.05144
	Train loss: 0.02261, Accuracy: 6154/8883 (69.00%)
	Validation loss: 0.00036, Accuracy: 1322/1692 (78.00%)
	Test loss: 0.00053, Accuracy: 1144/1772 (64.00%)

Training Progress: 	Epoch 17 [0/8883 (0.00%)]		Loss: 0.80769
Training Progress: 	Epoch 17 [320/8883 (3.60%)]		Loss: 0.97266
Training Progress: 	Epoch 17 [640/8883 (7.19%)]		Loss: 0.90235
Training Progress: 	Epoch 17 [960/8883 (10.79%)]		Loss: 1.03064
Training Progress: 	Epoch 17 [1280/8883 (14.39%)]		Loss: 0.90660
Training Progress: 	Epoch 17 [1600/8883 (17.99%)]		Loss: 0.97242
Training Progress: 	Epoch 17 [1920/8883 (21.58%)]		Loss: 0.83981
Training Progress: 	Epoch 17 [2240/8883 (25.18%)]		Loss: 0.77807
Training Progress: 	Epoch 17 [2560/8883 (28.78%)]		Loss: 0.92913
Training Progress: 	Epoch 17 [2880/8883 (32.37%)]		Loss: 0.95654
Training Progress: 	Epoch 17 [3200/8883 (35.97%)]		Loss: 0.97571
Training Progress: 	Epoch 17 [3520/8883 (39.57%)]		Loss: 1.24192
Training Progress: 	Epoch 17 [3840/8883 (43.17%)]		Loss: 0.98230
Training Progress: 	Epoch 17 [4160/8883 (46.76%)]		Loss: 0.90509
Training Progress: 	Epoch 17 [4480/8883 (50.36%)]		Loss: 0.74098
Training Progress: 	Epoch 17 [4800/8883 (53.96%)]		Loss: 0.86000
Training Progress: 	Epoch 17 [5120/8883 (57.55%)]		Loss: 1.10809
Training Progress: 	Epoch 17 [5440/8883 (61.15%)]		Loss: 0.98163
Training Progress: 	Epoch 17 [5760/8883 (64.75%)]		Loss: 0.79350
Training Progress: 	Epoch 17 [6080/8883 (68.35%)]		Loss: 0.72681
Training Progress: 	Epoch 17 [6400/8883 (71.94%)]		Loss: 0.85262
Training Progress: 	Epoch 17 [6720/8883 (75.54%)]		Loss: 1.10614
Training Progress: 	Epoch 17 [7040/8883 (79.14%)]		Loss: 0.93559
Training Progress: 	Epoch 17 [7360/8883 (82.73%)]		Loss: 0.98827
Training Progress: 	Epoch 17 [7680/8883 (86.33%)]		Loss: 0.74535
Training Progress: 	Epoch 17 [8000/8883 (89.93%)]		Loss: 0.86771
Training Progress: 	Epoch 17 [8320/8883 (93.53%)]		Loss: 0.89622
Training Progress: 	Epoch 17 [8640/8883 (97.12%)]		Loss: 0.94149
	Train loss: 0.02166, Accuracy: 6347/8883 (71.00%)
	Validation loss: 0.00033, Accuracy: 1387/1692 (81.00%)
	Test loss: 0.00050, Accuracy: 1152/1772 (65.00%)

Training Progress: 	Epoch 18 [0/8883 (0.00%)]		Loss: 1.14399
Training Progress: 	Epoch 18 [320/8883 (3.60%)]		Loss: 0.85151
Training Progress: 	Epoch 18 [640/8883 (7.19%)]		Loss: 0.75566
Training Progress: 	Epoch 18 [960/8883 (10.79%)]		Loss: 0.93781
Training Progress: 	Epoch 18 [1280/8883 (14.39%)]		Loss: 1.02634
Training Progress: 	Epoch 18 [1600/8883 (17.99%)]		Loss: 0.64440
Training Progress: 	Epoch 18 [1920/8883 (21.58%)]		Loss: 0.77451
Training Progress: 	Epoch 18 [2240/8883 (25.18%)]		Loss: 0.74677
Training Progress: 	Epoch 18 [2560/8883 (28.78%)]		Loss: 1.01443
Training Progress: 	Epoch 18 [2880/8883 (32.37%)]		Loss: 0.92104
Training Progress: 	Epoch 18 [3200/8883 (35.97%)]		Loss: 1.14423
Training Progress: 	Epoch 18 [3520/8883 (39.57%)]		Loss: 1.11159
Training Progress: 	Epoch 18 [3840/8883 (43.17%)]		Loss: 1.07873
Training Progress: 	Epoch 18 [4160/8883 (46.76%)]		Loss: 1.06462
Training Progress: 	Epoch 18 [4480/8883 (50.36%)]		Loss: 0.89914
Training Progress: 	Epoch 18 [4800/8883 (53.96%)]		Loss: 0.77904
Training Progress: 	Epoch 18 [5120/8883 (57.55%)]		Loss: 0.84983
Training Progress: 	Epoch 18 [5440/8883 (61.15%)]		Loss: 1.20429
Training Progress: 	Epoch 18 [5760/8883 (64.75%)]		Loss: 0.83337
Training Progress: 	Epoch 18 [6080/8883 (68.35%)]		Loss: 0.71056
Training Progress: 	Epoch 18 [6400/8883 (71.94%)]		Loss: 0.90109
Training Progress: 	Epoch 18 [6720/8883 (75.54%)]		Loss: 0.89658
Training Progress: 	Epoch 18 [7040/8883 (79.14%)]		Loss: 0.93951
Training Progress: 	Epoch 18 [7360/8883 (82.73%)]		Loss: 0.82041
Training Progress: 	Epoch 18 [7680/8883 (86.33%)]		Loss: 0.87960
Training Progress: 	Epoch 18 [8000/8883 (89.93%)]		Loss: 0.98792
Training Progress: 	Epoch 18 [8320/8883 (93.53%)]		Loss: 0.91134
Training Progress: 	Epoch 18 [8640/8883 (97.12%)]		Loss: 0.98455
	Train loss: 0.02120, Accuracy: 6308/8883 (71.00%)
	Validation loss: 0.00033, Accuracy: 1359/1692 (80.00%)
	Test loss: 0.00051, Accuracy: 1165/1772 (65.00%)

Training Progress: 	Epoch 19 [0/8883 (0.00%)]		Loss: 0.70798
Training Progress: 	Epoch 19 [320/8883 (3.60%)]		Loss: 0.91243
Training Progress: 	Epoch 19 [640/8883 (7.19%)]		Loss: 0.81178
Training Progress: 	Epoch 19 [960/8883 (10.79%)]		Loss: 0.97598
Training Progress: 	Epoch 19 [1280/8883 (14.39%)]		Loss: 0.92164
Training Progress: 	Epoch 19 [1600/8883 (17.99%)]		Loss: 1.00500
Training Progress: 	Epoch 19 [1920/8883 (21.58%)]		Loss: 0.74311
Training Progress: 	Epoch 19 [2240/8883 (25.18%)]		Loss: 0.84030
Training Progress: 	Epoch 19 [2560/8883 (28.78%)]		Loss: 0.86573
Training Progress: 	Epoch 19 [2880/8883 (32.37%)]		Loss: 0.83136
Training Progress: 	Epoch 19 [3200/8883 (35.97%)]		Loss: 0.99590
Training Progress: 	Epoch 19 [3520/8883 (39.57%)]		Loss: 1.00784
Training Progress: 	Epoch 19 [3840/8883 (43.17%)]		Loss: 0.88930
Training Progress: 	Epoch 19 [4160/8883 (46.76%)]		Loss: 1.13334
Training Progress: 	Epoch 19 [4480/8883 (50.36%)]		Loss: 0.61679
Training Progress: 	Epoch 19 [4800/8883 (53.96%)]		Loss: 0.90519
Training Progress: 	Epoch 19 [5120/8883 (57.55%)]		Loss: 0.92010
Training Progress: 	Epoch 19 [5440/8883 (61.15%)]		Loss: 0.99042
Training Progress: 	Epoch 19 [5760/8883 (64.75%)]		Loss: 0.81785
Training Progress: 	Epoch 19 [6080/8883 (68.35%)]		Loss: 0.76408
Training Progress: 	Epoch 19 [6400/8883 (71.94%)]		Loss: 0.86108
Training Progress: 	Epoch 19 [6720/8883 (75.54%)]		Loss: 0.94593
Training Progress: 	Epoch 19 [7040/8883 (79.14%)]		Loss: 0.79056
Training Progress: 	Epoch 19 [7360/8883 (82.73%)]		Loss: 0.89459
Training Progress: 	Epoch 19 [7680/8883 (86.33%)]		Loss: 0.67951
Training Progress: 	Epoch 19 [8000/8883 (89.93%)]		Loss: 0.90778
Training Progress: 	Epoch 19 [8320/8883 (93.53%)]		Loss: 0.86646
Training Progress: 	Epoch 19 [8640/8883 (97.12%)]		Loss: 0.97597
	Train loss: 0.02019, Accuracy: 6549/8883 (73.00%)
	Validation loss: 0.00030, Accuracy: 1422/1692 (84.00%)
	Test loss: 0.00050, Accuracy: 1166/1772 (65.00%)

Training Progress: 	Epoch 20 [0/8883 (0.00%)]		Loss: 1.03093
Training Progress: 	Epoch 20 [320/8883 (3.60%)]		Loss: 0.87969
Training Progress: 	Epoch 20 [640/8883 (7.19%)]		Loss: 0.67986
Training Progress: 	Epoch 20 [960/8883 (10.79%)]		Loss: 1.03905
Training Progress: 	Epoch 20 [1280/8883 (14.39%)]		Loss: 0.87425
Training Progress: 	Epoch 20 [1600/8883 (17.99%)]		Loss: 0.85156
Training Progress: 	Epoch 20 [1920/8883 (21.58%)]		Loss: 0.76866
Training Progress: 	Epoch 20 [2240/8883 (25.18%)]		Loss: 0.71761
Training Progress: 	Epoch 20 [2560/8883 (28.78%)]		Loss: 0.99135
Training Progress: 	Epoch 20 [2880/8883 (32.37%)]		Loss: 0.96914
Training Progress: 	Epoch 20 [3200/8883 (35.97%)]		Loss: 0.94963
Training Progress: 	Epoch 20 [3520/8883 (39.57%)]		Loss: 1.09346
Training Progress: 	Epoch 20 [3840/8883 (43.17%)]		Loss: 1.09157
Training Progress: 	Epoch 20 [4160/8883 (46.76%)]		Loss: 0.62730
Training Progress: 	Epoch 20 [4480/8883 (50.36%)]		Loss: 0.65009
Training Progress: 	Epoch 20 [4800/8883 (53.96%)]		Loss: 0.76388
Training Progress: 	Epoch 20 [5120/8883 (57.55%)]		Loss: 0.98086
Training Progress: 	Epoch 20 [5440/8883 (61.15%)]		Loss: 1.04313
Training Progress: 	Epoch 20 [5760/8883 (64.75%)]		Loss: 0.89520
Training Progress: 	Epoch 20 [6080/8883 (68.35%)]		Loss: 0.70728
Training Progress: 	Epoch 20 [6400/8883 (71.94%)]		Loss: 0.90448
Training Progress: 	Epoch 20 [6720/8883 (75.54%)]		Loss: 0.95628
Training Progress: 	Epoch 20 [7040/8883 (79.14%)]		Loss: 0.97730
Training Progress: 	Epoch 20 [7360/8883 (82.73%)]		Loss: 0.93654
Training Progress: 	Epoch 20 [7680/8883 (86.33%)]		Loss: 0.68184
Training Progress: 	Epoch 20 [8000/8883 (89.93%)]		Loss: 1.15410
Training Progress: 	Epoch 20 [8320/8883 (93.53%)]		Loss: 0.72477
Training Progress: 	Epoch 20 [8640/8883 (97.12%)]		Loss: 0.92478
	Train loss: 0.02054, Accuracy: 6424/8883 (72.00%)
	Validation loss: 0.00031, Accuracy: 1391/1692 (82.00%)
	Test loss: 0.00053, Accuracy: 1111/1772 (62.00%)

Training Progress: 	Epoch 21 [0/8883 (0.00%)]		Loss: 0.92609
Training Progress: 	Epoch 21 [320/8883 (3.60%)]		Loss: 0.77240
Training Progress: 	Epoch 21 [640/8883 (7.19%)]		Loss: 0.83420
Training Progress: 	Epoch 21 [960/8883 (10.79%)]		Loss: 1.05915
Training Progress: 	Epoch 21 [1280/8883 (14.39%)]		Loss: 0.90602
Training Progress: 	Epoch 21 [1600/8883 (17.99%)]		Loss: 0.74378
Training Progress: 	Epoch 21 [1920/8883 (21.58%)]		Loss: 0.52758
Training Progress: 	Epoch 21 [2240/8883 (25.18%)]		Loss: 0.61734
Training Progress: 	Epoch 21 [2560/8883 (28.78%)]		Loss: 0.93572
Training Progress: 	Epoch 21 [2880/8883 (32.37%)]		Loss: 0.86481
Training Progress: 	Epoch 21 [3200/8883 (35.97%)]		Loss: 1.09590
Training Progress: 	Epoch 21 [3520/8883 (39.57%)]		Loss: 0.94120
Training Progress: 	Epoch 21 [3840/8883 (43.17%)]		Loss: 0.89240
Training Progress: 	Epoch 21 [4160/8883 (46.76%)]		Loss: 1.02596
Training Progress: 	Epoch 21 [4480/8883 (50.36%)]		Loss: 0.80684
Training Progress: 	Epoch 21 [4800/8883 (53.96%)]		Loss: 0.94240
Training Progress: 	Epoch 21 [5120/8883 (57.55%)]		Loss: 0.81716
Training Progress: 	Epoch 21 [5440/8883 (61.15%)]		Loss: 0.89980
Training Progress: 	Epoch 21 [5760/8883 (64.75%)]		Loss: 0.85431
Training Progress: 	Epoch 21 [6080/8883 (68.35%)]		Loss: 0.54816
Training Progress: 	Epoch 21 [6400/8883 (71.94%)]		Loss: 0.78509
Training Progress: 	Epoch 21 [6720/8883 (75.54%)]		Loss: 0.89508
Training Progress: 	Epoch 21 [7040/8883 (79.14%)]		Loss: 0.86391
Training Progress: 	Epoch 21 [7360/8883 (82.73%)]		Loss: 0.87872
Training Progress: 	Epoch 21 [7680/8883 (86.33%)]		Loss: 0.68170
Training Progress: 	Epoch 21 [8000/8883 (89.93%)]		Loss: 0.90197
Training Progress: 	Epoch 21 [8320/8883 (93.53%)]		Loss: 0.65994
Training Progress: 	Epoch 21 [8640/8883 (97.12%)]		Loss: 1.02651
	Train loss: 0.01984, Accuracy: 6541/8883 (73.00%)
	Validation loss: 0.00030, Accuracy: 1400/1692 (82.00%)
	Test loss: 0.00054, Accuracy: 1108/1772 (62.00%)

Training Progress: 	Epoch 22 [0/8883 (0.00%)]		Loss: 0.88232
Training Progress: 	Epoch 22 [0/8883 (0.00%)]		Loss: 0.95159
Training Progress: 	Epoch 22 [320/8883 (3.60%)]		Loss: 1.03424
Training Progress: 	Epoch 22 [640/8883 (7.19%)]		Loss: 1.11961
Training Progress: 	Epoch 22 [960/8883 (10.79%)]		Loss: 1.00437
Training Progress: 	Epoch 22 [1280/8883 (14.39%)]		Loss: 1.04860
Training Progress: 	Epoch 22 [1600/8883 (17.99%)]		Loss: 1.11183
Training Progress: 	Epoch 22 [1920/8883 (21.58%)]		Loss: 1.04519
Training Progress: 	Epoch 22 [2240/8883 (25.18%)]		Loss: 1.02326
Training Progress: 	Epoch 22 [2560/8883 (28.78%)]		Loss: 1.00057
Training Progress: 	Epoch 22 [2880/8883 (32.37%)]		Loss: 1.14804
Training Progress: 	Epoch 22 [3200/8883 (35.97%)]		Loss: 1.06565
Training Progress: 	Epoch 22 [3520/8883 (39.57%)]		Loss: 1.12618
Training Progress: 	Epoch 22 [3840/8883 (43.17%)]		Loss: 1.10261
Training Progress: 	Epoch 22 [4160/8883 (46.76%)]		Loss: 0.84525
Training Progress: 	Epoch 22 [4480/8883 (50.36%)]		Loss: 0.83228
Training Progress: 	Epoch 22 [4800/8883 (53.96%)]		Loss: 0.77269
Training Progress: 	Epoch 22 [5120/8883 (57.55%)]		Loss: 0.95622
Training Progress: 	Epoch 22 [5440/8883 (61.15%)]		Loss: 1.10576
Training Progress: 	Epoch 22 [5760/8883 (64.75%)]		Loss: 1.02724
Training Progress: 	Epoch 22 [6080/8883 (68.35%)]		Loss: 0.99145
Training Progress: 	Epoch 22 [6400/8883 (71.94%)]		Loss: 1.04737
Training Progress: 	Epoch 22 [6720/8883 (75.54%)]		Loss: 0.93683
Training Progress: 	Epoch 22 [7040/8883 (79.14%)]		Loss: 0.98211
Training Progress: 	Epoch 22 [7360/8883 (82.73%)]		Loss: 0.90584
Training Progress: 	Epoch 22 [7680/8883 (86.33%)]		Loss: 0.89048
Training Progress: 	Epoch 22 [8000/8883 (89.93%)]		Loss: 0.85185
Training Progress: 	Epoch 22 [8320/8883 (93.53%)]		Loss: 1.13161
Training Progress: 	Epoch 22 [8640/8883 (97.12%)]		Loss: 1.05964
	Train loss: 0.02786, Accuracy: 5301/8883 (59.00%)
	Validation loss: 0.00058, Accuracy: 1006/1692 (59.00%)
	Test loss: 0.00103, Accuracy: 533/1772 (30.00%)

Training Progress: 	Epoch 23 [0/8883 (0.00%)]		Loss: 1.00275
Training Progress: 	Epoch 23 [320/8883 (3.60%)]		Loss: 1.12703
Training Progress: 	Epoch 23 [640/8883 (7.19%)]		Loss: 1.10898
Training Progress: 	Epoch 23 [960/8883 (10.79%)]		Loss: 0.93372
Training Progress: 	Epoch 23 [1280/8883 (14.39%)]		Loss: 1.10980
Training Progress: 	Epoch 23 [1600/8883 (17.99%)]		Loss: 1.18458
Training Progress: 	Epoch 23 [1920/8883 (21.58%)]		Loss: 0.91204
Training Progress: 	Epoch 23 [2240/8883 (25.18%)]		Loss: 0.92928
Training Progress: 	Epoch 23 [2560/8883 (28.78%)]		Loss: 1.05406
Training Progress: 	Epoch 23 [2880/8883 (32.37%)]		Loss: 1.05094
Training Progress: 	Epoch 23 [3200/8883 (35.97%)]		Loss: 0.89831
Training Progress: 	Epoch 23 [3520/8883 (39.57%)]		Loss: 1.22702
Training Progress: 	Epoch 23 [3840/8883 (43.17%)]		Loss: 1.04158
Training Progress: 	Epoch 23 [4160/8883 (46.76%)]		Loss: 0.82829
Training Progress: 	Epoch 23 [4480/8883 (50.36%)]		Loss: 0.92310
Training Progress: 	Epoch 23 [4800/8883 (53.96%)]		Loss: 0.85924
Training Progress: 	Epoch 23 [5120/8883 (57.55%)]		Loss: 0.89953
Training Progress: 	Epoch 23 [5440/8883 (61.15%)]		Loss: 1.05338
Training Progress: 	Epoch 23 [5760/8883 (64.75%)]		Loss: 0.90362
Training Progress: 	Epoch 23 [6080/8883 (68.35%)]		Loss: 1.17094
Training Progress: 	Epoch 23 [6400/8883 (71.94%)]		Loss: 1.04912
Training Progress: 	Epoch 23 [6720/8883 (75.54%)]		Loss: 1.01301
Training Progress: 	Epoch 23 [7040/8883 (79.14%)]		Loss: 1.04011
Training Progress: 	Epoch 23 [7360/8883 (82.73%)]		Loss: 0.88332
Training Progress: 	Epoch 23 [7680/8883 (86.33%)]		Loss: 0.98921
Training Progress: 	Epoch 23 [8000/8883 (89.93%)]		Loss: 0.79717
Training Progress: 	Epoch 23 [8320/8883 (93.53%)]		Loss: 0.98505
Training Progress: 	Epoch 23 [8640/8883 (97.12%)]		Loss: 1.03152
	Train loss: 0.02692, Accuracy: 5480/8883 (61.00%)
	Validation loss: 0.00055, Accuracy: 1032/1692 (60.00%)
	Test loss: 0.00103, Accuracy: 559/1772 (31.00%)

Training Progress: 	Epoch 24 [0/8883 (0.00%)]		Loss: 0.90905
Training Progress: 	Epoch 24 [320/8883 (3.60%)]		Loss: 1.05719
Training Progress: 	Epoch 24 [640/8883 (7.19%)]		Loss: 0.88427
Training Progress: 	Epoch 24 [960/8883 (10.79%)]		Loss: 0.85699
Training Progress: 	Epoch 24 [1280/8883 (14.39%)]		Loss: 0.93714
Training Progress: 	Epoch 24 [1600/8883 (17.99%)]		Loss: 1.10999
Training Progress: 	Epoch 24 [1920/8883 (21.58%)]		Loss: 1.00226
Training Progress: 	Epoch 24 [2240/8883 (25.18%)]		Loss: 0.92457
Training Progress: 	Epoch 24 [2560/8883 (28.78%)]		Loss: 1.01870
Training Progress: 	Epoch 24 [2880/8883 (32.37%)]		Loss: 1.09875
Training Progress: 	Epoch 24 [3200/8883 (35.97%)]		Loss: 1.03046
Training Progress: 	Epoch 24 [3520/8883 (39.57%)]		Loss: 1.07810
Training Progress: 	Epoch 24 [3840/8883 (43.17%)]		Loss: 1.03897
Training Progress: 	Epoch 24 [4160/8883 (46.76%)]		Loss: 0.82312
Training Progress: 	Epoch 24 [4480/8883 (50.36%)]		Loss: 0.89583
Training Progress: 	Epoch 24 [4800/8883 (53.96%)]		Loss: 0.80179
Training Progress: 	Epoch 24 [5120/8883 (57.55%)]		Loss: 1.04072
Training Progress: 	Epoch 24 [5440/8883 (61.15%)]		Loss: 1.01280
Training Progress: 	Epoch 24 [5760/8883 (64.75%)]		Loss: 0.92067
Training Progress: 	Epoch 24 [6080/8883 (68.35%)]		Loss: 0.99374
Training Progress: 	Epoch 24 [6400/8883 (71.94%)]		Loss: 0.96141
Training Progress: 	Epoch 24 [6720/8883 (75.54%)]		Loss: 0.91176
Training Progress: 	Epoch 24 [7040/8883 (79.14%)]		Loss: 1.00307
Training Progress: 	Epoch 24 [7360/8883 (82.73%)]		Loss: 0.87073
Training Progress: 	Epoch 24 [7680/8883 (86.33%)]		Loss: 0.84002
Training Progress: 	Epoch 24 [8000/8883 (89.93%)]		Loss: 0.87518
Training Progress: 	Epoch 24 [8320/8883 (93.53%)]		Loss: 1.06604
Training Progress: 	Epoch 24 [8640/8883 (97.12%)]		Loss: 0.97690
	Train loss: 0.02700, Accuracy: 5446/8883 (61.00%)
	Validation loss: 0.00055, Accuracy: 1036/1692 (61.00%)
	Test loss: 0.00105, Accuracy: 508/1772 (28.00%)

Training Progress: 	Epoch 25 [0/8883 (0.00%)]		Loss: 1.02595
Training Progress: 	Epoch 25 [320/8883 (3.60%)]		Loss: 1.04790
Training Progress: 	Epoch 25 [640/8883 (7.19%)]		Loss: 0.97026
Training Progress: 	Epoch 25 [960/8883 (10.79%)]		Loss: 0.91741
Training Progress: 	Epoch 25 [1280/8883 (14.39%)]		Loss: 0.97925
Training Progress: 	Epoch 25 [1600/8883 (17.99%)]		Loss: 1.22283
Training Progress: 	Epoch 25 [1920/8883 (21.58%)]		Loss: 0.88898
Training Progress: 	Epoch 25 [2240/8883 (25.18%)]		Loss: 0.98058
Training Progress: 	Epoch 25 [2560/8883 (28.78%)]		Loss: 1.01381
Training Progress: 	Epoch 25 [2880/8883 (32.37%)]		Loss: 1.05285
Training Progress: 	Epoch 25 [3200/8883 (35.97%)]		Loss: 1.19557
Training Progress: 	Epoch 25 [3520/8883 (39.57%)]		Loss: 0.86573
Training Progress: 	Epoch 25 [3840/8883 (43.17%)]		Loss: 1.08437
Training Progress: 	Epoch 25 [4160/8883 (46.76%)]		Loss: 0.90619
Training Progress: 	Epoch 25 [4480/8883 (50.36%)]		Loss: 0.78059
Training Progress: 	Epoch 25 [4800/8883 (53.96%)]		Loss: 0.83059
Training Progress: 	Epoch 25 [5120/8883 (57.55%)]		Loss: 0.82720
Training Progress: 	Epoch 25 [5440/8883 (61.15%)]		Loss: 1.03046
Training Progress: 	Epoch 25 [5760/8883 (64.75%)]		Loss: 0.78515
Training Progress: 	Epoch 25 [6080/8883 (68.35%)]		Loss: 0.87110
Training Progress: 	Epoch 25 [6400/8883 (71.94%)]		Loss: 0.95583
Training Progress: 	Epoch 25 [6720/8883 (75.54%)]		Loss: 1.14913
Training Progress: 	Epoch 25 [7040/8883 (79.14%)]		Loss: 1.12888
Training Progress: 	Epoch 25 [7360/8883 (82.73%)]		Loss: 0.85433
Training Progress: 	Epoch 25 [7680/8883 (86.33%)]		Loss: 0.79847
Training Progress: 	Epoch 25 [8000/8883 (89.93%)]		Loss: 0.81405
Training Progress: 	Epoch 25 [8320/8883 (93.53%)]		Loss: 1.03669
Training Progress: 	Epoch 25 [8640/8883 (97.12%)]		Loss: 0.96153
	Train loss: 0.02588, Accuracy: 5532/8883 (62.00%)
	Validation loss: 0.00055, Accuracy: 1039/1692 (61.00%)
	Test loss: 0.00111, Accuracy: 543/1772 (30.00%)

Training Progress: 	Epoch 26 [0/8883 (0.00%)]		Loss: 0.83372
Training Progress: 	Epoch 26 [320/8883 (3.60%)]		Loss: 0.89586
Training Progress: 	Epoch 26 [640/8883 (7.19%)]		Loss: 0.92696
Training Progress: 	Epoch 26 [960/8883 (10.79%)]		Loss: 0.84894
Training Progress: 	Epoch 26 [1280/8883 (14.39%)]		Loss: 0.98391
Training Progress: 	Epoch 26 [1600/8883 (17.99%)]		Loss: 0.97997
Training Progress: 	Epoch 26 [1920/8883 (21.58%)]		Loss: 0.94924
Training Progress: 	Epoch 26 [2240/8883 (25.18%)]		Loss: 0.97496
Training Progress: 	Epoch 26 [2560/8883 (28.78%)]		Loss: 1.04541
Training Progress: 	Epoch 26 [2880/8883 (32.37%)]		Loss: 0.98283
Training Progress: 	Epoch 26 [3200/8883 (35.97%)]		Loss: 0.92109
Training Progress: 	Epoch 26 [3520/8883 (39.57%)]		Loss: 0.90255
Training Progress: 	Epoch 26 [3840/8883 (43.17%)]		Loss: 0.97279
Training Progress: 	Epoch 26 [4160/8883 (46.76%)]		Loss: 0.80793
Training Progress: 	Epoch 26 [4480/8883 (50.36%)]		Loss: 0.86932
Training Progress: 	Epoch 26 [4800/8883 (53.96%)]		Loss: 0.64804
Training Progress: 	Epoch 26 [5120/8883 (57.55%)]		Loss: 0.83627
Training Progress: 	Epoch 26 [5440/8883 (61.15%)]		Loss: 0.81666
Training Progress: 	Epoch 26 [5760/8883 (64.75%)]		Loss: 0.79482
Training Progress: 	Epoch 26 [6080/8883 (68.35%)]		Loss: 0.85091
Training Progress: 	Epoch 26 [6400/8883 (71.94%)]		Loss: 1.03798
Training Progress: 	Epoch 26 [6720/8883 (75.54%)]		Loss: 0.86554
Training Progress: 	Epoch 26 [7040/8883 (79.14%)]		Loss: 1.07604
Training Progress: 	Epoch 26 [7360/8883 (82.73%)]		Loss: 0.97531
Training Progress: 	Epoch 26 [7680/8883 (86.33%)]		Loss: 0.91601
Training Progress: 	Epoch 26 [8000/8883 (89.93%)]		Loss: 0.83723
Training Progress: 	Epoch 26 [8320/8883 (93.53%)]		Loss: 1.19425
Training Progress: 	Epoch 26 [8640/8883 (97.12%)]		Loss: 0.90931
	Train loss: 0.02603, Accuracy: 5577/8883 (62.00%)
	Validation loss: 0.00055, Accuracy: 1049/1692 (61.00%)
	Test loss: 0.00113, Accuracy: 506/1772 (28.00%)

Training Progress: 	Epoch 27 [0/8883 (0.00%)]		Loss: 0.97568
Training Progress: 	Epoch 27 [320/8883 (3.60%)]		Loss: 1.06857
Training Progress: 	Epoch 27 [640/8883 (7.19%)]		Loss: 1.09123
Training Progress: 	Epoch 27 [960/8883 (10.79%)]		Loss: 0.78719
Training Progress: 	Epoch 27 [1280/8883 (14.39%)]		Loss: 0.96327
Training Progress: 	Epoch 27 [1600/8883 (17.99%)]		Loss: 1.18029
Training Progress: 	Epoch 27 [1920/8883 (21.58%)]		Loss: 0.82069
Training Progress: 	Epoch 27 [2240/8883 (25.18%)]		Loss: 0.92013
Training Progress: 	Epoch 27 [2560/8883 (28.78%)]		Loss: 1.15986
Training Progress: 	Epoch 27 [2880/8883 (32.37%)]		Loss: 0.93170
Training Progress: 	Epoch 27 [3200/8883 (35.97%)]		Loss: 0.86341
Training Progress: 	Epoch 27 [3520/8883 (39.57%)]		Loss: 1.05004
Training Progress: 	Epoch 27 [3840/8883 (43.17%)]		Loss: 1.11811
Training Progress: 	Epoch 27 [4160/8883 (46.76%)]		Loss: 0.79524
Training Progress: 	Epoch 27 [4480/8883 (50.36%)]		Loss: 0.68220
Training Progress: 	Epoch 27 [4800/8883 (53.96%)]		Loss: 0.86537
Training Progress: 	Epoch 27 [5120/8883 (57.55%)]		Loss: 0.75040
Training Progress: 	Epoch 27 [5440/8883 (61.15%)]		Loss: 1.02185
Training Progress: 	Epoch 27 [5760/8883 (64.75%)]		Loss: 0.82206
Training Progress: 	Epoch 27 [6080/8883 (68.35%)]		Loss: 1.07948
Training Progress: 	Epoch 27 [6400/8883 (71.94%)]		Loss: 0.87623
Training Progress: 	Epoch 27 [6720/8883 (75.54%)]		Loss: 0.84815
Training Progress: 	Epoch 27 [7040/8883 (79.14%)]		Loss: 1.15147
Training Progress: 	Epoch 27 [7360/8883 (82.73%)]		Loss: 0.85078
Training Progress: 	Epoch 27 [7680/8883 (86.33%)]		Loss: 0.69356
Training Progress: 	Epoch 27 [8000/8883 (89.93%)]		Loss: 0.82837
Training Progress: 	Epoch 27 [8320/8883 (93.53%)]		Loss: 1.11589
Training Progress: 	Epoch 27 [8640/8883 (97.12%)]		Loss: 0.91391
	Train loss: 0.02436, Accuracy: 5805/8883 (65.00%)
	Validation loss: 0.00051, Accuracy: 1090/1692 (64.00%)
	Test loss: 0.00116, Accuracy: 487/1772 (27.00%)

Training Progress: 	Epoch 28 [0/8883 (0.00%)]		Loss: 0.87691
Training Progress: 	Epoch 28 [320/8883 (3.60%)]		Loss: 1.12089
Training Progress: 	Epoch 28 [640/8883 (7.19%)]		Loss: 1.04879
Training Progress: 	Epoch 28 [960/8883 (10.79%)]		Loss: 1.25857
Training Progress: 	Epoch 28 [1280/8883 (14.39%)]		Loss: 1.00553
Training Progress: 	Epoch 28 [1600/8883 (17.99%)]		Loss: 1.24128
Training Progress: 	Epoch 28 [1920/8883 (21.58%)]		Loss: 0.76379
Training Progress: 	Epoch 28 [2240/8883 (25.18%)]		Loss: 1.13219
Training Progress: 	Epoch 28 [2560/8883 (28.78%)]		Loss: 1.02863
Training Progress: 	Epoch 28 [2880/8883 (32.37%)]		Loss: 1.00343
Training Progress: 	Epoch 28 [3200/8883 (35.97%)]		Loss: 1.05725
Training Progress: 	Epoch 28 [3520/8883 (39.57%)]		Loss: 0.84421
Training Progress: 	Epoch 28 [3840/8883 (43.17%)]		Loss: 1.00616
Training Progress: 	Epoch 28 [4160/8883 (46.76%)]		Loss: 0.77528
Training Progress: 	Epoch 28 [4480/8883 (50.36%)]		Loss: 0.74625
Training Progress: 	Epoch 28 [4800/8883 (53.96%)]		Loss: 0.72075
Training Progress: 	Epoch 28 [5120/8883 (57.55%)]		Loss: 0.86790
Training Progress: 	Epoch 28 [5440/8883 (61.15%)]		Loss: 0.88147
Training Progress: 	Epoch 28 [5760/8883 (64.75%)]		Loss: 0.71164
Training Progress: 	Epoch 28 [6080/8883 (68.35%)]		Loss: 0.96892
Training Progress: 	Epoch 28 [6400/8883 (71.94%)]		Loss: 0.95296
Training Progress: 	Epoch 28 [6720/8883 (75.54%)]		Loss: 1.09878
Training Progress: 	Epoch 28 [7040/8883 (79.14%)]		Loss: 1.10728
Training Progress: 	Epoch 28 [7360/8883 (82.73%)]		Loss: 0.99227
Training Progress: 	Epoch 28 [7680/8883 (86.33%)]		Loss: 0.84981
Training Progress: 	Epoch 28 [8000/8883 (89.93%)]		Loss: 0.85274
Training Progress: 	Epoch 28 [8320/8883 (93.53%)]		Loss: 1.07539
Training Progress: 	Epoch 28 [8640/8883 (97.12%)]		Loss: 0.95630
	Train loss: 0.02450, Accuracy: 5724/8883 (64.00%)
	Validation loss: 0.00052, Accuracy: 1084/1692 (64.00%)
	Test loss: 0.00111, Accuracy: 528/1772 (29.00%)

Training Progress: 	Epoch 29 [0/8883 (0.00%)]		Loss: 0.81630
Training Progress: 	Epoch 29 [320/8883 (3.60%)]		Loss: 0.88709
Training Progress: 	Epoch 29 [640/8883 (7.19%)]		Loss: 0.87420
Training Progress: 	Epoch 29 [960/8883 (10.79%)]		Loss: 0.96205
Training Progress: 	Epoch 29 [1280/8883 (14.39%)]		Loss: 1.16488
Training Progress: 	Epoch 29 [1600/8883 (17.99%)]		Loss: 0.93186
Training Progress: 	Epoch 29 [1920/8883 (21.58%)]		Loss: 0.75991
Training Progress: 	Epoch 29 [2240/8883 (25.18%)]		Loss: 0.87158
Training Progress: 	Epoch 29 [2560/8883 (28.78%)]		Loss: 0.95696
Training Progress: 	Epoch 29 [2880/8883 (32.37%)]		Loss: 0.92411
Training Progress: 	Epoch 29 [3200/8883 (35.97%)]		Loss: 0.78018
Training Progress: 	Epoch 29 [3520/8883 (39.57%)]		Loss: 0.94437
Training Progress: 	Epoch 29 [3840/8883 (43.17%)]		Loss: 0.95373
Training Progress: 	Epoch 29 [4160/8883 (46.76%)]		Loss: 0.78221
Training Progress: 	Epoch 29 [4480/8883 (50.36%)]		Loss: 0.71399
Training Progress: 	Epoch 29 [4800/8883 (53.96%)]		Loss: 0.73407
Training Progress: 	Epoch 29 [5120/8883 (57.55%)]		Loss: 0.87485
Training Progress: 	Epoch 29 [5440/8883 (61.15%)]		Loss: 0.97038
Training Progress: 	Epoch 29 [5760/8883 (64.75%)]		Loss: 0.75842
Training Progress: 	Epoch 29 [6080/8883 (68.35%)]		Loss: 0.88670
Training Progress: 	Epoch 29 [6400/8883 (71.94%)]		Loss: 0.92554
Training Progress: 	Epoch 29 [6720/8883 (75.54%)]		Loss: 0.87053
Training Progress: 	Epoch 29 [7040/8883 (79.14%)]		Loss: 0.90021
Training Progress: 	Epoch 29 [7360/8883 (82.73%)]		Loss: 0.87951
Training Progress: 	Epoch 29 [7680/8883 (86.33%)]		Loss: 0.93119
Training Progress: 	Epoch 29 [8000/8883 (89.93%)]		Loss: 0.77419
Training Progress: 	Epoch 29 [8320/8883 (93.53%)]		Loss: 0.90679
Training Progress: 	Epoch 29 [8640/8883 (97.12%)]		Loss: 0.94495
	Train loss: 0.02357, Accuracy: 5893/8883 (66.00%)
	Validation loss: 0.00049, Accuracy: 1122/1692 (66.00%)
	Test loss: 0.00111, Accuracy: 478/1772 (26.00%)

Training Progress: 	Epoch 30 [0/8883 (0.00%)]		Loss: 0.86931
Training Progress: 	Epoch 30 [320/8883 (3.60%)]		Loss: 1.05915
Training Progress: 	Epoch 30 [640/8883 (7.19%)]		Loss: 0.83533
Training Progress: 	Epoch 30 [960/8883 (10.79%)]		Loss: 0.91731
Training Progress: 	Epoch 30 [1280/8883 (14.39%)]		Loss: 0.97783
Training Progress: 	Epoch 30 [1600/8883 (17.99%)]		Loss: 1.00277
Training Progress: 	Epoch 30 [1920/8883 (21.58%)]		Loss: 0.80991
Training Progress: 	Epoch 30 [2240/8883 (25.18%)]		Loss: 0.78937
Training Progress: 	Epoch 30 [2560/8883 (28.78%)]		Loss: 0.91740
Training Progress: 	Epoch 30 [2880/8883 (32.37%)]		Loss: 0.95272
Training Progress: 	Epoch 30 [3200/8883 (35.97%)]		Loss: 1.03527
Training Progress: 	Epoch 30 [3520/8883 (39.57%)]		Loss: 0.92590
Training Progress: 	Epoch 30 [3840/8883 (43.17%)]		Loss: 0.94144
Training Progress: 	Epoch 30 [4160/8883 (46.76%)]		Loss: 0.85647
Training Progress: 	Epoch 30 [4480/8883 (50.36%)]		Loss: 0.74070
Training Progress: 	Epoch 30 [4800/8883 (53.96%)]		Loss: 0.83689
Training Progress: 	Epoch 30 [5120/8883 (57.55%)]		Loss: 0.98608
Training Progress: 	Epoch 30 [5440/8883 (61.15%)]		Loss: 0.94487
Training Progress: 	Epoch 30 [5760/8883 (64.75%)]		Loss: 0.71853
Training Progress: 	Epoch 30 [6080/8883 (68.35%)]		Loss: 0.70451
Training Progress: 	Epoch 30 [6400/8883 (71.94%)]		Loss: 0.90704
Training Progress: 	Epoch 30 [6720/8883 (75.54%)]		Loss: 0.94657
Training Progress: 	Epoch 30 [7040/8883 (79.14%)]		Loss: 0.92263
Training Progress: 	Epoch 30 [7360/8883 (82.73%)]		Loss: 0.85090
Training Progress: 	Epoch 30 [7680/8883 (86.33%)]		Loss: 0.68377
Training Progress: 	Epoch 30 [8000/8883 (89.93%)]		Loss: 0.73459
Training Progress: 	Epoch 30 [8320/8883 (93.53%)]		Loss: 1.19831
Training Progress: 	Epoch 30 [8640/8883 (97.12%)]		Loss: 0.83525
	Train loss: 0.02341, Accuracy: 5891/8883 (66.00%)
	Validation loss: 0.00049, Accuracy: 1156/1692 (68.00%)
	Test loss: 0.00121, Accuracy: 485/1772 (27.00%)

Training Progress: 	Epoch 31 [0/8883 (0.00%)]		Loss: 0.87104
Training Progress: 	Epoch 31 [320/8883 (3.60%)]		Loss: 1.00672
Training Progress: 	Epoch 31 [640/8883 (7.19%)]		Loss: 0.82107
Training Progress: 	Epoch 31 [960/8883 (10.79%)]		Loss: 1.05854
Training Progress: 	Epoch 31 [1280/8883 (14.39%)]		Loss: 0.92924
Training Progress: 	Epoch 31 [1600/8883 (17.99%)]		Loss: 1.20447
Training Progress: 	Epoch 31 [1920/8883 (21.58%)]		Loss: 0.63884
Training Progress: 	Epoch 31 [2240/8883 (25.18%)]		Loss: 0.93042
Training Progress: 	Epoch 31 [2560/8883 (28.78%)]		Loss: 0.96572
Training Progress: 	Epoch 31 [2880/8883 (32.37%)]		Loss: 0.94661
Training Progress: 	Epoch 31 [3200/8883 (35.97%)]		Loss: 0.80083
Training Progress: 	Epoch 31 [3520/8883 (39.57%)]		Loss: 1.12348
Training Progress: 	Epoch 31 [3840/8883 (43.17%)]		Loss: 0.83939
Training Progress: 	Epoch 31 [4160/8883 (46.76%)]		Loss: 0.77598
Training Progress: 	Epoch 31 [4480/8883 (50.36%)]		Loss: 0.75086
Training Progress: 	Epoch 31 [4800/8883 (53.96%)]		Loss: 0.71648
Training Progress: 	Epoch 31 [5120/8883 (57.55%)]		Loss: 0.84255
Training Progress: 	Epoch 31 [5440/8883 (61.15%)]		Loss: 0.87377
Training Progress: 	Epoch 31 [5760/8883 (64.75%)]		Loss: 0.83757
Training Progress: 	Epoch 31 [6080/8883 (68.35%)]		Loss: 1.02067
Training Progress: 	Epoch 31 [6400/8883 (71.94%)]		Loss: 0.94755
Training Progress: 	Epoch 31 [6720/8883 (75.54%)]		Loss: 0.88258
Training Progress: 	Epoch 31 [7040/8883 (79.14%)]		Loss: 0.86708
Training Progress: 	Epoch 31 [7360/8883 (82.73%)]		Loss: 0.76559
Training Progress: 	Epoch 31 [7680/8883 (86.33%)]		Loss: 0.75738
Training Progress: 	Epoch 31 [8000/8883 (89.93%)]		Loss: 0.65948
Training Progress: 	Epoch 31 [8320/8883 (93.53%)]		Loss: 1.02982
Training Progress: 	Epoch 31 [8640/8883 (97.12%)]		Loss: 0.98937
	Train loss: 0.02191, Accuracy: 6115/8883 (68.00%)
	Validation loss: 0.00043, Accuracy: 1187/1692 (70.00%)
	Test loss: 0.00112, Accuracy: 539/1772 (30.00%)

Training Progress: 	Epoch 32 [0/8883 (0.00%)]		Loss: 0.70272
Training Progress: 	Epoch 32 [320/8883 (3.60%)]		Loss: 0.88453
Training Progress: 	Epoch 32 [640/8883 (7.19%)]		Loss: 0.87027
Training Progress: 	Epoch 32 [960/8883 (10.79%)]		Loss: 0.81328
Training Progress: 	Epoch 32 [1280/8883 (14.39%)]		Loss: 0.91915
Training Progress: 	Epoch 32 [1600/8883 (17.99%)]		Loss: 0.92151
Training Progress: 	Epoch 32 [1920/8883 (21.58%)]		Loss: 0.86155
Training Progress: 	Epoch 32 [2240/8883 (25.18%)]		Loss: 0.79666
Training Progress: 	Epoch 32 [2560/8883 (28.78%)]		Loss: 1.09304
Training Progress: 	Epoch 32 [2880/8883 (32.37%)]		Loss: 1.05796
Training Progress: 	Epoch 32 [3200/8883 (35.97%)]		Loss: 0.77822
Training Progress: 	Epoch 32 [3520/8883 (39.57%)]		Loss: 0.85908
Training Progress: 	Epoch 32 [3840/8883 (43.17%)]		Loss: 0.90575
Training Progress: 	Epoch 32 [4160/8883 (46.76%)]		Loss: 0.73497
Training Progress: 	Epoch 32 [4480/8883 (50.36%)]		Loss: 0.61699
Training Progress: 	Epoch 32 [4800/8883 (53.96%)]		Loss: 0.52291
Training Progress: 	Epoch 32 [5120/8883 (57.55%)]		Loss: 0.92664
Training Progress: 	Epoch 32 [5440/8883 (61.15%)]		Loss: 0.83974
Training Progress: 	Epoch 32 [5760/8883 (64.75%)]		Loss: 0.73410
Training Progress: 	Epoch 32 [6080/8883 (68.35%)]		Loss: 1.06766
Training Progress: 	Epoch 32 [6400/8883 (71.94%)]		Loss: 0.84973
Training Progress: 	Epoch 32 [6720/8883 (75.54%)]		Loss: 0.86461
Training Progress: 	Epoch 32 [7040/8883 (79.14%)]		Loss: 0.89252
Training Progress: 	Epoch 32 [7360/8883 (82.73%)]		Loss: 0.87987
Training Progress: 	Epoch 32 [7680/8883 (86.33%)]		Loss: 0.72921
Training Progress: 	Epoch 32 [8000/8883 (89.93%)]		Loss: 0.88979
Training Progress: 	Epoch 32 [8320/8883 (93.53%)]		Loss: 0.98678
Training Progress: 	Epoch 32 [8640/8883 (97.12%)]		Loss: 0.97964
	Train loss: 0.02163, Accuracy: 6074/8883 (68.00%)
	Validation loss: 0.00044, Accuracy: 1178/1692 (69.00%)
	Test loss: 0.00123, Accuracy: 522/1772 (29.00%)

Training Progress: 	Epoch 33 [0/8883 (0.00%)]		Loss: 0.79072
Training Progress: 	Epoch 33 [320/8883 (3.60%)]		Loss: 0.85045
Training Progress: 	Epoch 33 [640/8883 (7.19%)]		Loss: 0.80137
Training Progress: 	Epoch 33 [960/8883 (10.79%)]		Loss: 0.88844
Training Progress: 	Epoch 33 [1280/8883 (14.39%)]		Loss: 0.80798
Training Progress: 	Epoch 33 [1600/8883 (17.99%)]		Loss: 1.05369
Training Progress: 	Epoch 33 [1920/8883 (21.58%)]		Loss: 0.66606
Training Progress: 	Epoch 33 [2240/8883 (25.18%)]		Loss: 0.76890
Training Progress: 	Epoch 33 [2560/8883 (28.78%)]		Loss: 1.06043
Training Progress: 	Epoch 33 [2880/8883 (32.37%)]		Loss: 0.96373
Training Progress: 	Epoch 33 [3200/8883 (35.97%)]		Loss: 0.95817
Training Progress: 	Epoch 33 [3520/8883 (39.57%)]		Loss: 0.94884
Training Progress: 	Epoch 33 [3840/8883 (43.17%)]		Loss: 0.90443
Training Progress: 	Epoch 33 [4160/8883 (46.76%)]		Loss: 0.65610
Training Progress: 	Epoch 33 [4480/8883 (50.36%)]		Loss: 0.70898
Training Progress: 	Epoch 33 [4800/8883 (53.96%)]		Loss: 0.60515
Training Progress: 	Epoch 33 [5120/8883 (57.55%)]		Loss: 0.81759
Training Progress: 	Epoch 33 [5440/8883 (61.15%)]		Loss: 0.63808
Training Progress: 	Epoch 33 [5760/8883 (64.75%)]		Loss: 0.69845
Training Progress: 	Epoch 33 [6080/8883 (68.35%)]		Loss: 0.71599
Training Progress: 	Epoch 33 [6400/8883 (71.94%)]		Loss: 0.82915
Training Progress: 	Epoch 33 [6720/8883 (75.54%)]		Loss: 0.80423
Training Progress: 	Epoch 33 [7040/8883 (79.14%)]		Loss: 0.85106
Training Progress: 	Epoch 33 [7360/8883 (82.73%)]		Loss: 0.76595
Training Progress: 	Epoch 33 [7680/8883 (86.33%)]		Loss: 0.79355
Training Progress: 	Epoch 33 [8000/8883 (89.93%)]		Loss: 0.93422
Training Progress: 	Epoch 33 [8320/8883 (93.53%)]		Loss: 0.93842
Training Progress: 	Epoch 33 [8640/8883 (97.12%)]		Loss: 0.79055
	Train loss: 0.02145, Accuracy: 6142/8883 (69.00%)
	Validation loss: 0.00043, Accuracy: 1192/1692 (70.00%)
	Test loss: 0.00116, Accuracy: 528/1772 (29.00%)

Training Progress: 	Epoch 34 [0/8883 (0.00%)]		Loss: 0.66106
Training Progress: 	Epoch 34 [320/8883 (3.60%)]		Loss: 0.78748
Training Progress: 	Epoch 34 [640/8883 (7.19%)]		Loss: 0.73648
Training Progress: 	Epoch 34 [960/8883 (10.79%)]		Loss: 0.72141
Training Progress: 	Epoch 34 [1280/8883 (14.39%)]		Loss: 0.98323
Training Progress: 	Epoch 34 [1600/8883 (17.99%)]		Loss: 1.12724
Training Progress: 	Epoch 34 [1920/8883 (21.58%)]		Loss: 0.63906
Training Progress: 	Epoch 34 [2240/8883 (25.18%)]		Loss: 0.83609
Training Progress: 	Epoch 34 [2560/8883 (28.78%)]		Loss: 1.01789
Training Progress: 	Epoch 34 [2880/8883 (32.37%)]		Loss: 1.07821
Training Progress: 	Epoch 34 [3200/8883 (35.97%)]		Loss: 0.74811
Training Progress: 	Epoch 34 [3520/8883 (39.57%)]		Loss: 0.91159
Training Progress: 	Epoch 34 [3840/8883 (43.17%)]		Loss: 1.16495
Training Progress: 	Epoch 34 [4160/8883 (46.76%)]		Loss: 0.80499
Training Progress: 	Epoch 34 [4480/8883 (50.36%)]		Loss: 0.52356
Training Progress: 	Epoch 34 [4800/8883 (53.96%)]		Loss: 0.76039Training Progress: 	Epoch 34 [5120/8883 (57.55%)]		Loss: 0.75100
Training Progress: 	Epoch 34 [5440/8883 (61.15%)]		Loss: 0.72243
Training Progress: 	Epoch 34 [5760/8883 (64.75%)]		Loss: 0.58396
Training Progress: 	Epoch 34 [6080/8883 (68.35%)]		Loss: 0.75127
Training Progress: 	Epoch 34 [6400/8883 (71.94%)]		Loss: 0.78804
Training Progress: 	Epoch 34 [6720/8883 (75.54%)]		Loss: 1.00033
Training Progress: 	Epoch 34 [7040/8883 (79.14%)]		Loss: 0.81268
Training Progress: 	Epoch 34 [7360/8883 (82.73%)]		Loss: 0.85481
Training Progress: 	Epoch 34 [7680/8883 (86.33%)]		Loss: 0.75942
Training Progress: 	Epoch 34 [8000/8883 (89.93%)]		Loss: 0.66129
Training Progress: 	Epoch 34 [8320/8883 (93.53%)]		Loss: 0.92035
Training Progress: 	Epoch 34 [8640/8883 (97.12%)]		Loss: 0.92081
	Train loss: 0.02107, Accuracy: 6153/8883 (69.00%)
	Validation loss: 0.00044, Accuracy: 1191/1692 (70.00%)
	Test loss: 0.00127, Accuracy: 510/1772 (28.00%)

Training Progress: 	Epoch 35 [0/8883 (0.00%)]		Loss: 0.85510
Training Progress: 	Epoch 35 [320/8883 (3.60%)]		Loss: 0.84116
Training Progress: 	Epoch 35 [640/8883 (7.19%)]		Loss: 0.70203
Training Progress: 	Epoch 35 [960/8883 (10.79%)]		Loss: 0.61363
Training Progress: 	Epoch 35 [1280/8883 (14.39%)]		Loss: 1.14918
Training Progress: 	Epoch 35 [1600/8883 (17.99%)]		Loss: 0.97238
Training Progress: 	Epoch 35 [1920/8883 (21.58%)]		Loss: 0.82480
Training Progress: 	Epoch 35 [2240/8883 (25.18%)]		Loss: 0.82407
Training Progress: 	Epoch 35 [2560/8883 (28.78%)]		Loss: 0.81322
Training Progress: 	Epoch 35 [2880/8883 (32.37%)]		Loss: 0.71355
Training Progress: 	Epoch 35 [3200/8883 (35.97%)]		Loss: 0.70830
Training Progress: 	Epoch 35 [3520/8883 (39.57%)]		Loss: 0.79220
Training Progress: 	Epoch 35 [3840/8883 (43.17%)]		Loss: 0.88587
Training Progress: 	Epoch 35 [4160/8883 (46.76%)]		Loss: 0.69900
Training Progress: 	Epoch 35 [4480/8883 (50.36%)]		Loss: 0.70457
Training Progress: 	Epoch 35 [4800/8883 (53.96%)]		Loss: 0.64284
Training Progress: 	Epoch 35 [5120/8883 (57.55%)]		Loss: 0.86431
Training Progress: 	Epoch 35 [5440/8883 (61.15%)]		Loss: 0.80357
Training Progress: 	Epoch 35 [5760/8883 (64.75%)]		Loss: 0.70214
Training Progress: 	Epoch 35 [6080/8883 (68.35%)]		Loss: 0.76921
Training Progress: 	Epoch 35 [6400/8883 (71.94%)]		Loss: 0.65066
Training Progress: 	Epoch 35 [6720/8883 (75.54%)]		Loss: 0.76348
Training Progress: 	Epoch 35 [7040/8883 (79.14%)]		Loss: 0.95366
Training Progress: 	Epoch 35 [7360/8883 (82.73%)]		Loss: 0.75242
Training Progress: 	Epoch 35 [7680/8883 (86.33%)]		Loss: 0.68287
Training Progress: 	Epoch 35 [8000/8883 (89.93%)]		Loss: 0.65807
Training Progress: 	Epoch 35 [8320/8883 (93.53%)]		Loss: 0.87381
Training Progress: 	Epoch 35 [8640/8883 (97.12%)]		Loss: 0.86255
	Train loss: 0.02122, Accuracy: 6098/8883 (68.00%)
	Validation loss: 0.00045, Accuracy: 1198/1692 (70.00%)
	Test loss: 0.00129, Accuracy: 531/1772 (29.00%)

Training Progress: 	Epoch 36 [0/8883 (0.00%)]		Loss: 0.73194
Training Progress: 	Epoch 36 [320/8883 (3.60%)]		Loss: 0.68483
Training Progress: 	Epoch 36 [640/8883 (7.19%)]		Loss: 0.67241
Training Progress: 	Epoch 36 [960/8883 (10.79%)]		Loss: 0.69541
Training Progress: 	Epoch 36 [1280/8883 (14.39%)]		Loss: 0.84390
Training Progress: 	Epoch 36 [1600/8883 (17.99%)]		Loss: 0.86122
Training Progress: 	Epoch 36 [1920/8883 (21.58%)]		Loss: 0.76850
Training Progress: 	Epoch 36 [2240/8883 (25.18%)]		Loss: 0.81659
Training Progress: 	Epoch 36 [2560/8883 (28.78%)]		Loss: 0.98581
Training Progress: 	Epoch 36 [2880/8883 (32.37%)]		Loss: 1.04782
Training Progress: 	Epoch 36 [3200/8883 (35.97%)]		Loss: 0.79337
Training Progress: 	Epoch 36 [3520/8883 (39.57%)]		Loss: 0.85495
Training Progress: 	Epoch 36 [3840/8883 (43.17%)]		Loss: 0.92272
Training Progress: 	Epoch 36 [4160/8883 (46.76%)]		Loss: 0.85872
Training Progress: 	Epoch 36 [4480/8883 (50.36%)]		Loss: 0.67357
Training Progress: 	Epoch 36 [4800/8883 (53.96%)]		Loss: 0.60641
Training Progress: 	Epoch 36 [5120/8883 (57.55%)]		Loss: 0.73490
Training Progress: 	Epoch 36 [5440/8883 (61.15%)]		Loss: 0.82050
Training Progress: 	Epoch 36 [5760/8883 (64.75%)]		Loss: 0.71780
Training Progress: 	Epoch 36 [6080/8883 (68.35%)]		Loss: 0.58540
Training Progress: 	Epoch 36 [6400/8883 (71.94%)]		Loss: 0.73590
Training Progress: 	Epoch 36 [6720/8883 (75.54%)]		Loss: 0.81787
Training Progress: 	Epoch 36 [7040/8883 (79.14%)]		Loss: 0.88225
Training Progress: 	Epoch 36 [7360/8883 (82.73%)]		Loss: 0.69614
Training Progress: 	Epoch 36 [7680/8883 (86.33%)]		Loss: 0.59406
Training Progress: 	Epoch 36 [8000/8883 (89.93%)]		Loss: 0.59157
Training Progress: 	Epoch 36 [8320/8883 (93.53%)]		Loss: 1.11537
Training Progress: 	Epoch 36 [8640/8883 (97.12%)]		Loss: 0.81279
	Train loss: 0.02023, Accuracy: 6308/8883 (71.00%)
	Validation loss: 0.00040, Accuracy: 1242/1692 (73.00%)
	Test loss: 0.00126, Accuracy: 497/1772 (28.00%)

Training Progress: 	Epoch 37 [0/8883 (0.00%)]		Loss: 0.71467
Training Progress: 	Epoch 37 [320/8883 (3.60%)]		Loss: 0.74964
Training Progress: 	Epoch 37 [640/8883 (7.19%)]		Loss: 0.72884
Training Progress: 	Epoch 37 [960/8883 (10.79%)]		Loss: 0.60432
Training Progress: 	Epoch 37 [1280/8883 (14.39%)]		Loss: 0.82418
Training Progress: 	Epoch 37 [1600/8883 (17.99%)]		Loss: 0.81254
Training Progress: 	Epoch 37 [1920/8883 (21.58%)]		Loss: 0.63303
Training Progress: 	Epoch 37 [2240/8883 (25.18%)]		Loss: 0.74347
Training Progress: 	Epoch 37 [2560/8883 (28.78%)]		Loss: 0.96732
Training Progress: 	Epoch 37 [2880/8883 (32.37%)]		Loss: 0.86573
Training Progress: 	Epoch 37 [3200/8883 (35.97%)]		Loss: 0.77342
Training Progress: 	Epoch 37 [3520/8883 (39.57%)]		Loss: 0.81363
Training Progress: 	Epoch 37 [3840/8883 (43.17%)]		Loss: 0.95940
Training Progress: 	Epoch 37 [4160/8883 (46.76%)]		Loss: 0.76536
Training Progress: 	Epoch 37 [4480/8883 (50.36%)]		Loss: 0.64671
Training Progress: 	Epoch 37 [4800/8883 (53.96%)]		Loss: 0.61087
Training Progress: 	Epoch 37 [5120/8883 (57.55%)]		Loss: 0.67255
Training Progress: 	Epoch 37 [5440/8883 (61.15%)]		Loss: 0.68751
Training Progress: 	Epoch 37 [5760/8883 (64.75%)]		Loss: 0.84526
Training Progress: 	Epoch 37 [6080/8883 (68.35%)]		Loss: 0.73895
Training Progress: 	Epoch 37 [6400/8883 (71.94%)]		Loss: 0.74683
Training Progress: 	Epoch 37 [6720/8883 (75.54%)]		Loss: 0.75412
Training Progress: 	Epoch 37 [7040/8883 (79.14%)]		Loss: 0.81476
Training Progress: 	Epoch 37 [7360/8883 (82.73%)]		Loss: 0.99320
Training Progress: 	Epoch 37 [7680/8883 (86.33%)]		Loss: 0.62632
Training Progress: 	Epoch 37 [8000/8883 (89.93%)]		Loss: 0.76100
Training Progress: 	Epoch 37 [8320/8883 (93.53%)]		Loss: 0.92850
Training Progress: 	Epoch 37 [8640/8883 (97.12%)]		Loss: 0.78863
	Train loss: 0.01985, Accuracy: 6321/8883 (71.00%)
	Validation loss: 0.00041, Accuracy: 1220/1692 (72.00%)
	Test loss: 0.00124, Accuracy: 520/1772 (29.00%)

Training Progress: 	Epoch 38 [0/8883 (0.00%)]		Loss: 0.70965
Training Progress: 	Epoch 38 [320/8883 (3.60%)]		Loss: 0.84779
Training Progress: 	Epoch 38 [640/8883 (7.19%)]		Loss: 0.87280
Training Progress: 	Epoch 38 [960/8883 (10.79%)]		Loss: 0.60477
Training Progress: 	Epoch 38 [1280/8883 (14.39%)]		Loss: 0.85285
Training Progress: 	Epoch 38 [1600/8883 (17.99%)]		Loss: 0.69034
Training Progress: 	Epoch 38 [1920/8883 (21.58%)]		Loss: 0.76455
Training Progress: 	Epoch 38 [2240/8883 (25.18%)]		Loss: 0.73281
Training Progress: 	Epoch 38 [2560/8883 (28.78%)]		Loss: 0.97724
Training Progress: 	Epoch 38 [2880/8883 (32.37%)]		Loss: 0.75551
Training Progress: 	Epoch 38 [3200/8883 (35.97%)]		Loss: 0.79182
Training Progress: 	Epoch 38 [3520/8883 (39.57%)]		Loss: 0.94623
Training Progress: 	Epoch 38 [3840/8883 (43.17%)]		Loss: 0.97676
Training Progress: 	Epoch 38 [4160/8883 (46.76%)]		Loss: 0.55539
Training Progress: 	Epoch 38 [4480/8883 (50.36%)]		Loss: 0.72728
Training Progress: 	Epoch 38 [4800/8883 (53.96%)]		Loss: 0.68704
Training Progress: 	Epoch 38 [5120/8883 (57.55%)]		Loss: 0.65799
Training Progress: 	Epoch 38 [5440/8883 (61.15%)]		Loss: 0.84851
Training Progress: 	Epoch 38 [5760/8883 (64.75%)]		Loss: 0.56556
Training Progress: 	Epoch 38 [6080/8883 (68.35%)]		Loss: 0.56089
Training Progress: 	Epoch 38 [6400/8883 (71.94%)]		Loss: 0.84173
Training Progress: 	Epoch 38 [6720/8883 (75.54%)]		Loss: 0.81697
Training Progress: 	Epoch 38 [7040/8883 (79.14%)]		Loss: 0.99258
Training Progress: 	Epoch 38 [7360/8883 (82.73%)]		Loss: 0.86498
Training Progress: 	Epoch 38 [7680/8883 (86.33%)]		Loss: 0.70790
Training Progress: 	Epoch 38 [8000/8883 (89.93%)]		Loss: 0.69426
Training Progress: 	Epoch 38 [8320/8883 (93.53%)]		Loss: 1.07307
Training Progress: 	Epoch 38 [8640/8883 (97.12%)]		Loss: 0.91162
	Train loss: 0.01929, Accuracy: 6409/8883 (72.00%)
	Validation loss: 0.00038, Accuracy: 1271/1692 (75.00%)
	Test loss: 0.00127, Accuracy: 529/1772 (29.00%)

Training Progress: 	Epoch 39 [0/8883 (0.00%)]		Loss: 0.78442
Training Progress: 	Epoch 39 [320/8883 (3.60%)]		Loss: 0.64980
Training Progress: 	Epoch 39 [640/8883 (7.19%)]		Loss: 0.73845
Training Progress: 	Epoch 39 [960/8883 (10.79%)]		Loss: 0.57182
Training Progress: 	Epoch 39 [1280/8883 (14.39%)]		Loss: 0.95065
Training Progress: 	Epoch 39 [1600/8883 (17.99%)]		Loss: 1.02742
Training Progress: 	Epoch 39 [1920/8883 (21.58%)]		Loss: 0.53708
Training Progress: 	Epoch 39 [2240/8883 (25.18%)]		Loss: 0.93570
Training Progress: 	Epoch 39 [2560/8883 (28.78%)]		Loss: 0.94638
Training Progress: 	Epoch 39 [2880/8883 (32.37%)]		Loss: 1.00606
Training Progress: 	Epoch 39 [3200/8883 (35.97%)]		Loss: 0.62423
Training Progress: 	Epoch 39 [3520/8883 (39.57%)]		Loss: 0.80812
Training Progress: 	Epoch 39 [3840/8883 (43.17%)]		Loss: 1.06795
Training Progress: 	Epoch 39 [4160/8883 (46.76%)]		Loss: 0.59143
Training Progress: 	Epoch 39 [4480/8883 (50.36%)]		Loss: 0.58420
Training Progress: 	Epoch 39 [4800/8883 (53.96%)]		Loss: 0.57998
Training Progress: 	Epoch 39 [5120/8883 (57.55%)]		Loss: 0.89679
Training Progress: 	Epoch 39 [5440/8883 (61.15%)]		Loss: 0.60571
Training Progress: 	Epoch 39 [5760/8883 (64.75%)]		Loss: 0.57046
Training Progress: 	Epoch 39 [6080/8883 (68.35%)]		Loss: 0.74816
Training Progress: 	Epoch 39 [6400/8883 (71.94%)]		Loss: 0.79111
Training Progress: 	Epoch 39 [6720/8883 (75.54%)]		Loss: 0.62503
Training Progress: 	Epoch 39 [7040/8883 (79.14%)]		Loss: 0.79501
Training Progress: 	Epoch 39 [7360/8883 (82.73%)]		Loss: 0.72197
Training Progress: 	Epoch 39 [7680/8883 (86.33%)]		Loss: 0.59390
Training Progress: 	Epoch 39 [8000/8883 (89.93%)]		Loss: 0.62334
Training Progress: 	Epoch 39 [8320/8883 (93.53%)]		Loss: 0.73562
Training Progress: 	Epoch 39 [8640/8883 (97.12%)]		Loss: 0.74587
	Train loss: 0.01881, Accuracy: 6419/8883 (72.00%)
	Validation loss: 0.00039, Accuracy: 1260/1692 (74.00%)
	Test loss: 0.00133, Accuracy: 492/1772 (27.00%)

Training Progress: 	Epoch 40 [0/8883 (0.00%)]		Loss: 0.66970
Training Progress: 	Epoch 40 [320/8883 (3.60%)]		Loss: 0.84125
Training Progress: 	Epoch 40 [640/8883 (7.19%)]		Loss: 0.66230
Training Progress: 	Epoch 40 [960/8883 (10.79%)]		Loss: 0.71447
Training Progress: 	Epoch 40 [1280/8883 (14.39%)]		Loss: 0.95487
Training Progress: 	Epoch 40 [1600/8883 (17.99%)]		Loss: 0.79881
Training Progress: 	Epoch 40 [1920/8883 (21.58%)]		Loss: 0.49841
Training Progress: 	Epoch 40 [2240/8883 (25.18%)]		Loss: 0.79227
Training Progress: 	Epoch 40 [2560/8883 (28.78%)]		Loss: 0.78649
Training Progress: 	Epoch 40 [2880/8883 (32.37%)]		Loss: 0.77065
Training Progress: 	Epoch 40 [3200/8883 (35.97%)]		Loss: 0.94152
Training Progress: 	Epoch 40 [3520/8883 (39.57%)]		Loss: 0.71246
Training Progress: 	Epoch 40 [3840/8883 (43.17%)]		Loss: 0.98966
Training Progress: 	Epoch 40 [4160/8883 (46.76%)]		Loss: 0.53265
Training Progress: 	Epoch 40 [4480/8883 (50.36%)]		Loss: 0.65062
Training Progress: 	Epoch 40 [4800/8883 (53.96%)]		Loss: 0.42361
Training Progress: 	Epoch 40 [5120/8883 (57.55%)]		Loss: 0.68082
Training Progress: 	Epoch 40 [5440/8883 (61.15%)]		Loss: 0.69580
Training Progress: 	Epoch 40 [5760/8883 (64.75%)]		Loss: 0.58140
Training Progress: 	Epoch 40 [6080/8883 (68.35%)]		Loss: 0.82260
Training Progress: 	Epoch 40 [6400/8883 (71.94%)]		Loss: 0.50605
Training Progress: 	Epoch 40 [6720/8883 (75.54%)]		Loss: 0.76232
Training Progress: 	Epoch 40 [7040/8883 (79.14%)]		Loss: 0.78942
Training Progress: 	Epoch 40 [7360/8883 (82.73%)]		Loss: 0.64939
Training Progress: 	Epoch 40 [7680/8883 (86.33%)]		Loss: 0.76245
Training Progress: 	Epoch 40 [8000/8883 (89.93%)]		Loss: 0.60384
Training Progress: 	Epoch 40 [8320/8883 (93.53%)]		Loss: 0.88222
Training Progress: 	Epoch 40 [8640/8883 (97.12%)]		Loss: 0.82116
	Train loss: 0.01851, Accuracy: 6470/8883 (72.00%)
	Validation loss: 0.00038, Accuracy: 1272/1692 (75.00%)
	Test loss: 0.00133, Accuracy: 530/1772 (29.00%)

Training Progress: 	Epoch 41 [0/8883 (0.00%)]		Loss: 0.69145
Training Progress: 	Epoch 41 [320/8883 (3.60%)]		Loss: 0.81290
Training Progress: 	Epoch 41 [640/8883 (7.19%)]		Loss: 0.71119
Training Progress: 	Epoch 41 [960/8883 (10.79%)]		Loss: 0.78818
Training Progress: 	Epoch 41 [1280/8883 (14.39%)]		Loss: 0.88391
Training Progress: 	Epoch 41 [1600/8883 (17.99%)]		Loss: 0.67554
Training Progress: 	Epoch 41 [1920/8883 (21.58%)]		Loss: 0.70711
Training Progress: 	Epoch 41 [2240/8883 (25.18%)]		Loss: 0.61631
Training Progress: 	Epoch 41 [2560/8883 (28.78%)]		Loss: 0.91342
Training Progress: 	Epoch 41 [2880/8883 (32.37%)]		Loss: 0.68822
Training Progress: 	Epoch 41 [3200/8883 (35.97%)]		Loss: 0.76518
Training Progress: 	Epoch 41 [3520/8883 (39.57%)]		Loss: 0.90067
Training Progress: 	Epoch 41 [3840/8883 (43.17%)]		Loss: 0.79965
Training Progress: 	Epoch 41 [4160/8883 (46.76%)]		Loss: 0.56291
Training Progress: 	Epoch 41 [4480/8883 (50.36%)]		Loss: 0.66392
Training Progress: 	Epoch 41 [4800/8883 (53.96%)]		Loss: 0.45525
Training Progress: 	Epoch 41 [5120/8883 (57.55%)]		Loss: 0.70633
Training Progress: 	Epoch 41 [5440/8883 (61.15%)]		Loss: 0.60287
Training Progress: 	Epoch 41 [5760/8883 (64.75%)]		Loss: 0.62104
Training Progress: 	Epoch 41 [6080/8883 (68.35%)]		Loss: 0.68783
Training Progress: 	Epoch 41 [6400/8883 (71.94%)]		Loss: 0.52849
Training Progress: 	Epoch 41 [6720/8883 (75.54%)]		Loss: 0.64371
Training Progress: 	Epoch 41 [7040/8883 (79.14%)]		Loss: 0.79388
Training Progress: 	Epoch 41 [7360/8883 (82.73%)]		Loss: 0.70771
Training Progress: 	Epoch 41 [7680/8883 (86.33%)]		Loss: 0.65683
Training Progress: 	Epoch 41 [8000/8883 (89.93%)]		Loss: 0.52328
Training Progress: 	Epoch 41 [8320/8883 (93.53%)]		Loss: 0.98283
Training Progress: 	Epoch 41 [8640/8883 (97.12%)]		Loss: 0.71015
	Train loss: 0.01760, Accuracy: 6588/8883 (74.00%)
	Validation loss: 0.00034, Accuracy: 1335/1692 (78.00%)
	Test loss: 0.00134, Accuracy: 531/1772 (29.00%)

Training Progress: 	Epoch 42 [0/8883 (0.00%)]		Loss: 0.65105
Training Progress: 	Epoch 42 [320/8883 (3.60%)]		Loss: 0.57415
Training Progress: 	Epoch 42 [640/8883 (7.19%)]		Loss: 0.54648
Training Progress: 	Epoch 42 [960/8883 (10.79%)]		Loss: 0.67530
Training Progress: 	Epoch 42 [1280/8883 (14.39%)]		Loss: 0.80411
Training Progress: 	Epoch 42 [1600/8883 (17.99%)]		Loss: 0.78040
Training Progress: 	Epoch 42 [1920/8883 (21.58%)]		Loss: 0.55332
Training Progress: 	Epoch 42 [2240/8883 (25.18%)]		Loss: 0.88712
Training Progress: 	Epoch 42 [2560/8883 (28.78%)]		Loss: 0.95558
Training Progress: 	Epoch 42 [2880/8883 (32.37%)]		Loss: 0.77638
Training Progress: 	Epoch 42 [3200/8883 (35.97%)]		Loss: 0.63490
Training Progress: 	Epoch 42 [3520/8883 (39.57%)]		Loss: 0.78417
Training Progress: 	Epoch 42 [3840/8883 (43.17%)]		Loss: 0.76615
Training Progress: 	Epoch 42 [4160/8883 (46.76%)]		Loss: 0.56213
Training Progress: 	Epoch 42 [4480/8883 (50.36%)]		Loss: 0.66654
Training Progress: 	Epoch 42 [4800/8883 (53.96%)]		Loss: 0.54957
Training Progress: 	Epoch 42 [5120/8883 (57.55%)]		Loss: 0.63908
Training Progress: 	Epoch 42 [5440/8883 (61.15%)]		Loss: 0.64084
Training Progress: 	Epoch 42 [5760/8883 (64.75%)]		Loss: 0.61356
Training Progress: 	Epoch 42 [6080/8883 (68.35%)]		Loss: 0.56699
Training Progress: 	Epoch 42 [6400/8883 (71.94%)]		Loss: 0.63260
Training Progress: 	Epoch 42 [6720/8883 (75.54%)]		Loss: 0.82137
Training Progress: 	Epoch 42 [7040/8883 (79.14%)]		Loss: 0.87892
Training Progress: 	Epoch 42 [7360/8883 (82.73%)]		Loss: 0.66915
Training Progress: 	Epoch 42 [7680/8883 (86.33%)]		Loss: 0.64834
Training Progress: 	Epoch 42 [8000/8883 (89.93%)]		Loss: 0.60839
Training Progress: 	Epoch 42 [8320/8883 (93.53%)]		Loss: 0.81019
Training Progress: 	Epoch 42 [8640/8883 (97.12%)]		Loss: 0.71966
	Train loss: 0.01842, Accuracy: 6503/8883 (73.00%)
	Validation loss: 0.00036, Accuracy: 1289/1692 (76.00%)
	Test loss: 0.00136, Accuracy: 531/1772 (29.00%)

Training Progress: 	Epoch 43 [0/8883 (0.00%)]		Loss: 0.59585
Training Progress: 	Epoch 43 [320/8883 (3.60%)]		Loss: 0.79473
Training Progress: 	Epoch 43 [640/8883 (7.19%)]		Loss: 0.52393
Training Progress: 	Epoch 43 [960/8883 (10.79%)]		Loss: 0.83093
Training Progress: 	Epoch 43 [1280/8883 (14.39%)]		Loss: 0.78995
Training Progress: 	Epoch 43 [1600/8883 (17.99%)]		Loss: 0.49403
Training Progress: 	Epoch 43 [1920/8883 (21.58%)]		Loss: 0.63807
Training Progress: 	Epoch 43 [2240/8883 (25.18%)]		Loss: 0.71934
Training Progress: 	Epoch 43 [2560/8883 (28.78%)]		Loss: 1.07503
Training Progress: 	Epoch 43 [2880/8883 (32.37%)]		Loss: 0.72481
Training Progress: 	Epoch 43 [3200/8883 (35.97%)]		Loss: 0.90617
Training Progress: 	Epoch 43 [3520/8883 (39.57%)]		Loss: 0.81947
Training Progress: 	Epoch 43 [3840/8883 (43.17%)]		Loss: 0.85677
Training Progress: 	Epoch 43 [4160/8883 (46.76%)]		Loss: 0.57554
Training Progress: 	Epoch 43 [4480/8883 (50.36%)]		Loss: 0.67865
Training Progress: 	Epoch 43 [4800/8883 (53.96%)]		Loss: 0.52330
Training Progress: 	Epoch 43 [5120/8883 (57.55%)]		Loss: 0.90129
Training Progress: 	Epoch 43 [5440/8883 (61.15%)]		Loss: 0.56072
Training Progress: 	Epoch 43 [5760/8883 (64.75%)]		Loss: 0.65723
Training Progress: 	Epoch 43 [6080/8883 (68.35%)]		Loss: 0.59274
Training Progress: 	Epoch 43 [6400/8883 (71.94%)]		Loss: 0.66575
Training Progress: 	Epoch 43 [6720/8883 (75.54%)]		Loss: 0.75885
Training Progress: 	Epoch 43 [7040/8883 (79.14%)]		Loss: 1.14871
Training Progress: 	Epoch 43 [7360/8883 (82.73%)]		Loss: 0.82382
Training Progress: 	Epoch 43 [7680/8883 (86.33%)]		Loss: 0.63173
Training Progress: 	Epoch 43 [8000/8883 (89.93%)]		Loss: 0.74093
Training Progress: 	Epoch 43 [8320/8883 (93.53%)]		Loss: 1.03659
Training Progress: 	Epoch 43 [8640/8883 (97.12%)]		Loss: 0.57774
	Train loss: 0.01749, Accuracy: 6614/8883 (74.00%)
	Validation loss: 0.00034, Accuracy: 1334/1692 (78.00%)
	Test loss: 0.00135, Accuracy: 503/1772 (28.00%)

Training Progress: 	Epoch 44 [0/8883 (0.00%)]		Loss: 0.61135
Training Progress: 	Epoch 44 [320/8883 (3.60%)]		Loss: 0.91730
Training Progress: 	Epoch 44 [640/8883 (7.19%)]		Loss: 0.59104
Training Progress: 	Epoch 44 [960/8883 (10.79%)]		Loss: 0.68294
Training Progress: 	Epoch 44 [1280/8883 (14.39%)]		Loss: 0.58409
Training Progress: 	Epoch 44 [1600/8883 (17.99%)]		Loss: 0.62667
Training Progress: 	Epoch 44 [1920/8883 (21.58%)]		Loss: 0.62571
Training Progress: 	Epoch 44 [2240/8883 (25.18%)]		Loss: 0.66269
Training Progress: 	Epoch 44 [2560/8883 (28.78%)]		Loss: 0.84550
Training Progress: 	Epoch 44 [2880/8883 (32.37%)]		Loss: 0.83807
Training Progress: 	Epoch 44 [3200/8883 (35.97%)]		Loss: 0.91587
Training Progress: 	Epoch 44 [3520/8883 (39.57%)]		Loss: 0.91275
Training Progress: 	Epoch 44 [3840/8883 (43.17%)]		Loss: 0.82671
Training Progress: 	Epoch 44 [4160/8883 (46.76%)]		Loss: 0.68423
Training Progress: 	Epoch 44 [4480/8883 (50.36%)]		Loss: 0.55654
Training Progress: 	Epoch 44 [4800/8883 (53.96%)]		Loss: 0.65588
Training Progress: 	Epoch 44 [5120/8883 (57.55%)]		Loss: 0.60556
Training Progress: 	Epoch 44 [5440/8883 (61.15%)]		Loss: 0.60851
Training Progress: 	Epoch 44 [5760/8883 (64.75%)]		Loss: 0.71653
Training Progress: 	Epoch 44 [6080/8883 (68.35%)]		Loss: 0.62301
Training Progress: 	Epoch 44 [6400/8883 (71.94%)]		Loss: 0.60651
Training Progress: 	Epoch 44 [6720/8883 (75.54%)]		Loss: 0.72399
Training Progress: 	Epoch 44 [7040/8883 (79.14%)]		Loss: 0.82418
Training Progress: 	Epoch 44 [7360/8883 (82.73%)]		Loss: 0.62233
Training Progress: 	Epoch 44 [7680/8883 (86.33%)]		Loss: 0.91303
Training Progress: 	Epoch 44 [8000/8883 (89.93%)]		Loss: 0.67875
Training Progress: 	Epoch 44 [8320/8883 (93.53%)]		Loss: 0.91387
Training Progress: 	Epoch 44 [8640/8883 (97.12%)]		Loss: 0.81113
	Train loss: 0.01833, Accuracy: 6487/8883 (73.00%)
	Validation loss: 0.00037, Accuracy: 1306/1692 (77.00%)
	Test loss: 0.00140, Accuracy: 522/1772 (29.00%)

Training Progress: 	Epoch 45 [0/8883 (0.00%)]		Loss: 0.64103
Training Progress: 	Epoch 45 [320/8883 (3.60%)]		Loss: 0.80904
Training Progress: 	Epoch 45 [640/8883 (7.19%)]		Loss: 0.59945
Training Progress: 	Epoch 45 [960/8883 (10.79%)]		Loss: 0.64755
Training Progress: 	Epoch 45 [1280/8883 (14.39%)]		Loss: 0.64869
Training Progress: 	Epoch 45 [1600/8883 (17.99%)]		Loss: 0.67492
Training Progress: 	Epoch 45 [1920/8883 (21.58%)]		Loss: 0.58386
Training Progress: 	Epoch 45 [2240/8883 (25.18%)]		Loss: 0.55372
Training Progress: 	Epoch 45 [2560/8883 (28.78%)]		Loss: 1.05770
Training Progress: 	Epoch 45 [2880/8883 (32.37%)]		Loss: 0.72046
Training Progress: 	Epoch 45 [3200/8883 (35.97%)]		Loss: 0.59347
Training Progress: 	Epoch 45 [3520/8883 (39.57%)]		Loss: 0.67998
Training Progress: 	Epoch 45 [3840/8883 (43.17%)]		Loss: 0.83554
Training Progress: 	Epoch 45 [4160/8883 (46.76%)]		Loss: 0.51333
Training Progress: 	Epoch 45 [4480/8883 (50.36%)]		Loss: 0.50345
Training Progress: 	Epoch 45 [4800/8883 (53.96%)]		Loss: 0.36359
Training Progress: 	Epoch 45 [5120/8883 (57.55%)]		Loss: 0.68301
Training Progress: 	Epoch 45 [5440/8883 (61.15%)]		Loss: 1.08194
Training Progress: 	Epoch 45 [5760/8883 (64.75%)]		Loss: 0.59396
Training Progress: 	Epoch 45 [6080/8883 (68.35%)]		Loss: 0.60322
Training Progress: 	Epoch 45 [6400/8883 (71.94%)]		Loss: 0.68760
Training Progress: 	Epoch 45 [6720/8883 (75.54%)]		Loss: 0.62679
Training Progress: 	Epoch 45 [7040/8883 (79.14%)]		Loss: 0.88827
Training Progress: 	Epoch 45 [7360/8883 (82.73%)]		Loss: 0.64268
Training Progress: 	Epoch 45 [7680/8883 (86.33%)]		Loss: 0.65082
Training Progress: 	Epoch 45 [8000/8883 (89.93%)]		Loss: 0.84238
Training Progress: 	Epoch 45 [8320/8883 (93.53%)]		Loss: 0.78455
Training Progress: 	Epoch 45 [8640/8883 (97.12%)]		Loss: 0.78388
	Train loss: 0.01687, Accuracy: 6650/8883 (74.00%)
	Validation loss: 0.00033, Accuracy: 1331/1692 (78.00%)
	Test loss: 0.00138, Accuracy: 545/1772 (30.00%)

Training Progress: 	Epoch 46 [0/8883 (0.00%)]		Loss: 0.81581
Training Progress: 	Epoch 46 [320/8883 (3.60%)]		Loss: 0.91983
Training Progress: 	Epoch 46 [640/8883 (7.19%)]		Loss: 0.47805
Training Progress: 	Epoch 46 [960/8883 (10.79%)]		Loss: 0.58924
Training Progress: 	Epoch 46 [1280/8883 (14.39%)]		Loss: 0.81629
Training Progress: 	Epoch 46 [1600/8883 (17.99%)]		Loss: 0.90601
Training Progress: 	Epoch 46 [1920/8883 (21.58%)]		Loss: 0.56435
Training Progress: 	Epoch 46 [2240/8883 (25.18%)]		Loss: 0.69696
Training Progress: 	Epoch 46 [2560/8883 (28.78%)]		Loss: 0.96204
Training Progress: 	Epoch 46 [2880/8883 (32.37%)]		Loss: 0.80272
Training Progress: 	Epoch 46 [3200/8883 (35.97%)]		Loss: 0.69147
Training Progress: 	Epoch 46 [3520/8883 (39.57%)]		Loss: 0.83044
Training Progress: 	Epoch 46 [3840/8883 (43.17%)]		Loss: 0.82587
Training Progress: 	Epoch 46 [4160/8883 (46.76%)]		Loss: 0.43483
Training Progress: 	Epoch 46 [4480/8883 (50.36%)]		Loss: 0.72330
Training Progress: 	Epoch 46 [4800/8883 (53.96%)]		Loss: 0.58017
Training Progress: 	Epoch 46 [5120/8883 (57.55%)]		Loss: 0.56644
Training Progress: 	Epoch 46 [5440/8883 (61.15%)]		Loss: 0.62122
Training Progress: 	Epoch 46 [5760/8883 (64.75%)]		Loss: 0.72932
Training Progress: 	Epoch 46 [6080/8883 (68.35%)]		Loss: 0.69885
Training Progress: 	Epoch 46 [6400/8883 (71.94%)]		Loss: 0.76972
Training Progress: 	Epoch 46 [6720/8883 (75.54%)]		Loss: 0.70125
Training Progress: 	Epoch 46 [7040/8883 (79.14%)]		Loss: 0.74121
Training Progress: 	Epoch 46 [7360/8883 (82.73%)]		Loss: 0.74931
Training Progress: 	Epoch 46 [7680/8883 (86.33%)]		Loss: 0.72818
Training Progress: 	Epoch 46 [8000/8883 (89.93%)]		Loss: 0.55327
Training Progress: 	Epoch 46 [8320/8883 (93.53%)]		Loss: 0.89024
Training Progress: 	Epoch 46 [8640/8883 (97.12%)]		Loss: 0.74168
	Train loss: 0.01673, Accuracy: 6687/8883 (75.00%)
	Validation loss: 0.00033, Accuracy: 1343/1692 (79.00%)
	Test loss: 0.00137, Accuracy: 536/1772 (30.00%)

Training Progress: 	Epoch 47 [0/8883 (0.00%)]		Loss: 0.79129
Training Progress: 	Epoch 47 [320/8883 (3.60%)]		Loss: 0.75602
Training Progress: 	Epoch 47 [640/8883 (7.19%)]		Loss: 0.64217
Training Progress: 	Epoch 47 [960/8883 (10.79%)]		Loss: 0.71545
Training Progress: 	Epoch 47 [1280/8883 (14.39%)]		Loss: 0.63425
Training Progress: 	Epoch 47 [1600/8883 (17.99%)]		Loss: 0.50295
Training Progress: 	Epoch 47 [1920/8883 (21.58%)]		Loss: 0.51061
Training Progress: 	Epoch 47 [2240/8883 (25.18%)]		Loss: 0.77862
Training Progress: 	Epoch 47 [2560/8883 (28.78%)]		Loss: 0.80617
Training Progress: 	Epoch 47 [2880/8883 (32.37%)]		Loss: 0.55249
Training Progress: 	Epoch 47 [3200/8883 (35.97%)]		Loss: 0.81854
Training Progress: 	Epoch 47 [3520/8883 (39.57%)]		Loss: 0.71119
Training Progress: 	Epoch 47 [3840/8883 (43.17%)]		Loss: 0.62694
Training Progress: 	Epoch 47 [4160/8883 (46.76%)]		Loss: 0.49651
Training Progress: 	Epoch 47 [4480/8883 (50.36%)]		Loss: 0.54077
Training Progress: 	Epoch 47 [4800/8883 (53.96%)]		Loss: 0.39911
Training Progress: 	Epoch 47 [5120/8883 (57.55%)]		Loss: 0.60891
Training Progress: 	Epoch 47 [5440/8883 (61.15%)]		Loss: 0.77228
Training Progress: 	Epoch 47 [5760/8883 (64.75%)]		Loss: 0.84040
Training Progress: 	Epoch 47 [6080/8883 (68.35%)]		Loss: 0.58943
Training Progress: 	Epoch 47 [6400/8883 (71.94%)]		Loss: 0.55694
Training Progress: 	Epoch 47 [6720/8883 (75.54%)]		Loss: 0.56921
Training Progress: 	Epoch 47 [7040/8883 (79.14%)]		Loss: 0.63420
Training Progress: 	Epoch 47 [7360/8883 (82.73%)]		Loss: 0.73956
Training Progress: 	Epoch 47 [7680/8883 (86.33%)]		Loss: 0.57874
Training Progress: 	Epoch 47 [8000/8883 (89.93%)]		Loss: 0.84904
Training Progress: 	Epoch 47 [8320/8883 (93.53%)]		Loss: 0.78308
Training Progress: 	Epoch 47 [8640/8883 (97.12%)]		Loss: 0.75272
	Train loss: 0.01654, Accuracy: 6674/8883 (75.00%)
	Validation loss: 0.00032, Accuracy: 1350/1692 (79.00%)
	Test loss: 0.00144, Accuracy: 522/1772 (29.00%)

Training Progress: 	Epoch 48 [0/8883 (0.00%)]		Loss: 0.57060
Training Progress: 	Epoch 48 [320/8883 (3.60%)]		Loss: 0.81993
Training Progress: 	Epoch 48 [640/8883 (7.19%)]		Loss: 0.43736
Training Progress: 	Epoch 48 [960/8883 (10.79%)]		Loss: 0.52274
Training Progress: 	Epoch 48 [1280/8883 (14.39%)]		Loss: 0.69386
Training Progress: 	Epoch 48 [1600/8883 (17.99%)]		Loss: 0.47845
Training Progress: 	Epoch 48 [1920/8883 (21.58%)]		Loss: 0.65940
Training Progress: 	Epoch 48 [2240/8883 (25.18%)]		Loss: 0.56571
Training Progress: 	Epoch 48 [2560/8883 (28.78%)]		Loss: 0.85797
Training Progress: 	Epoch 48 [2880/8883 (32.37%)]		Loss: 0.68173
Training Progress: 	Epoch 48 [3200/8883 (35.97%)]		Loss: 0.61130
Training Progress: 	Epoch 48 [3520/8883 (39.57%)]		Loss: 0.88692
Training Progress: 	Epoch 48 [3840/8883 (43.17%)]		Loss: 0.68995
Training Progress: 	Epoch 48 [4160/8883 (46.76%)]		Loss: 0.56701
Training Progress: 	Epoch 48 [4480/8883 (50.36%)]		Loss: 0.40781
Training Progress: 	Epoch 48 [4800/8883 (53.96%)]		Loss: 0.61434
Training Progress: 	Epoch 48 [5120/8883 (57.55%)]		Loss: 0.73903
Training Progress: 	Epoch 48 [5440/8883 (61.15%)]		Loss: 0.60151
Training Progress: 	Epoch 48 [5760/8883 (64.75%)]		Loss: 0.51105
Training Progress: 	Epoch 48 [6080/8883 (68.35%)]		Loss: 0.52496
Training Progress: 	Epoch 48 [6400/8883 (71.94%)]		Loss: 0.76200
Training Progress: 	Epoch 48 [6720/8883 (75.54%)]		Loss: 0.67625
Training Progress: 	Epoch 48 [7040/8883 (79.14%)]		Loss: 0.71859
Training Progress: 	Epoch 48 [7360/8883 (82.73%)]		Loss: 0.63371
Training Progress: 	Epoch 48 [7680/8883 (86.33%)]		Loss: 0.80530
Training Progress: 	Epoch 48 [8000/8883 (89.93%)]		Loss: 0.61613
Training Progress: 	Epoch 48 [8320/8883 (93.53%)]		Loss: 0.72896
Training Progress: 	Epoch 48 [8640/8883 (97.12%)]		Loss: 0.62553
	Train loss: 0.01611, Accuracy: 6729/8883 (75.00%)
	Validation loss: 0.00030, Accuracy: 1373/1692 (81.00%)
	Test loss: 0.00142, Accuracy: 502/1772 (28.00%)

Training Progress: 	Epoch 49 [0/8883 (0.00%)]		Loss: 0.70264
Training Progress: 	Epoch 49 [320/8883 (3.60%)]		Loss: 0.72008
Training Progress: 	Epoch 49 [640/8883 (7.19%)]		Loss: 0.45387
Training Progress: 	Epoch 49 [960/8883 (10.79%)]		Loss: 0.61663
Training Progress: 	Epoch 49 [1280/8883 (14.39%)]		Loss: 0.74748
Training Progress: 	Epoch 49 [1600/8883 (17.99%)]		Loss: 0.83803
Training Progress: 	Epoch 49 [1920/8883 (21.58%)]		Loss: 0.63291
Training Progress: 	Epoch 49 [2240/8883 (25.18%)]		Loss: 0.84764
Training Progress: 	Epoch 49 [2560/8883 (28.78%)]		Loss: 0.62461
Training Progress: 	Epoch 49 [2880/8883 (32.37%)]		Loss: 0.82506
Training Progress: 	Epoch 49 [3200/8883 (35.97%)]		Loss: 0.52699
Training Progress: 	Epoch 49 [3520/8883 (39.57%)]		Loss: 0.84930
Training Progress: 	Epoch 49 [3840/8883 (43.17%)]		Loss: 0.83165
Training Progress: 	Epoch 49 [4160/8883 (46.76%)]		Loss: 0.41486
Training Progress: 	Epoch 49 [4480/8883 (50.36%)]		Loss: 0.32976
Training Progress: 	Epoch 49 [4800/8883 (53.96%)]		Loss: 0.46291
Training Progress: 	Epoch 49 [5120/8883 (57.55%)]		Loss: 0.74853
Training Progress: 	Epoch 49 [5440/8883 (61.15%)]		Loss: 0.57786
Training Progress: 	Epoch 49 [5760/8883 (64.75%)]		Loss: 0.79770
Training Progress: 	Epoch 49 [6080/8883 (68.35%)]		Loss: 0.56591
Training Progress: 	Epoch 49 [6400/8883 (71.94%)]		Loss: 0.60655
Training Progress: 	Epoch 49 [6720/8883 (75.54%)]		Loss: 0.59642
Training Progress: 	Epoch 49 [7040/8883 (79.14%)]		Loss: 0.73570
Training Progress: 	Epoch 49 [7360/8883 (82.73%)]		Loss: 0.61856
Training Progress: 	Epoch 49 [7680/8883 (86.33%)]		Loss: 0.64583
Training Progress: 	Epoch 49 [8000/8883 (89.93%)]		Loss: 0.45577
Training Progress: 	Epoch 49 [8320/8883 (93.53%)]		Loss: 0.79573
Training Progress: 	Epoch 49 [8640/8883 (97.12%)]		Loss: 0.78095
	Train loss: 0.01586, Accuracy: 6768/8883 (76.00%)
	Validation loss: 0.00030, Accuracy: 1367/1692 (80.00%)
	Test loss: 0.00142, Accuracy: 539/1772 (30.00%)

Training Progress: 	Epoch 50 [0/8883 (0.00%)]		Loss: 0.49152
Training Progress: 	Epoch 50 [320/8883 (3.60%)]		Loss: 0.68791
Training Progress: 	Epoch 50 [640/8883 (7.19%)]		Loss: 0.95568
Training Progress: 	Epoch 50 [960/8883 (10.79%)]		Loss: 0.58131
Training Progress: 	Epoch 50 [1280/8883 (14.39%)]		Loss: 0.59391
Training Progress: 	Epoch 50 [1600/8883 (17.99%)]		Loss: 0.59474
Training Progress: 	Epoch 50 [1920/8883 (21.58%)]		Loss: 0.52324
Training Progress: 	Epoch 50 [2240/8883 (25.18%)]		Loss: 0.61681
Training Progress: 	Epoch 50 [2560/8883 (28.78%)]		Loss: 0.68693
Training Progress: 	Epoch 50 [2880/8883 (32.37%)]		Loss: 0.69904
Training Progress: 	Epoch 50 [3200/8883 (35.97%)]		Loss: 0.50302
Training Progress: 	Epoch 50 [3520/8883 (39.57%)]		Loss: 0.99362
Training Progress: 	Epoch 50 [3840/8883 (43.17%)]		Loss: 0.74347
Training Progress: 	Epoch 50 [4160/8883 (46.76%)]		Loss: 0.62791
Training Progress: 	Epoch 50 [4480/8883 (50.36%)]		Loss: 0.43455
Training Progress: 	Epoch 50 [4800/8883 (53.96%)]		Loss: 0.51378
Training Progress: 	Epoch 50 [5120/8883 (57.55%)]		Loss: 0.60295
Training Progress: 	Epoch 50 [5440/8883 (61.15%)]		Loss: 0.63547
Training Progress: 	Epoch 50 [5760/8883 (64.75%)]		Loss: 0.51929
Training Progress: 	Epoch 50 [6080/8883 (68.35%)]		Loss: 0.49503
Training Progress: 	Epoch 50 [6400/8883 (71.94%)]		Loss: 0.46179
Training Progress: 	Epoch 50 [6720/8883 (75.54%)]		Loss: 0.86717
Training Progress: 	Epoch 50 [7040/8883 (79.14%)]		Loss: 0.72839
Training Progress: 	Epoch 50 [7360/8883 (82.73%)]		Loss: 0.90452
Training Progress: 	Epoch 50 [7680/8883 (86.33%)]		Loss: 0.56231
Training Progress: 	Epoch 50 [8000/8883 (89.93%)]		Loss: 0.62422
Training Progress: 	Epoch 50 [8320/8883 (93.53%)]		Loss: 0.77668
Training Progress: 	Epoch 50 [8640/8883 (97.12%)]		Loss: 0.78914
	Train loss: 0.01584, Accuracy: 6756/8883 (76.00%)
	Validation loss: 0.00031, Accuracy: 1362/1692 (80.00%)
	Test loss: 0.00148, Accuracy: 520/1772 (29.00%)

Training Progress: 	Epoch 51 [0/8883 (0.00%)]		Loss: 0.53507
Training Progress: 	Epoch 51 [320/8883 (3.60%)]		Loss: 0.57857
Training Progress: 	Epoch 51 [640/8883 (7.19%)]		Loss: 0.66945
Training Progress: 	Epoch 51 [960/8883 (10.79%)]		Loss: 0.71373
Training Progress: 	Epoch 51 [1280/8883 (14.39%)]		Loss: 0.80204
Training Progress: 	Epoch 51 [1600/8883 (17.99%)]		Loss: 0.46259
Training Progress: 	Epoch 51 [1920/8883 (21.58%)]		Loss: 0.59116
Training Progress: 	Epoch 51 [2240/8883 (25.18%)]		Loss: 0.66343
Training Progress: 	Epoch 51 [2560/8883 (28.78%)]		Loss: 0.67385
Training Progress: 	Epoch 51 [2880/8883 (32.37%)]		Loss: 0.77433
Training Progress: 	Epoch 51 [3200/8883 (35.97%)]		Loss: 0.74914
Training Progress: 	Epoch 51 [3520/8883 (39.57%)]		Loss: 0.64551
Training Progress: 	Epoch 51 [3840/8883 (43.17%)]		Loss: 0.70375
Training Progress: 	Epoch 51 [4160/8883 (46.76%)]		Loss: 0.66185
Training Progress: 	Epoch 51 [4480/8883 (50.36%)]		Loss: 0.56566
Training Progress: 	Epoch 51 [4800/8883 (53.96%)]		Loss: 0.41941
Training Progress: 	Epoch 51 [5120/8883 (57.55%)]		Loss: 0.67330
Training Progress: 	Epoch 51 [5440/8883 (61.15%)]		Loss: 0.58773
Training Progress: 	Epoch 51 [5760/8883 (64.75%)]		Loss: 0.43492
Training Progress: 	Epoch 51 [6080/8883 (68.35%)]		Loss: 0.49125
Training Progress: 	Epoch 51 [6400/8883 (71.94%)]		Loss: 0.60017
Training Progress: 	Epoch 51 [6720/8883 (75.54%)]		Loss: 0.74888
Training Progress: 	Epoch 51 [7040/8883 (79.14%)]		Loss: 0.80982
Training Progress: 	Epoch 51 [7360/8883 (82.73%)]		Loss: 0.69810
Training Progress: 	Epoch 51 [7680/8883 (86.33%)]		Loss: 0.59142
Training Progress: 	Epoch 51 [8000/8883 (89.93%)]		Loss: 0.61010
Training Progress: 	Epoch 51 [8320/8883 (93.53%)]		Loss: 0.67524
Training Progress: 	Epoch 51 [8640/8883 (97.12%)]		Loss: 0.74286
	Train loss: 0.01583, Accuracy: 6774/8883 (76.00%)
	Validation loss: 0.00031, Accuracy: 1377/1692 (81.00%)
	Test loss: 0.00148, Accuracy: 538/1772 (30.00%)

Training Progress: 	Epoch 52 [0/8883 (0.00%)]		Loss: 0.83738
Training Progress: 	Epoch 52 [320/8883 (3.60%)]		Loss: 0.58264
Training Progress: 	Epoch 52 [640/8883 (7.19%)]		Loss: 0.51679
Training Progress: 	Epoch 52 [960/8883 (10.79%)]		Loss: 0.55407
Training Progress: 	Epoch 52 [1280/8883 (14.39%)]		Loss: 0.53902
Training Progress: 	Epoch 52 [1600/8883 (17.99%)]		Loss: 0.57651
Training Progress: 	Epoch 52 [1920/8883 (21.58%)]		Loss: 0.41681
Training Progress: 	Epoch 52 [2240/8883 (25.18%)]		Loss: 0.57644
Training Progress: 	Epoch 52 [2560/8883 (28.78%)]		Loss: 0.67407
Training Progress: 	Epoch 52 [2880/8883 (32.37%)]		Loss: 0.68926
Training Progress: 	Epoch 52 [3200/8883 (35.97%)]		Loss: 0.79908
Training Progress: 	Epoch 52 [3520/8883 (39.57%)]		Loss: 0.67362
Training Progress: 	Epoch 52 [3840/8883 (43.17%)]		Loss: 0.78768
Training Progress: 	Epoch 52 [4160/8883 (46.76%)]		Loss: 0.48015
Training Progress: 	Epoch 52 [4480/8883 (50.36%)]		Loss: 0.36201
Training Progress: 	Epoch 52 [4800/8883 (53.96%)]		Loss: 0.46124
Training Progress: 	Epoch 52 [5120/8883 (57.55%)]		Loss: 0.84804
Training Progress: 	Epoch 52 [5440/8883 (61.15%)]		Loss: 0.67846
Training Progress: 	Epoch 52 [5760/8883 (64.75%)]		Loss: 0.71290
Training Progress: 	Epoch 52 [6080/8883 (68.35%)]		Loss: 0.51403
Training Progress: 	Epoch 52 [6400/8883 (71.94%)]		Loss: 0.56369
Training Progress: 	Epoch 52 [6720/8883 (75.54%)]		Loss: 0.72030
Training Progress: 	Epoch 52 [7040/8883 (79.14%)]		Loss: 0.80082
Training Progress: 	Epoch 52 [7360/8883 (82.73%)]		Loss: 0.61153
Training Progress: 	Epoch 52 [7680/8883 (86.33%)]		Loss: 0.51691
Training Progress: 	Epoch 52 [8000/8883 (89.93%)]		Loss: 0.46083
Training Progress: 	Epoch 52 [8320/8883 (93.53%)]		Loss: 0.73924
Training Progress: 	Epoch 52 [8640/8883 (97.12%)]		Loss: 0.61484
	Train loss: 0.01616, Accuracy: 6687/8883 (75.00%)
	Validation loss: 0.00032, Accuracy: 1361/1692 (80.00%)
	Test loss: 0.00152, Accuracy: 513/1772 (28.00%)

Training Progress: 	Epoch 53 [0/8883 (0.00%)]		Loss: 0.48420
Training Progress: 	Epoch 53 [320/8883 (3.60%)]		Loss: 0.62459
Training Progress: 	Epoch 53 [640/8883 (7.19%)]		Loss: 0.70641
Training Progress: 	Epoch 53 [960/8883 (10.79%)]		Loss: 0.57276
Training Progress: 	Epoch 53 [1280/8883 (14.39%)]		Loss: 0.74107
Training Progress: 	Epoch 53 [1600/8883 (17.99%)]		Loss: 0.49605
Training Progress: 	Epoch 53 [1920/8883 (21.58%)]		Loss: 0.46553
Training Progress: 	Epoch 53 [2240/8883 (25.18%)]		Loss: 0.79267
Training Progress: 	Epoch 53 [2560/8883 (28.78%)]		Loss: 0.93690
Training Progress: 	Epoch 53 [2880/8883 (32.37%)]		Loss: 0.79138
Training Progress: 	Epoch 53 [3200/8883 (35.97%)]		Loss: 0.57388
Training Progress: 	Epoch 53 [3520/8883 (39.57%)]		Loss: 0.73013
Training Progress: 	Epoch 53 [3840/8883 (43.17%)]		Loss: 0.96703
Training Progress: 	Epoch 53 [4160/8883 (46.76%)]		Loss: 0.45985
Training Progress: 	Epoch 53 [4480/8883 (50.36%)]		Loss: 0.36038
Training Progress: 	Epoch 53 [4800/8883 (53.96%)]		Loss: 0.49359
Training Progress: 	Epoch 53 [5120/8883 (57.55%)]		Loss: 0.71112
Training Progress: 	Epoch 53 [5440/8883 (61.15%)]		Loss: 0.67395
Training Progress: 	Epoch 53 [5760/8883 (64.75%)]		Loss: 0.58408
Training Progress: 	Epoch 53 [6080/8883 (68.35%)]		Loss: 0.38427
Training Progress: 	Epoch 53 [6400/8883 (71.94%)]		Loss: 0.55464
Training Progress: 	Epoch 53 [6720/8883 (75.54%)]		Loss: 0.80052
Training Progress: 	Epoch 53 [7040/8883 (79.14%)]		Loss: 0.69117
Training Progress: 	Epoch 53 [7360/8883 (82.73%)]		Loss: 0.54374
Training Progress: 	Epoch 53 [7680/8883 (86.33%)]		Loss: 0.50127
Training Progress: 	Epoch 53 [8000/8883 (89.93%)]		Loss: 0.44851
Training Progress: 	Epoch 53 [8320/8883 (93.53%)]		Loss: 0.83463
Training Progress: 	Epoch 53 [8640/8883 (97.12%)]		Loss: 0.63317
	Train loss: 0.01477, Accuracy: 6882/8883 (77.00%)
	Validation loss: 0.00027, Accuracy: 1414/1692 (83.00%)
	Test loss: 0.00153, Accuracy: 516/1772 (29.00%)

Training Progress: 	Epoch 54 [0/8883 (0.00%)]		Loss: 0.56191
Training Progress: 	Epoch 54 [320/8883 (3.60%)]		Loss: 0.60081
Training Progress: 	Epoch 54 [640/8883 (7.19%)]		Loss: 0.62269
Training Progress: 	Epoch 54 [960/8883 (10.79%)]		Loss: 0.54607
Training Progress: 	Epoch 54 [1280/8883 (14.39%)]		Loss: 0.62510
Training Progress: 	Epoch 54 [1600/8883 (17.99%)]		Loss: 0.57723
Training Progress: 	Epoch 54 [1920/8883 (21.58%)]		Loss: 0.56949
Training Progress: 	Epoch 54 [2240/8883 (25.18%)]		Loss: 0.71434
Training Progress: 	Epoch 54 [2560/8883 (28.78%)]		Loss: 0.75758
Training Progress: 	Epoch 54 [2880/8883 (32.37%)]		Loss: 0.67037
Training Progress: 	Epoch 54 [3200/8883 (35.97%)]		Loss: 0.56252
Training Progress: 	Epoch 54 [3520/8883 (39.57%)]		Loss: 0.65069
Training Progress: 	Epoch 54 [3840/8883 (43.17%)]		Loss: 0.88185
Training Progress: 	Epoch 54 [4160/8883 (46.76%)]		Loss: 0.41566
Training Progress: 	Epoch 54 [4480/8883 (50.36%)]		Loss: 0.62118
Training Progress: 	Epoch 54 [4800/8883 (53.96%)]		Loss: 0.44800
Training Progress: 	Epoch 54 [5120/8883 (57.55%)]		Loss: 0.73772
Training Progress: 	Epoch 54 [5440/8883 (61.15%)]		Loss: 0.75036
Training Progress: 	Epoch 54 [5760/8883 (64.75%)]		Loss: 0.64394
Training Progress: 	Epoch 54 [6080/8883 (68.35%)]		Loss: 0.31356
Training Progress: 	Epoch 54 [6400/8883 (71.94%)]		Loss: 0.69852
Training Progress: 	Epoch 54 [6720/8883 (75.54%)]		Loss: 0.78287
Training Progress: 	Epoch 54 [7040/8883 (79.14%)]		Loss: 0.69916
Training Progress: 	Epoch 54 [7360/8883 (82.73%)]		Loss: 0.68336
Training Progress: 	Epoch 54 [7680/8883 (86.33%)]		Loss: 0.59784
Training Progress: 	Epoch 54 [8000/8883 (89.93%)]		Loss: 0.42620
Training Progress: 	Epoch 54 [8320/8883 (93.53%)]		Loss: 0.65948
Training Progress: 	Epoch 54 [8640/8883 (97.12%)]		Loss: 0.65172
	Train loss: 0.01507, Accuracy: 6832/8883 (76.00%)
	Validation loss: 0.00029, Accuracy: 1394/1692 (82.00%)
	Test loss: 0.00151, Accuracy: 532/1772 (30.00%)

Training Progress: 	Epoch 55 [0/8883 (0.00%)]		Loss: 0.67839
Training Progress: 	Epoch 55 [320/8883 (3.60%)]		Loss: 0.70701
Training Progress: 	Epoch 55 [640/8883 (7.19%)]		Loss: 0.61267
Training Progress: 	Epoch 55 [960/8883 (10.79%)]		Loss: 0.78174
Training Progress: 	Epoch 55 [1280/8883 (14.39%)]		Loss: 0.54942
Training Progress: 	Epoch 55 [1600/8883 (17.99%)]		Loss: 0.43896
Training Progress: 	Epoch 55 [1920/8883 (21.58%)]		Loss: 0.53842
Training Progress: 	Epoch 55 [2240/8883 (25.18%)]		Loss: 0.60801
Training Progress: 	Epoch 55 [2560/8883 (28.78%)]		Loss: 0.76035
Training Progress: 	Epoch 55 [2880/8883 (32.37%)]		Loss: 0.88394
Training Progress: 	Epoch 55 [3200/8883 (35.97%)]		Loss: 0.48184
Training Progress: 	Epoch 55 [3520/8883 (39.57%)]		Loss: 0.84213
Training Progress: 	Epoch 55 [3840/8883 (43.17%)]		Loss: 0.72502
Training Progress: 	Epoch 55 [4160/8883 (46.76%)]		Loss: 0.50042
Training Progress: 	Epoch 55 [4480/8883 (50.36%)]		Loss: 0.57833
Training Progress: 	Epoch 55 [4800/8883 (53.96%)]		Loss: 0.45745
Training Progress: 	Epoch 55 [5120/8883 (57.55%)]		Loss: 0.69192
Training Progress: 	Epoch 55 [5440/8883 (61.15%)]		Loss: 0.57472
Training Progress: 	Epoch 55 [5760/8883 (64.75%)]		Loss: 0.48914
Training Progress: 	Epoch 55 [6080/8883 (68.35%)]		Loss: 0.45776
Training Progress: 	Epoch 55 [6400/8883 (71.94%)]		Loss: 0.75358
Training Progress: 	Epoch 55 [6720/8883 (75.54%)]		Loss: 0.73701
Training Progress: 	Epoch 55 [7040/8883 (79.14%)]		Loss: 0.76071
Training Progress: 	Epoch 55 [7360/8883 (82.73%)]		Loss: 0.62727
Training Progress: 	Epoch 55 [7680/8883 (86.33%)]		Loss: 0.56881
Training Progress: 	Epoch 55 [8000/8883 (89.93%)]		Loss: 0.54937
Training Progress: 	Epoch 55 [8320/8883 (93.53%)]		Loss: 0.86741
Training Progress: 	Epoch 55 [8640/8883 (97.12%)]		Loss: 0.60810
	Train loss: 0.01510, Accuracy: 6845/8883 (77.00%)
	Validation loss: 0.00030, Accuracy: 1373/1692 (81.00%)
	Test loss: 0.00154, Accuracy: 538/1772 (30.00%)

Training Progress: 	Epoch 56 [0/8883 (0.00%)]		Loss: 0.45029
Training Progress: 	Epoch 56 [320/8883 (3.60%)]		Loss: 0.60562
Training Progress: 	Epoch 56 [640/8883 (7.19%)]		Loss: 0.60350
Training Progress: 	Epoch 56 [960/8883 (10.79%)]		Loss: 0.57532
Training Progress: 	Epoch 56 [1280/8883 (14.39%)]		Loss: 0.48903
Training Progress: 	Epoch 56 [1600/8883 (17.99%)]		Loss: 0.58947
Training Progress: 	Epoch 56 [1920/8883 (21.58%)]		Loss: 0.48879
Training Progress: 	Epoch 56 [2240/8883 (25.18%)]		Loss: 0.73735
Training Progress: 	Epoch 56 [2560/8883 (28.78%)]		Loss: 0.90109
Training Progress: 	Epoch 56 [2880/8883 (32.37%)]		Loss: 0.51286
Training Progress: 	Epoch 56 [3200/8883 (35.97%)]		Loss: 0.70629
Training Progress: 	Epoch 56 [3520/8883 (39.57%)]		Loss: 0.66594
Training Progress: 	Epoch 56 [3840/8883 (43.17%)]		Loss: 0.57627
Training Progress: 	Epoch 56 [4160/8883 (46.76%)]		Loss: 0.46376
Training Progress: 	Epoch 56 [4480/8883 (50.36%)]		Loss: 0.36236
Training Progress: 	Epoch 56 [4800/8883 (53.96%)]		Loss: 0.50159
Training Progress: 	Epoch 56 [5120/8883 (57.55%)]		Loss: 0.56536
Training Progress: 	Epoch 56 [5440/8883 (61.15%)]		Loss: 0.48940
Training Progress: 	Epoch 56 [5760/8883 (64.75%)]		Loss: 0.61983
Training Progress: 	Epoch 56 [6080/8883 (68.35%)]		Loss: 0.40866
Training Progress: 	Epoch 56 [6400/8883 (71.94%)]		Loss: 0.52325
Training Progress: 	Epoch 56 [6720/8883 (75.54%)]		Loss: 0.64376
Training Progress: 	Epoch 56 [7040/8883 (79.14%)]		Loss: 0.70761
Training Progress: 	Epoch 56 [7360/8883 (82.73%)]		Loss: 0.56598
Training Progress: 	Epoch 56 [7680/8883 (86.33%)]		Loss: 0.53343
Training Progress: 	Epoch 56 [8000/8883 (89.93%)]		Loss: 0.58648
Training Progress: 	Epoch 56 [8320/8883 (93.53%)]		Loss: 0.57720
Training Progress: 	Epoch 56 [8640/8883 (97.12%)]		Loss: 0.74241
	Train loss: 0.01525, Accuracy: 6830/8883 (76.00%)
	Validation loss: 0.00031, Accuracy: 1367/1692 (80.00%)
	Test loss: 0.00154, Accuracy: 498/1772 (28.00%)

Training Progress: 	Epoch 57 [0/8883 (0.00%)]		Loss: 0.52461
Training Progress: 	Epoch 57 [320/8883 (3.60%)]		Loss: 0.67009
Training Progress: 	Epoch 57 [640/8883 (7.19%)]		Loss: 0.47937
Training Progress: 	Epoch 57 [960/8883 (10.79%)]		Loss: 0.70620
Training Progress: 	Epoch 57 [1280/8883 (14.39%)]		Loss: 0.67151
Training Progress: 	Epoch 57 [1600/8883 (17.99%)]		Loss: 0.67424
Training Progress: 	Epoch 57 [1920/8883 (21.58%)]		Loss: 0.53609
Training Progress: 	Epoch 57 [2240/8883 (25.18%)]		Loss: 0.79559
Training Progress: 	Epoch 57 [2560/8883 (28.78%)]		Loss: 0.74264
Training Progress: 	Epoch 57 [2880/8883 (32.37%)]		Loss: 0.59353
Training Progress: 	Epoch 57 [3200/8883 (35.97%)]		Loss: 0.63070
Training Progress: 	Epoch 57 [3520/8883 (39.57%)]		Loss: 0.77112
Training Progress: 	Epoch 57 [3840/8883 (43.17%)]		Loss: 0.71963
Training Progress: 	Epoch 57 [4160/8883 (46.76%)]		Loss: 0.55833
Training Progress: 	Epoch 57 [4480/8883 (50.36%)]		Loss: 0.42930
Training Progress: 	Epoch 57 [4800/8883 (53.96%)]		Loss: 0.55404
Training Progress: 	Epoch 57 [5120/8883 (57.55%)]		Loss: 0.72117
Training Progress: 	Epoch 57 [5440/8883 (61.15%)]		Loss: 0.81170
Training Progress: 	Epoch 57 [5760/8883 (64.75%)]		Loss: 0.60653
Training Progress: 	Epoch 57 [6080/8883 (68.35%)]		Loss: 0.39584
Training Progress: 	Epoch 57 [6400/8883 (71.94%)]		Loss: 0.66994
Training Progress: 	Epoch 57 [6720/8883 (75.54%)]		Loss: 0.59628
Training Progress: 	Epoch 57 [7040/8883 (79.14%)]		Loss: 0.71860
Training Progress: 	Epoch 57 [7360/8883 (82.73%)]		Loss: 0.65860
Training Progress: 	Epoch 57 [7680/8883 (86.33%)]		Loss: 0.49364
Training Progress: 	Epoch 57 [8000/8883 (89.93%)]		Loss: 0.50264
Training Progress: 	Epoch 57 [8320/8883 (93.53%)]		Loss: 0.56895
Training Progress: 	Epoch 57 [8640/8883 (97.12%)]		Loss: 0.56334
	Train loss: 0.01449, Accuracy: 6898/8883 (77.00%)
	Validation loss: 0.00026, Accuracy: 1438/1692 (84.00%)
	Test loss: 0.00149, Accuracy: 580/1772 (32.00%)

Training Progress: 	Epoch 58 [0/8883 (0.00%)]		Loss: 0.72614
Training Progress: 	Epoch 58 [320/8883 (3.60%)]		Loss: 0.72467
Training Progress: 	Epoch 58 [640/8883 (7.19%)]		Loss: 0.58034
Training Progress: 	Epoch 58 [960/8883 (10.79%)]		Loss: 0.64548
Training Progress: 	Epoch 58 [1280/8883 (14.39%)]		Loss: 0.65423
Training Progress: 	Epoch 58 [1600/8883 (17.99%)]		Loss: 0.47573
Training Progress: 	Epoch 58 [1920/8883 (21.58%)]		Loss: 0.58270
Training Progress: 	Epoch 58 [2240/8883 (25.18%)]		Loss: 0.65027
Training Progress: 	Epoch 58 [2560/8883 (28.78%)]		Loss: 0.71734
Training Progress: 	Epoch 58 [2880/8883 (32.37%)]		Loss: 0.57859
Training Progress: 	Epoch 58 [3200/8883 (35.97%)]		Loss: 0.50321
Training Progress: 	Epoch 58 [3520/8883 (39.57%)]		Loss: 0.64391
Training Progress: 	Epoch 58 [3840/8883 (43.17%)]		Loss: 0.66846
Training Progress: 	Epoch 58 [4160/8883 (46.76%)]		Loss: 0.45312
Training Progress: 	Epoch 58 [4480/8883 (50.36%)]		Loss: 0.45369
Training Progress: 	Epoch 58 [4800/8883 (53.96%)]		Loss: 0.45684
Training Progress: 	Epoch 58 [5120/8883 (57.55%)]		Loss: 0.80597
Training Progress: 	Epoch 58 [5440/8883 (61.15%)]		Loss: 0.56676
Training Progress: 	Epoch 58 [5760/8883 (64.75%)]		Loss: 0.57218
Training Progress: 	Epoch 58 [6080/8883 (68.35%)]		Loss: 0.59048
Training Progress: 	Epoch 58 [6400/8883 (71.94%)]		Loss: 0.47990
Training Progress: 	Epoch 58 [6720/8883 (75.54%)]		Loss: 0.73010
Training Progress: 	Epoch 58 [7040/8883 (79.14%)]		Loss: 0.70299
Training Progress: 	Epoch 58 [7360/8883 (82.73%)]		Loss: 0.78305
Training Progress: 	Epoch 58 [7680/8883 (86.33%)]		Loss: 0.46619
Training Progress: 	Epoch 58 [8000/8883 (89.93%)]		Loss: 0.42102
Training Progress: 	Epoch 58 [8320/8883 (93.53%)]		Loss: 0.67748
Training Progress: 	Epoch 58 [8640/8883 (97.12%)]		Loss: 0.48941
	Train loss: 0.01468, Accuracy: 6891/8883 (77.00%)
	Validation loss: 0.00028, Accuracy: 1402/1692 (82.00%)
	Test loss: 0.00157, Accuracy: 524/1772 (29.00%)

Training Progress: 	Epoch 59 [0/8883 (0.00%)]		Loss: 0.46343
Training Progress: 	Epoch 59 [320/8883 (3.60%)]		Loss: 0.61535
Training Progress: 	Epoch 59 [640/8883 (7.19%)]		Loss: 0.64990
Training Progress: 	Epoch 59 [960/8883 (10.79%)]		Loss: 0.64738
Training Progress: 	Epoch 59 [1280/8883 (14.39%)]		Loss: 0.67243
Training Progress: 	Epoch 59 [1600/8883 (17.99%)]		Loss: 0.71706
Training Progress: 	Epoch 59 [1920/8883 (21.58%)]		Loss: 0.48116
Training Progress: 	Epoch 59 [2240/8883 (25.18%)]		Loss: 0.64861
Training Progress: 	Epoch 59 [2560/8883 (28.78%)]		Loss: 1.07511
Training Progress: 	Epoch 59 [2880/8883 (32.37%)]		Loss: 0.69126
Training Progress: 	Epoch 59 [3200/8883 (35.97%)]		Loss: 0.51625
Training Progress: 	Epoch 59 [3520/8883 (39.57%)]		Loss: 1.04035
Training Progress: 	Epoch 59 [3840/8883 (43.17%)]		Loss: 0.57153
Training Progress: 	Epoch 59 [4160/8883 (46.76%)]		Loss: 0.40866
Training Progress: 	Epoch 59 [4480/8883 (50.36%)]		Loss: 0.43017
Training Progress: 	Epoch 59 [4800/8883 (53.96%)]		Loss: 0.46126
Training Progress: 	Epoch 59 [5120/8883 (57.55%)]		Loss: 0.57454
Training Progress: 	Epoch 59 [5440/8883 (61.15%)]		Loss: 0.45205
Training Progress: 	Epoch 59 [5760/8883 (64.75%)]		Loss: 0.43808
Training Progress: 	Epoch 59 [6080/8883 (68.35%)]		Loss: 0.62644
Training Progress: 	Epoch 59 [6400/8883 (71.94%)]		Loss: 0.49116
Training Progress: 	Epoch 59 [6720/8883 (75.54%)]		Loss: 0.76387
Training Progress: 	Epoch 59 [7040/8883 (79.14%)]		Loss: 0.73331
Training Progress: 	Epoch 59 [7360/8883 (82.73%)]		Loss: 0.51892
Training Progress: 	Epoch 59 [7680/8883 (86.33%)]		Loss: 0.54894
Training Progress: 	Epoch 59 [8000/8883 (89.93%)]		Loss: 0.58265
Training Progress: 	Epoch 59 [8320/8883 (93.53%)]		Loss: 0.72718
Training Progress: 	Epoch 59 [8640/8883 (97.12%)]		Loss: 0.72641
	Train loss: 0.01438, Accuracy: 6855/8883 (77.00%)
	Validation loss: 0.00027, Accuracy: 1434/1692 (84.00%)
	Test loss: 0.00154, Accuracy: 568/1772 (32.00%)

Training Progress: 	Epoch 60 [0/8883 (0.00%)]		Loss: 0.51863
Training Progress: 	Epoch 60 [320/8883 (3.60%)]		Loss: 0.50162
Training Progress: 	Epoch 60 [640/8883 (7.19%)]		Loss: 0.37141
Training Progress: 	Epoch 60 [960/8883 (10.79%)]		Loss: 0.56529
Training Progress: 	Epoch 60 [1280/8883 (14.39%)]		Loss: 0.58521
Training Progress: 	Epoch 60 [1600/8883 (17.99%)]		Loss: 0.67068
Training Progress: 	Epoch 60 [1920/8883 (21.58%)]		Loss: 0.44777
Training Progress: 	Epoch 60 [2240/8883 (25.18%)]		Loss: 0.45233
Training Progress: 	Epoch 60 [2560/8883 (28.78%)]		Loss: 0.54677
Training Progress: 	Epoch 60 [2880/8883 (32.37%)]		Loss: 0.68241
Training Progress: 	Epoch 60 [3200/8883 (35.97%)]		Loss: 0.46149
Training Progress: 	Epoch 60 [3520/8883 (39.57%)]		Loss: 0.70926
Training Progress: 	Epoch 60 [3840/8883 (43.17%)]		Loss: 0.86573
Training Progress: 	Epoch 60 [4160/8883 (46.76%)]		Loss: 0.40297
Training Progress: 	Epoch 60 [4480/8883 (50.36%)]		Loss: 0.33800
Training Progress: 	Epoch 60 [4800/8883 (53.96%)]		Loss: 0.41752
Training Progress: 	Epoch 60 [5120/8883 (57.55%)]		Loss: 0.50494
Training Progress: 	Epoch 60 [5440/8883 (61.15%)]		Loss: 0.74090
Training Progress: 	Epoch 60 [5760/8883 (64.75%)]		Loss: 0.55929
Training Progress: 	Epoch 60 [6080/8883 (68.35%)]		Loss: 0.34767
Training Progress: 	Epoch 60 [6400/8883 (71.94%)]		Loss: 0.55363
Training Progress: 	Epoch 60 [6720/8883 (75.54%)]		Loss: 0.59558
Training Progress: 	Epoch 60 [7040/8883 (79.14%)]		Loss: 0.71629
Training Progress: 	Epoch 60 [7360/8883 (82.73%)]		Loss: 0.49043
Training Progress: 	Epoch 60 [7680/8883 (86.33%)]		Loss: 0.51335
Training Progress: 	Epoch 60 [8000/8883 (89.93%)]		Loss: 0.69429
Training Progress: 	Epoch 60 [8320/8883 (93.53%)]		Loss: 0.69039
Training Progress: 	Epoch 60 [8640/8883 (97.12%)]		Loss: 0.62550
	Train loss: 0.01409, Accuracy: 6954/8883 (78.00%)
	Validation loss: 0.00028, Accuracy: 1428/1692 (84.00%)
	Test loss: 0.00166, Accuracy: 522/1772 (29.00%)

Training Progress: 	Epoch 61 [0/8883 (0.00%)]		Loss: 0.54213
Training Progress: 	Epoch 61 [320/8883 (3.60%)]		Loss: 0.54999
Training Progress: 	Epoch 61 [640/8883 (7.19%)]		Loss: 0.30047
Training Progress: 	Epoch 61 [960/8883 (10.79%)]		Loss: 0.75338
Training Progress: 	Epoch 61 [1280/8883 (14.39%)]		Loss: 0.47513
Training Progress: 	Epoch 61 [1600/8883 (17.99%)]		Loss: 0.43663
Training Progress: 	Epoch 61 [1920/8883 (21.58%)]		Loss: 0.37199
Training Progress: 	Epoch 61 [2240/8883 (25.18%)]		Loss: 0.74401
Training Progress: 	Epoch 61 [2560/8883 (28.78%)]		Loss: 0.75339
Training Progress: 	Epoch 61 [2880/8883 (32.37%)]		Loss: 0.53388
Training Progress: 	Epoch 61 [3200/8883 (35.97%)]		Loss: 0.49065
Training Progress: 	Epoch 61 [3520/8883 (39.57%)]		Loss: 0.73241
Training Progress: 	Epoch 61 [3840/8883 (43.17%)]		Loss: 0.73463
Training Progress: 	Epoch 61 [4160/8883 (46.76%)]		Loss: 0.55331
Training Progress: 	Epoch 61 [4480/8883 (50.36%)]		Loss: 0.37111
Training Progress: 	Epoch 61 [4800/8883 (53.96%)]		Loss: 0.53032
Training Progress: 	Epoch 61 [5120/8883 (57.55%)]		Loss: 0.76634
Training Progress: 	Epoch 61 [5440/8883 (61.15%)]		Loss: 0.59532
Training Progress: 	Epoch 61 [5760/8883 (64.75%)]		Loss: 0.47720
Training Progress: 	Epoch 61 [6080/8883 (68.35%)]		Loss: 0.42769
Training Progress: 	Epoch 61 [6400/8883 (71.94%)]		Loss: 0.48211
Training Progress: 	Epoch 61 [6720/8883 (75.54%)]		Loss: 0.90632
Training Progress: 	Epoch 61 [7040/8883 (79.14%)]		Loss: 0.74285
Training Progress: 	Epoch 61 [7360/8883 (82.73%)]		Loss: 0.59527
Training Progress: 	Epoch 61 [7680/8883 (86.33%)]		Loss: 0.65011
Training Progress: 	Epoch 61 [8000/8883 (89.93%)]		Loss: 0.45622
Training Progress: 	Epoch 61 [8320/8883 (93.53%)]		Loss: 0.66961
Training Progress: 	Epoch 61 [8640/8883 (97.12%)]		Loss: 0.48811
	Train loss: 0.01383, Accuracy: 6959/8883 (78.00%)
	Validation loss: 0.00025, Accuracy: 1442/1692 (85.00%)
	Test loss: 0.00159, Accuracy: 535/1772 (30.00%)

Training Progress: 	Epoch 62 [0/8883 (0.00%)]		Loss: 0.49907
Training Progress: 	Epoch 62 [320/8883 (3.60%)]		Loss: 0.53248
Training Progress: 	Epoch 62 [640/8883 (7.19%)]		Loss: 0.43610
Training Progress: 	Epoch 62 [960/8883 (10.79%)]		Loss: 0.59438
Training Progress: 	Epoch 62 [1280/8883 (14.39%)]		Loss: 0.75884
Training Progress: 	Epoch 62 [1600/8883 (17.99%)]		Loss: 0.67267
Training Progress: 	Epoch 62 [1920/8883 (21.58%)]		Loss: 0.56437
Training Progress: 	Epoch 62 [2240/8883 (25.18%)]		Loss: 0.66091
Training Progress: 	Epoch 62 [2560/8883 (28.78%)]		Loss: 0.87462
Training Progress: 	Epoch 62 [2880/8883 (32.37%)]		Loss: 0.51635
Training Progress: 	Epoch 62 [3200/8883 (35.97%)]		Loss: 0.46029
Training Progress: 	Epoch 62 [3520/8883 (39.57%)]		Loss: 1.00954
Training Progress: 	Epoch 62 [3840/8883 (43.17%)]		Loss: 0.80312
Training Progress: 	Epoch 62 [4160/8883 (46.76%)]		Loss: 0.38381
Training Progress: 	Epoch 62 [4480/8883 (50.36%)]		Loss: 0.39943
Training Progress: 	Epoch 62 [4800/8883 (53.96%)]		Loss: 0.39275
Training Progress: 	Epoch 62 [5120/8883 (57.55%)]		Loss: 0.58827
Training Progress: 	Epoch 62 [5440/8883 (61.15%)]		Loss: 0.48885
Training Progress: 	Epoch 62 [5760/8883 (64.75%)]		Loss: 0.48003
Training Progress: 	Epoch 62 [6080/8883 (68.35%)]		Loss: 0.56079
Training Progress: 	Epoch 62 [6400/8883 (71.94%)]		Loss: 0.58256
Training Progress: 	Epoch 62 [6720/8883 (75.54%)]		Loss: 0.65643
Training Progress: 	Epoch 62 [7040/8883 (79.14%)]		Loss: 0.71168
Training Progress: 	Epoch 62 [7360/8883 (82.73%)]		Loss: 0.62082
Training Progress: 	Epoch 62 [7680/8883 (86.33%)]		Loss: 0.44357
Training Progress: 	Epoch 62 [8000/8883 (89.93%)]		Loss: 0.44637
Training Progress: 	Epoch 62 [8320/8883 (93.53%)]		Loss: 0.80600
Training Progress: 	Epoch 62 [8640/8883 (97.12%)]		Loss: 0.63009
	Train loss: 0.01425, Accuracy: 6930/8883 (78.00%)
	Validation loss: 0.00026, Accuracy: 1433/1692 (84.00%)
	Test loss: 0.00159, Accuracy: 531/1772 (29.00%)

Training Progress: 	Epoch 63 [0/8883 (0.00%)]		Loss: 0.52672
Training Progress: 	Epoch 63 [320/8883 (3.60%)]		Loss: 0.57113
Training Progress: 	Epoch 63 [640/8883 (7.19%)]		Loss: 0.60512
Training Progress: 	Epoch 63 [960/8883 (10.79%)]		Loss: 0.54048
Training Progress: 	Epoch 63 [1280/8883 (14.39%)]		Loss: 0.60121
Training Progress: 	Epoch 63 [1600/8883 (17.99%)]		Loss: 0.49942
Training Progress: 	Epoch 63 [1920/8883 (21.58%)]		Loss: 0.57680
Training Progress: 	Epoch 63 [2240/8883 (25.18%)]		Loss: 0.52347
Training Progress: 	Epoch 63 [2560/8883 (28.78%)]		Loss: 0.65347
Training Progress: 	Epoch 63 [2880/8883 (32.37%)]		Loss: 0.58507
Training Progress: 	Epoch 63 [3200/8883 (35.97%)]		Loss: 0.71572
Training Progress: 	Epoch 63 [3520/8883 (39.57%)]		Loss: 0.63470
Training Progress: 	Epoch 63 [3840/8883 (43.17%)]		Loss: 1.01461
Training Progress: 	Epoch 63 [4160/8883 (46.76%)]		Loss: 0.46155
Training Progress: 	Epoch 63 [4480/8883 (50.36%)]		Loss: 0.55191
Training Progress: 	Epoch 63 [4800/8883 (53.96%)]		Loss: 0.61886
Training Progress: 	Epoch 63 [5120/8883 (57.55%)]		Loss: 0.58337
Training Progress: 	Epoch 63 [5440/8883 (61.15%)]		Loss: 0.55925
Training Progress: 	Epoch 63 [5760/8883 (64.75%)]		Loss: 0.51432
Training Progress: 	Epoch 63 [6080/8883 (68.35%)]		Loss: 0.43146
Training Progress: 	Epoch 63 [6400/8883 (71.94%)]		Loss: 0.52867
Training Progress: 	Epoch 63 [6720/8883 (75.54%)]		Loss: 0.54313
Training Progress: 	Epoch 63 [7040/8883 (79.14%)]		Loss: 0.56140
Training Progress: 	Epoch 63 [7360/8883 (82.73%)]		Loss: 0.53043
Training Progress: 	Epoch 63 [7680/8883 (86.33%)]		Loss: 0.62378
Training Progress: 	Epoch 63 [8000/8883 (89.93%)]		Loss: 0.44503
Training Progress: 	Epoch 63 [8320/8883 (93.53%)]		Loss: 0.49997
Training Progress: 	Epoch 63 [8640/8883 (97.12%)]		Loss: 0.50864
	Train loss: 0.01392, Accuracy: 6935/8883 (78.00%)
	Validation loss: 0.00026, Accuracy: 1441/1692 (85.00%)
	Test loss: 0.00163, Accuracy: 511/1772 (28.00%)

Training Progress: 	Epoch 64 [0/8883 (0.00%)]		Loss: 0.49699
Training Progress: 	Epoch 64 [320/8883 (3.60%)]		Loss: 0.55777
Training Progress: 	Epoch 64 [640/8883 (7.19%)]		Loss: 0.47463
Training Progress: 	Epoch 64 [960/8883 (10.79%)]		Loss: 0.51475
Training Progress: 	Epoch 64 [1280/8883 (14.39%)]		Loss: 0.47063
Training Progress: 	Epoch 64 [1600/8883 (17.99%)]		Loss: 0.65807
Training Progress: 	Epoch 64 [1920/8883 (21.58%)]		Loss: 0.32880
Training Progress: 	Epoch 64 [2240/8883 (25.18%)]		Loss: 0.66787
Training Progress: 	Epoch 64 [2560/8883 (28.78%)]		Loss: 0.63733
Training Progress: 	Epoch 64 [2880/8883 (32.37%)]		Loss: 0.45387
Training Progress: 	Epoch 64 [3200/8883 (35.97%)]		Loss: 0.50820
Training Progress: 	Epoch 64 [3520/8883 (39.57%)]		Loss: 0.58872
Training Progress: 	Epoch 64 [3840/8883 (43.17%)]		Loss: 0.58224
Training Progress: 	Epoch 64 [4160/8883 (46.76%)]		Loss: 0.38380
Training Progress: 	Epoch 64 [4480/8883 (50.36%)]		Loss: 0.30807
Training Progress: 	Epoch 64 [4800/8883 (53.96%)]		Loss: 0.41559
Training Progress: 	Epoch 64 [5120/8883 (57.55%)]		Loss: 0.80286
Training Progress: 	Epoch 64 [5440/8883 (61.15%)]		Loss: 0.62107
Training Progress: 	Epoch 64 [5760/8883 (64.75%)]		Loss: 0.44447
Training Progress: 	Epoch 64 [6080/8883 (68.35%)]		Loss: 0.37596
Training Progress: 	Epoch 64 [6400/8883 (71.94%)]		Loss: 0.46155
Training Progress: 	Epoch 64 [6720/8883 (75.54%)]		Loss: 0.59339
Training Progress: 	Epoch 64 [7040/8883 (79.14%)]		Loss: 0.71884
Training Progress: 	Epoch 64 [7360/8883 (82.73%)]		Loss: 0.56540
Training Progress: 	Epoch 64 [7680/8883 (86.33%)]		Loss: 0.54876
Training Progress: 	Epoch 64 [8000/8883 (89.93%)]		Loss: 0.47820
Training Progress: 	Epoch 64 [8320/8883 (93.53%)]		Loss: 0.71669
Training Progress: 	Epoch 64 [8640/8883 (97.12%)]		Loss: 0.71589
	Train loss: 0.01363, Accuracy: 7021/8883 (79.00%)
	Validation loss: 0.00025, Accuracy: 1454/1692 (85.00%)
	Test loss: 0.00162, Accuracy: 542/1772 (30.00%)

Training Progress: 	Epoch 65 [0/8883 (0.00%)]		Loss: 0.55074
Training Progress: 	Epoch 65 [320/8883 (3.60%)]		Loss: 0.59507
Training Progress: 	Epoch 65 [640/8883 (7.19%)]		Loss: 0.26246
Training Progress: 	Epoch 65 [960/8883 (10.79%)]		Loss: 0.44425
Training Progress: 	Epoch 65 [1280/8883 (14.39%)]		Loss: 0.55766
Training Progress: 	Epoch 65 [1600/8883 (17.99%)]		Loss: 0.54510
Training Progress: 	Epoch 65 [1920/8883 (21.58%)]		Loss: 0.38452
Training Progress: 	Epoch 65 [2240/8883 (25.18%)]		Loss: 0.61365
Training Progress: 	Epoch 65 [2560/8883 (28.78%)]		Loss: 0.59868
Training Progress: 	Epoch 65 [2880/8883 (32.37%)]		Loss: 0.71769
Training Progress: 	Epoch 65 [3200/8883 (35.97%)]		Loss: 0.39369
Training Progress: 	Epoch 65 [3520/8883 (39.57%)]		Loss: 0.72704
Training Progress: 	Epoch 65 [3840/8883 (43.17%)]		Loss: 0.57058
Training Progress: 	Epoch 65 [4160/8883 (46.76%)]		Loss: 0.40737
Training Progress: 	Epoch 65 [4480/8883 (50.36%)]		Loss: 0.46608
Training Progress: 	Epoch 65 [4800/8883 (53.96%)]		Loss: 0.36345
Training Progress: 	Epoch 65 [5120/8883 (57.55%)]		Loss: 0.61673
Training Progress: 	Epoch 65 [5440/8883 (61.15%)]		Loss: 0.79581
Training Progress: 	Epoch 65 [5760/8883 (64.75%)]		Loss: 0.66675
Training Progress: 	Epoch 65 [6080/8883 (68.35%)]		Loss: 0.45422
Training Progress: 	Epoch 65 [6400/8883 (71.94%)]		Loss: 0.39575
Training Progress: 	Epoch 65 [6720/8883 (75.54%)]		Loss: 0.73447
Training Progress: 	Epoch 65 [7040/8883 (79.14%)]		Loss: 1.01172
Training Progress: 	Epoch 65 [7360/8883 (82.73%)]		Loss: 0.58622
Training Progress: 	Epoch 65 [7680/8883 (86.33%)]		Loss: 0.59773
Training Progress: 	Epoch 65 [8000/8883 (89.93%)]		Loss: 0.49283
Training Progress: 	Epoch 65 [8320/8883 (93.53%)]		Loss: 0.67090
Training Progress: 	Epoch 65 [8640/8883 (97.12%)]		Loss: 0.44446
	Train loss: 0.01364, Accuracy: 6982/8883 (78.00%)
	Validation loss: 0.00024, Accuracy: 1467/1692 (86.00%)
	Test loss: 0.00163, Accuracy: 537/1772 (30.00%)

Training Progress: 	Epoch 66 [0/8883 (0.00%)]		Loss: 0.57313
Training Progress: 	Epoch 66 [320/8883 (3.60%)]		Loss: 0.75966
Training Progress: 	Epoch 66 [640/8883 (7.19%)]		Loss: 0.46998
Training Progress: 	Epoch 66 [960/8883 (10.79%)]		Loss: 0.65589
Training Progress: 	Epoch 66 [1280/8883 (14.39%)]		Loss: 0.50833
Training Progress: 	Epoch 66 [1600/8883 (17.99%)]		Loss: 0.48974
Training Progress: 	Epoch 66 [1920/8883 (21.58%)]		Loss: 0.48545
Training Progress: 	Epoch 66 [2240/8883 (25.18%)]		Loss: 0.63120
Training Progress: 	Epoch 66 [2560/8883 (28.78%)]		Loss: 0.65193
Training Progress: 	Epoch 66 [2880/8883 (32.37%)]		Loss: 0.56072
Training Progress: 	Epoch 66 [3200/8883 (35.97%)]		Loss: 0.54966
Training Progress: 	Epoch 66 [3520/8883 (39.57%)]		Loss: 0.57660
Training Progress: 	Epoch 66 [3840/8883 (43.17%)]		Loss: 0.59786
Training Progress: 	Epoch 66 [4160/8883 (46.76%)]		Loss: 0.61271
Training Progress: 	Epoch 66 [4480/8883 (50.36%)]		Loss: 0.49579
Training Progress: 	Epoch 66 [4800/8883 (53.96%)]		Loss: 0.39152
Training Progress: 	Epoch 66 [5120/8883 (57.55%)]		Loss: 0.81048
Training Progress: 	Epoch 66 [5440/8883 (61.15%)]		Loss: 0.56032
Training Progress: 	Epoch 66 [5760/8883 (64.75%)]		Loss: 0.43465
Training Progress: 	Epoch 66 [6080/8883 (68.35%)]		Loss: 0.56276
Training Progress: 	Epoch 66 [6400/8883 (71.94%)]		Loss: 0.43485
Training Progress: 	Epoch 66 [6720/8883 (75.54%)]		Loss: 0.63625
Training Progress: 	Epoch 66 [7040/8883 (79.14%)]		Loss: 0.80903
Training Progress: 	Epoch 66 [7360/8883 (82.73%)]		Loss: 0.52658
Training Progress: 	Epoch 66 [7680/8883 (86.33%)]		Loss: 0.53675
Training Progress: 	Epoch 66 [8000/8883 (89.93%)]		Loss: 0.58492
Training Progress: 	Epoch 66 [8320/8883 (93.53%)]		Loss: 0.63100
Training Progress: 	Epoch 66 [8640/8883 (97.12%)]		Loss: 0.57906
	Train loss: 0.01347, Accuracy: 6999/8883 (78.00%)
	Validation loss: 0.00024, Accuracy: 1464/1692 (86.00%)
	Test loss: 0.00165, Accuracy: 560/1772 (31.00%)

Training Progress: 	Epoch 67 [0/8883 (0.00%)]		Loss: 0.58873
Training Progress: 	Epoch 67 [320/8883 (3.60%)]		Loss: 0.80569
Training Progress: 	Epoch 67 [640/8883 (7.19%)]		Loss: 0.56030
Training Progress: 	Epoch 67 [960/8883 (10.79%)]		Loss: 0.49297
Training Progress: 	Epoch 67 [1280/8883 (14.39%)]		Loss: 0.54843
Training Progress: 	Epoch 67 [1600/8883 (17.99%)]		Loss: 0.63819
Training Progress: 	Epoch 67 [1920/8883 (21.58%)]		Loss: 0.47812
Training Progress: 	Epoch 67 [2240/8883 (25.18%)]		Loss: 0.71077
Training Progress: 	Epoch 67 [2560/8883 (28.78%)]		Loss: 0.56242
Training Progress: 	Epoch 67 [2880/8883 (32.37%)]		Loss: 0.48376
Training Progress: 	Epoch 67 [3200/8883 (35.97%)]		Loss: 0.62219
Training Progress: 	Epoch 67 [3520/8883 (39.57%)]		Loss: 0.82002
Training Progress: 	Epoch 67 [3840/8883 (43.17%)]		Loss: 0.67621
Training Progress: 	Epoch 67 [4160/8883 (46.76%)]		Loss: 0.44078
Training Progress: 	Epoch 67 [4480/8883 (50.36%)]		Loss: 0.38579
Training Progress: 	Epoch 67 [4800/8883 (53.96%)]		Loss: 0.35114
Training Progress: 	Epoch 67 [5120/8883 (57.55%)]		Loss: 0.60033
Training Progress: 	Epoch 67 [5440/8883 (61.15%)]		Loss: 0.56311
Training Progress: 	Epoch 67 [5760/8883 (64.75%)]		Loss: 0.71368
Training Progress: 	Epoch 67 [6080/8883 (68.35%)]		Loss: 0.41380
Training Progress: 	Epoch 67 [6400/8883 (71.94%)]		Loss: 0.59619
Training Progress: 	Epoch 67 [6720/8883 (75.54%)]		Loss: 0.63314
Training Progress: 	Epoch 67 [7040/8883 (79.14%)]		Loss: 0.75971
Training Progress: 	Epoch 67 [7360/8883 (82.73%)]		Loss: 0.51367
Training Progress: 	Epoch 67 [7680/8883 (86.33%)]		Loss: 0.49366
Training Progress: 	Epoch 67 [8000/8883 (89.93%)]		Loss: 0.55581
Training Progress: 	Epoch 67 [8320/8883 (93.53%)]		Loss: 0.55300
Training Progress: 	Epoch 67 [8640/8883 (97.12%)]		Loss: 0.57575
	Train loss: 0.01339, Accuracy: 7014/8883 (78.00%)
	Validation loss: 0.00027, Accuracy: 1425/1692 (84.00%)
	Test loss: 0.00168, Accuracy: 545/1772 (30.00%)

Training Progress: 	Epoch 68 [0/8883 (0.00%)]		Loss: 0.62325
Training Progress: 	Epoch 68 [320/8883 (3.60%)]		Loss: 0.54859
Training Progress: 	Epoch 68 [640/8883 (7.19%)]		Loss: 0.49295
Training Progress: 	Epoch 68 [960/8883 (10.79%)]		Loss: 0.55038
Training Progress: 	Epoch 68 [1280/8883 (14.39%)]		Loss: 0.41493
Training Progress: 	Epoch 68 [1600/8883 (17.99%)]		Loss: 0.60489
Training Progress: 	Epoch 68 [1920/8883 (21.58%)]		Loss: 0.50215
Training Progress: 	Epoch 68 [2240/8883 (25.18%)]		Loss: 0.65383
Training Progress: 	Epoch 68 [2560/8883 (28.78%)]		Loss: 0.58427
Training Progress: 	Epoch 68 [2880/8883 (32.37%)]		Loss: 0.45591
Training Progress: 	Epoch 68 [3200/8883 (35.97%)]		Loss: 0.59625
Training Progress: 	Epoch 68 [3520/8883 (39.57%)]		Loss: 0.77174
Training Progress: 	Epoch 68 [3840/8883 (43.17%)]		Loss: 0.85634
Training Progress: 	Epoch 68 [4160/8883 (46.76%)]		Loss: 0.50203
Training Progress: 	Epoch 68 [4480/8883 (50.36%)]		Loss: 0.43419
Training Progress: 	Epoch 68 [4800/8883 (53.96%)]		Loss: 0.40507
Training Progress: 	Epoch 68 [5120/8883 (57.55%)]		Loss: 0.66552
Training Progress: 	Epoch 68 [5440/8883 (61.15%)]		Loss: 0.63478
Training Progress: 	Epoch 68 [5760/8883 (64.75%)]		Loss: 0.53799
Training Progress: 	Epoch 68 [6080/8883 (68.35%)]		Loss: 0.43801
Training Progress: 	Epoch 68 [6400/8883 (71.94%)]		Loss: 0.87190
Training Progress: 	Epoch 68 [6720/8883 (75.54%)]		Loss: 0.55977
Training Progress: 	Epoch 68 [7040/8883 (79.14%)]		Loss: 0.79436
Training Progress: 	Epoch 68 [7360/8883 (82.73%)]		Loss: 0.55221
Training Progress: 	Epoch 68 [7680/8883 (86.33%)]		Loss: 0.44033
Training Progress: 	Epoch 68 [8000/8883 (89.93%)]		Loss: 0.45608
Training Progress: 	Epoch 68 [8320/8883 (93.53%)]		Loss: 0.48080
Training Progress: 	Epoch 68 [8640/8883 (97.12%)]		Loss: 0.51601
	Train loss: 0.01346, Accuracy: 7011/8883 (78.00%)
	Validation loss: 0.00024, Accuracy: 1458/1692 (86.00%)
	Test loss: 0.00164, Accuracy: 531/1772 (29.00%)

Training Progress: 	Epoch 69 [0/8883 (0.00%)]		Loss: 0.60779
Training Progress: 	Epoch 69 [320/8883 (3.60%)]		Loss: 0.41830
Training Progress: 	Epoch 69 [640/8883 (7.19%)]		Loss: 0.38176
Training Progress: 	Epoch 69 [960/8883 (10.79%)]		Loss: 0.51104
Training Progress: 	Epoch 69 [1280/8883 (14.39%)]		Loss: 0.59007
Training Progress: 	Epoch 69 [1600/8883 (17.99%)]		Loss: 0.78668
Training Progress: 	Epoch 69 [1920/8883 (21.58%)]		Loss: 0.53832
Training Progress: 	Epoch 69 [2240/8883 (25.18%)]		Loss: 0.53637
Training Progress: 	Epoch 69 [2560/8883 (28.78%)]		Loss: 0.69772
Training Progress: 	Epoch 69 [2880/8883 (32.37%)]		Loss: 0.59284
Training Progress: 	Epoch 69 [3200/8883 (35.97%)]		Loss: 0.48387
Training Progress: 	Epoch 69 [3520/8883 (39.57%)]		Loss: 0.83007
Training Progress: 	Epoch 69 [3840/8883 (43.17%)]		Loss: 0.56156
Training Progress: 	Epoch 69 [4160/8883 (46.76%)]		Loss: 0.45092
Training Progress: 	Epoch 69 [4480/8883 (50.36%)]		Loss: 0.48182
Training Progress: 	Epoch 69 [4800/8883 (53.96%)]		Loss: 0.27211
Training Progress: 	Epoch 69 [5120/8883 (57.55%)]		Loss: 0.57565
Training Progress: 	Epoch 69 [5440/8883 (61.15%)]		Loss: 0.76027
Training Progress: 	Epoch 69 [5760/8883 (64.75%)]		Loss: 0.75275
Training Progress: 	Epoch 69 [6080/8883 (68.35%)]		Loss: 0.45728
Training Progress: 	Epoch 69 [6400/8883 (71.94%)]		Loss: 0.53254
Training Progress: 	Epoch 69 [6720/8883 (75.54%)]		Loss: 0.74539
Training Progress: 	Epoch 69 [7040/8883 (79.14%)]		Loss: 0.80235
Training Progress: 	Epoch 69 [7360/8883 (82.73%)]		Loss: 0.67558
Training Progress: 	Epoch 69 [7680/8883 (86.33%)]		Loss: 0.46011
Training Progress: 	Epoch 69 [8000/8883 (89.93%)]		Loss: 0.39194
Training Progress: 	Epoch 69 [8320/8883 (93.53%)]		Loss: 0.93431
Training Progress: 	Epoch 69 [8640/8883 (97.12%)]		Loss: 0.54387
	Train loss: 0.01368, Accuracy: 6980/8883 (78.00%)
	Validation loss: 0.00025, Accuracy: 1443/1692 (85.00%)
	Test loss: 0.00170, Accuracy: 510/1772 (28.00%)

Training Progress: 	Epoch 70 [0/8883 (0.00%)]		Loss: 0.65498
Training Progress: 	Epoch 70 [320/8883 (3.60%)]		Loss: 0.61877
Training Progress: 	Epoch 70 [640/8883 (7.19%)]		Loss: 0.60652
Training Progress: 	Epoch 70 [960/8883 (10.79%)]		Loss: 0.55381
Training Progress: 	Epoch 70 [1280/8883 (14.39%)]		Loss: 0.76888
Training Progress: 	Epoch 70 [1600/8883 (17.99%)]		Loss: 0.28325
Training Progress: 	Epoch 70 [1920/8883 (21.58%)]		Loss: 0.43717
Training Progress: 	Epoch 70 [2240/8883 (25.18%)]		Loss: 0.59218
Training Progress: 	Epoch 70 [2560/8883 (28.78%)]		Loss: 0.53257
Training Progress: 	Epoch 70 [2880/8883 (32.37%)]		Loss: 0.64339
Training Progress: 	Epoch 70 [3200/8883 (35.97%)]		Loss: 0.52585
Training Progress: 	Epoch 70 [3520/8883 (39.57%)]		Loss: 0.68617
Training Progress: 	Epoch 70 [3840/8883 (43.17%)]		Loss: 0.58387
Training Progress: 	Epoch 70 [4160/8883 (46.76%)]		Loss: 0.32269
Training Progress: 	Epoch 70 [4480/8883 (50.36%)]		Loss: 0.50471
Training Progress: 	Epoch 70 [4800/8883 (53.96%)]		Loss: 0.39030
Training Progress: 	Epoch 70 [5120/8883 (57.55%)]		Loss: 0.55128
Training Progress: 	Epoch 70 [5440/8883 (61.15%)]		Loss: 0.41847
Training Progress: 	Epoch 70 [5760/8883 (64.75%)]		Loss: 0.38216
Training Progress: 	Epoch 70 [6080/8883 (68.35%)]		Loss: 0.40826
Training Progress: 	Epoch 70 [6400/8883 (71.94%)]		Loss: 0.59655
Training Progress: 	Epoch 70 [6720/8883 (75.54%)]		Loss: 0.50412
Training Progress: 	Epoch 70 [7040/8883 (79.14%)]		Loss: 0.62355
Training Progress: 	Epoch 70 [7360/8883 (82.73%)]		Loss: 0.61347
Training Progress: 	Epoch 70 [7680/8883 (86.33%)]		Loss: 0.37739
Training Progress: 	Epoch 70 [8000/8883 (89.93%)]		Loss: 0.46586
Training Progress: 	Epoch 70 [8320/8883 (93.53%)]		Loss: 0.61579
Training Progress: 	Epoch 70 [8640/8883 (97.12%)]		Loss: 0.52225
	Train loss: 0.01331, Accuracy: 7023/8883 (79.00%)
	Validation loss: 0.00023, Accuracy: 1461/1692 (86.00%)
	Test loss: 0.00175, Accuracy: 557/1772 (31.00%)

Training Progress: 	Epoch 71 [0/8883 (0.00%)]		Loss: 0.38694
Training Progress: 	Epoch 71 [320/8883 (3.60%)]		Loss: 0.42136
Training Progress: 	Epoch 71 [640/8883 (7.19%)]		Loss: 0.34116
Training Progress: 	Epoch 71 [960/8883 (10.79%)]		Loss: 0.38675
Training Progress: 	Epoch 71 [1280/8883 (14.39%)]		Loss: 0.40777
Training Progress: 	Epoch 71 [1600/8883 (17.99%)]		Loss: 0.45180
Training Progress: 	Epoch 71 [1920/8883 (21.58%)]		Loss: 0.59083
Training Progress: 	Epoch 71 [2240/8883 (25.18%)]		Loss: 0.52953
Training Progress: 	Epoch 71 [2560/8883 (28.78%)]		Loss: 0.65273
Training Progress: 	Epoch 71 [2880/8883 (32.37%)]		Loss: 0.71989
Training Progress: 	Epoch 71 [3200/8883 (35.97%)]		Loss: 0.43393
Training Progress: 	Epoch 71 [3520/8883 (39.57%)]		Loss: 0.58387
Training Progress: 	Epoch 71 [3840/8883 (43.17%)]		Loss: 0.58834
Training Progress: 	Epoch 71 [4160/8883 (46.76%)]		Loss: 0.38941
Training Progress: 	Epoch 71 [4480/8883 (50.36%)]		Loss: 0.31252
Training Progress: 	Epoch 71 [4800/8883 (53.96%)]		Loss: 0.29276
Training Progress: 	Epoch 71 [5120/8883 (57.55%)]		Loss: 0.51065
Training Progress: 	Epoch 71 [5440/8883 (61.15%)]		Loss: 0.56674
Training Progress: 	Epoch 71 [5760/8883 (64.75%)]		Loss: 0.64726
Training Progress: 	Epoch 71 [6080/8883 (68.35%)]		Loss: 0.40350
Training Progress: 	Epoch 71 [6400/8883 (71.94%)]		Loss: 0.39446
Training Progress: 	Epoch 71 [6720/8883 (75.54%)]		Loss: 0.62022
Training Progress: 	Epoch 71 [7040/8883 (79.14%)]		Loss: 0.60695
Training Progress: 	Epoch 71 [7360/8883 (82.73%)]		Loss: 0.56264
Training Progress: 	Epoch 71 [7680/8883 (86.33%)]		Loss: 0.50417
Training Progress: 	Epoch 71 [8000/8883 (89.93%)]		Loss: 0.49250
Training Progress: 	Epoch 71 [8320/8883 (93.53%)]		Loss: 0.66180
Training Progress: 	Epoch 71 [8640/8883 (97.12%)]		Loss: 0.54263
	Train loss: 0.01328, Accuracy: 7003/8883 (78.00%)
	Validation loss: 0.00023, Accuracy: 1449/1692 (85.00%)
	Test loss: 0.00165, Accuracy: 573/1772 (32.00%)

Training Progress: 	Epoch 72 [0/8883 (0.00%)]		Loss: 0.57150
Training Progress: 	Epoch 72 [320/8883 (3.60%)]		Loss: 0.50216
Training Progress: 	Epoch 72 [640/8883 (7.19%)]		Loss: 0.33386
Training Progress: 	Epoch 72 [960/8883 (10.79%)]		Loss: 0.38895
Training Progress: 	Epoch 72 [1280/8883 (14.39%)]		Loss: 0.53839
Training Progress: 	Epoch 72 [1600/8883 (17.99%)]		Loss: 0.51072
Training Progress: 	Epoch 72 [1920/8883 (21.58%)]		Loss: 0.32969
Training Progress: 	Epoch 72 [2240/8883 (25.18%)]		Loss: 0.53479
Training Progress: 	Epoch 72 [2560/8883 (28.78%)]		Loss: 0.56951
Training Progress: 	Epoch 72 [2880/8883 (32.37%)]		Loss: 0.52156
Training Progress: 	Epoch 72 [3200/8883 (35.97%)]		Loss: 0.51600
Training Progress: 	Epoch 72 [3520/8883 (39.57%)]		Loss: 0.55716
Training Progress: 	Epoch 72 [3840/8883 (43.17%)]		Loss: 0.79500
Training Progress: 	Epoch 72 [4160/8883 (46.76%)]		Loss: 0.29295
Training Progress: 	Epoch 72 [4480/8883 (50.36%)]		Loss: 0.54669
Training Progress: 	Epoch 72 [4800/8883 (53.96%)]		Loss: 0.44044
Training Progress: 	Epoch 72 [5120/8883 (57.55%)]		Loss: 0.56557
Training Progress: 	Epoch 72 [5440/8883 (61.15%)]		Loss: 0.54221
Training Progress: 	Epoch 72 [5760/8883 (64.75%)]		Loss: 0.68085
Training Progress: 	Epoch 72 [6080/8883 (68.35%)]		Loss: 0.48086
Training Progress: 	Epoch 72 [6400/8883 (71.94%)]		Loss: 0.37033
Training Progress: 	Epoch 72 [6720/8883 (75.54%)]		Loss: 0.62254
Training Progress: 	Epoch 72 [7040/8883 (79.14%)]		Loss: 0.55684
Training Progress: 	Epoch 72 [7360/8883 (82.73%)]		Loss: 0.53505
Training Progress: 	Epoch 72 [7680/8883 (86.33%)]		Loss: 0.47778
Training Progress: 	Epoch 72 [8000/8883 (89.93%)]		Loss: 0.42970
Training Progress: 	Epoch 72 [8320/8883 (93.53%)]		Loss: 0.67404
Training Progress: 	Epoch 72 [8640/8883 (97.12%)]		Loss: 0.50019
	Train loss: 0.01303, Accuracy: 7058/8883 (79.00%)
	Validation loss: 0.00024, Accuracy: 1461/1692 (86.00%)
	Test loss: 0.00175, Accuracy: 533/1772 (30.00%)

Training Progress: 	Epoch 73 [0/8883 (0.00%)]		Loss: 0.63094
Training Progress: 	Epoch 73 [320/8883 (3.60%)]		Loss: 0.62545
Training Progress: 	Epoch 73 [640/8883 (7.19%)]		Loss: 0.41901
Training Progress: 	Epoch 73 [960/8883 (10.79%)]		Loss: 0.62710
Training Progress: 	Epoch 73 [1280/8883 (14.39%)]		Loss: 0.61476
Training Progress: 	Epoch 73 [1600/8883 (17.99%)]		Loss: 0.61534
Training Progress: 	Epoch 73 [1920/8883 (21.58%)]		Loss: 0.41399
Training Progress: 	Epoch 73 [2240/8883 (25.18%)]		Loss: 0.58758
Training Progress: 	Epoch 73 [2560/8883 (28.78%)]		Loss: 0.65330
Training Progress: 	Epoch 73 [2880/8883 (32.37%)]		Loss: 0.43654
Training Progress: 	Epoch 73 [3200/8883 (35.97%)]		Loss: 0.60967
Training Progress: 	Epoch 73 [3520/8883 (39.57%)]		Loss: 0.98305
Training Progress: 	Epoch 73 [3840/8883 (43.17%)]		Loss: 0.66551
Training Progress: 	Epoch 73 [4160/8883 (46.76%)]		Loss: 0.34180
Training Progress: 	Epoch 73 [4480/8883 (50.36%)]		Loss: 0.48210
Training Progress: 	Epoch 73 [4800/8883 (53.96%)]		Loss: 0.44901
Training Progress: 	Epoch 73 [5120/8883 (57.55%)]		Loss: 0.78376
Training Progress: 	Epoch 73 [5440/8883 (61.15%)]		Loss: 0.54075
Training Progress: 	Epoch 73 [5760/8883 (64.75%)]		Loss: 0.44994
Training Progress: 	Epoch 73 [6080/8883 (68.35%)]		Loss: 0.39716
Training Progress: 	Epoch 73 [6400/8883 (71.94%)]		Loss: 0.37260
Training Progress: 	Epoch 73 [6720/8883 (75.54%)]		Loss: 0.81983
Training Progress: 	Epoch 73 [7040/8883 (79.14%)]		Loss: 0.63467
Training Progress: 	Epoch 73 [7360/8883 (82.73%)]		Loss: 0.65488
Training Progress: 	Epoch 73 [7680/8883 (86.33%)]		Loss: 0.42343
Training Progress: 	Epoch 73 [8000/8883 (89.93%)]		Loss: 0.60803
Training Progress: 	Epoch 73 [8320/8883 (93.53%)]		Loss: 0.66100
Training Progress: 	Epoch 73 [8640/8883 (97.12%)]		Loss: 0.62234
	Train loss: 0.01325, Accuracy: 7016/8883 (78.00%)
	Validation loss: 0.00024, Accuracy: 1461/1692 (86.00%)
	Test loss: 0.00167, Accuracy: 563/1772 (31.00%)

Training Progress: 	Epoch 74 [0/8883 (0.00%)]		Loss: 0.59153
Training Progress: 	Epoch 74 [320/8883 (3.60%)]		Loss: 0.61457
Training Progress: 	Epoch 74 [640/8883 (7.19%)]		Loss: 0.49191
Training Progress: 	Epoch 74 [960/8883 (10.79%)]		Loss: 0.43104
Training Progress: 	Epoch 74 [1280/8883 (14.39%)]		Loss: 0.49831
Training Progress: 	Epoch 74 [1600/8883 (17.99%)]		Loss: 0.65564
Training Progress: 	Epoch 74 [1920/8883 (21.58%)]		Loss: 0.58536
Training Progress: 	Epoch 74 [2240/8883 (25.18%)]		Loss: 0.70198
Training Progress: 	Epoch 74 [2560/8883 (28.78%)]		Loss: 0.59009
Training Progress: 	Epoch 74 [2880/8883 (32.37%)]		Loss: 0.63704
Training Progress: 	Epoch 74 [3200/8883 (35.97%)]		Loss: 0.53027
Training Progress: 	Epoch 74 [3520/8883 (39.57%)]		Loss: 0.66314
Training Progress: 	Epoch 74 [3840/8883 (43.17%)]		Loss: 0.63138
Training Progress: 	Epoch 74 [4160/8883 (46.76%)]		Loss: 0.51023
Training Progress: 	Epoch 74 [4480/8883 (50.36%)]		Loss: 0.41132
Training Progress: 	Epoch 74 [4800/8883 (53.96%)]		Loss: 0.31425
Training Progress: 	Epoch 74 [5120/8883 (57.55%)]		Loss: 0.49214
Training Progress: 	Epoch 74 [5440/8883 (61.15%)]		Loss: 0.44267
Training Progress: 	Epoch 74 [5760/8883 (64.75%)]		Loss: 0.33807
Training Progress: 	Epoch 74 [6080/8883 (68.35%)]		Loss: 0.35089
Training Progress: 	Epoch 74 [6400/8883 (71.94%)]		Loss: 0.55248
Training Progress: 	Epoch 74 [6720/8883 (75.54%)]		Loss: 0.70221
Training Progress: 	Epoch 74 [7040/8883 (79.14%)]		Loss: 0.78879
Training Progress: 	Epoch 74 [7360/8883 (82.73%)]		Loss: 0.45033
Training Progress: 	Epoch 74 [7680/8883 (86.33%)]		Loss: 0.61756
Training Progress: 	Epoch 74 [8000/8883 (89.93%)]		Loss: 0.40849
Training Progress: 	Epoch 74 [8320/8883 (93.53%)]		Loss: 0.65551
Training Progress: 	Epoch 74 [8640/8883 (97.12%)]		Loss: 0.58705
	Train loss: 0.01284, Accuracy: 7065/8883 (79.00%)
	Validation loss: 0.00020, Accuracy: 1508/1692 (89.00%)
	Test loss: 0.00171, Accuracy: 567/1772 (31.00%)

Training Progress: 	Epoch 75 [0/8883 (0.00%)]		Loss: 0.56719
Training Progress: 	Epoch 75 [320/8883 (3.60%)]		Loss: 0.50599
Training Progress: 	Epoch 75 [640/8883 (7.19%)]		Loss: 0.37647
Training Progress: 	Epoch 75 [960/8883 (10.79%)]		Loss: 0.64484
Training Progress: 	Epoch 75 [1280/8883 (14.39%)]		Loss: 0.68405
Training Progress: 	Epoch 75 [1600/8883 (17.99%)]		Loss: 0.43401
Training Progress: 	Epoch 75 [1920/8883 (21.58%)]		Loss: 0.37342
Training Progress: 	Epoch 75 [2240/8883 (25.18%)]		Loss: 0.45338
Training Progress: 	Epoch 75 [2560/8883 (28.78%)]		Loss: 0.60469
Training Progress: 	Epoch 75 [2880/8883 (32.37%)]		Loss: 0.58099
Training Progress: 	Epoch 75 [3200/8883 (35.97%)]		Loss: 0.58965
Training Progress: 	Epoch 75 [3520/8883 (39.57%)]		Loss: 0.68979
Training Progress: 	Epoch 75 [3840/8883 (43.17%)]		Loss: 0.52270
Training Progress: 	Epoch 75 [4160/8883 (46.76%)]		Loss: 0.46551
Training Progress: 	Epoch 75 [4480/8883 (50.36%)]		Loss: 0.37751
Training Progress: 	Epoch 75 [4800/8883 (53.96%)]		Loss: 0.48428
Training Progress: 	Epoch 75 [5120/8883 (57.55%)]		Loss: 0.66219
Training Progress: 	Epoch 75 [5440/8883 (61.15%)]		Loss: 0.50625
Training Progress: 	Epoch 75 [5760/8883 (64.75%)]		Loss: 0.54875
Training Progress: 	Epoch 75 [6080/8883 (68.35%)]		Loss: 0.45928
Training Progress: 	Epoch 75 [6400/8883 (71.94%)]		Loss: 0.59494
Training Progress: 	Epoch 75 [6720/8883 (75.54%)]		Loss: 0.61961
Training Progress: 	Epoch 75 [7040/8883 (79.14%)]		Loss: 0.61325
Training Progress: 	Epoch 75 [7360/8883 (82.73%)]		Loss: 0.48405
Training Progress: 	Epoch 75 [7680/8883 (86.33%)]		Loss: 0.40069
Training Progress: 	Epoch 75 [8000/8883 (89.93%)]		Loss: 0.43989
Training Progress: 	Epoch 75 [8320/8883 (93.53%)]		Loss: 0.80527
Training Progress: 	Epoch 75 [8640/8883 (97.12%)]		Loss: 0.66106
	Train loss: 0.01290, Accuracy: 7064/8883 (79.00%)
	Validation loss: 0.00022, Accuracy: 1476/1692 (87.00%)
	Test loss: 0.00176, Accuracy: 533/1772 (30.00%)

Training Progress: 	Epoch 76 [0/8883 (0.00%)]		Loss: 0.39162
Training Progress: 	Epoch 76 [320/8883 (3.60%)]		Loss: 0.40737
Training Progress: 	Epoch 76 [640/8883 (7.19%)]		Loss: 0.30151
Training Progress: 	Epoch 76 [960/8883 (10.79%)]		Loss: 0.65359
Training Progress: 	Epoch 76 [1280/8883 (14.39%)]		Loss: 0.46252
Training Progress: 	Epoch 76 [1600/8883 (17.99%)]		Loss: 0.54913
Training Progress: 	Epoch 76 [1920/8883 (21.58%)]		Loss: 0.45788
Training Progress: 	Epoch 76 [2240/8883 (25.18%)]		Loss: 0.51672
Training Progress: 	Epoch 76 [2560/8883 (28.78%)]		Loss: 0.72397
Training Progress: 	Epoch 76 [2880/8883 (32.37%)]		Loss: 0.79160
Training Progress: 	Epoch 76 [3200/8883 (35.97%)]		Loss: 0.46549
Training Progress: 	Epoch 76 [3520/8883 (39.57%)]		Loss: 0.65342
Training Progress: 	Epoch 76 [3840/8883 (43.17%)]		Loss: 0.59398
Training Progress: 	Epoch 76 [4160/8883 (46.76%)]		Loss: 0.32290
Training Progress: 	Epoch 76 [4480/8883 (50.36%)]		Loss: 0.33339
Training Progress: 	Epoch 76 [4800/8883 (53.96%)]		Loss: 0.30133
Training Progress: 	Epoch 76 [5120/8883 (57.55%)]		Loss: 0.45151
Training Progress: 	Epoch 76 [5440/8883 (61.15%)]		Loss: 0.75416
Training Progress: 	Epoch 76 [5760/8883 (64.75%)]		Loss: 0.43242
Training Progress: 	Epoch 76 [6080/8883 (68.35%)]		Loss: 0.25357
Training Progress: 	Epoch 76 [6400/8883 (71.94%)]		Loss: 0.36751
Training Progress: 	Epoch 76 [6720/8883 (75.54%)]		Loss: 0.65866
Training Progress: 	Epoch 76 [7040/8883 (79.14%)]		Loss: 0.84903
Training Progress: 	Epoch 76 [7360/8883 (82.73%)]		Loss: 0.60619
Training Progress: 	Epoch 76 [7680/8883 (86.33%)]		Loss: 0.34786
Training Progress: 	Epoch 76 [8000/8883 (89.93%)]		Loss: 0.40720
Training Progress: 	Epoch 76 [8320/8883 (93.53%)]		Loss: 0.50051
Training Progress: 	Epoch 76 [8640/8883 (97.12%)]		Loss: 0.55785
	Train loss: 0.01244, Accuracy: 7088/8883 (79.00%)
	Validation loss: 0.00020, Accuracy: 1492/1692 (88.00%)
	Test loss: 0.00175, Accuracy: 548/1772 (30.00%)

Training Progress: 	Epoch 77 [0/8883 (0.00%)]		Loss: 0.46940
Training Progress: 	Epoch 77 [320/8883 (3.60%)]		Loss: 0.51982
Training Progress: 	Epoch 77 [640/8883 (7.19%)]		Loss: 0.54885
Training Progress: 	Epoch 77 [960/8883 (10.79%)]		Loss: 0.46906
Training Progress: 	Epoch 77 [1280/8883 (14.39%)]		Loss: 0.64703
Training Progress: 	Epoch 77 [1600/8883 (17.99%)]		Loss: 0.59565
Training Progress: 	Epoch 77 [1920/8883 (21.58%)]		Loss: 0.59208
Training Progress: 	Epoch 77 [2240/8883 (25.18%)]		Loss: 0.48836
Training Progress: 	Epoch 77 [2560/8883 (28.78%)]		Loss: 0.58613
Training Progress: 	Epoch 77 [2880/8883 (32.37%)]		Loss: 0.47200
Training Progress: 	Epoch 77 [3200/8883 (35.97%)]		Loss: 0.53483
Training Progress: 	Epoch 77 [3520/8883 (39.57%)]		Loss: 0.83474
Training Progress: 	Epoch 77 [3840/8883 (43.17%)]		Loss: 0.63849
Training Progress: 	Epoch 77 [4160/8883 (46.76%)]		Loss: 0.31279
Training Progress: 	Epoch 77 [4480/8883 (50.36%)]		Loss: 0.47980
Training Progress: 	Epoch 77 [4800/8883 (53.96%)]		Loss: 0.32675
Training Progress: 	Epoch 77 [5120/8883 (57.55%)]		Loss: 0.53039
Training Progress: 	Epoch 77 [5440/8883 (61.15%)]		Loss: 0.51397
Training Progress: 	Epoch 77 [5760/8883 (64.75%)]		Loss: 0.43962
Training Progress: 	Epoch 77 [6080/8883 (68.35%)]		Loss: 0.22915
Training Progress: 	Epoch 77 [6400/8883 (71.94%)]		Loss: 0.46140
Training Progress: 	Epoch 77 [6720/8883 (75.54%)]		Loss: 0.58849
Training Progress: 	Epoch 77 [7040/8883 (79.14%)]		Loss: 0.71389
Training Progress: 	Epoch 77 [7360/8883 (82.73%)]		Loss: 0.56676
Training Progress: 	Epoch 77 [7680/8883 (86.33%)]		Loss: 0.43589
Training Progress: 	Epoch 77 [8000/8883 (89.93%)]		Loss: 0.56435
Training Progress: 	Epoch 77 [8320/8883 (93.53%)]		Loss: 0.66095
Training Progress: 	Epoch 77 [8640/8883 (97.12%)]		Loss: 0.74412
	Train loss: 0.01279, Accuracy: 7082/8883 (79.00%)
	Validation loss: 0.00022, Accuracy: 1478/1692 (87.00%)
	Test loss: 0.00179, Accuracy: 500/1772 (28.00%)

Training Progress: 	Epoch 78 [0/8883 (0.00%)]		Loss: 0.45727
Training Progress: 	Epoch 78 [320/8883 (3.60%)]		Loss: 0.39006
Training Progress: 	Epoch 78 [640/8883 (7.19%)]		Loss: 0.46181
Training Progress: 	Epoch 78 [960/8883 (10.79%)]		Loss: 0.51738
Training Progress: 	Epoch 78 [1280/8883 (14.39%)]		Loss: 0.70329
Training Progress: 	Epoch 78 [1600/8883 (17.99%)]		Loss: 0.36438
Training Progress: 	Epoch 78 [1920/8883 (21.58%)]		Loss: 0.40134
Training Progress: 	Epoch 78 [2240/8883 (25.18%)]		Loss: 0.47888
Training Progress: 	Epoch 78 [2560/8883 (28.78%)]		Loss: 0.48517
Training Progress: 	Epoch 78 [2880/8883 (32.37%)]		Loss: 0.59343
Training Progress: 	Epoch 78 [3200/8883 (35.97%)]		Loss: 0.70706
Training Progress: 	Epoch 78 [3520/8883 (39.57%)]		Loss: 0.86184
Training Progress: 	Epoch 78 [3840/8883 (43.17%)]		Loss: 0.63762
Training Progress: 	Epoch 78 [4160/8883 (46.76%)]		Loss: 0.40612
Training Progress: 	Epoch 78 [4480/8883 (50.36%)]		Loss: 0.45216
Training Progress: 	Epoch 78 [4800/8883 (53.96%)]		Loss: 0.33054
Training Progress: 	Epoch 78 [5120/8883 (57.55%)]		Loss: 0.56977
Training Progress: 	Epoch 78 [5440/8883 (61.15%)]		Loss: 0.61644
Training Progress: 	Epoch 78 [5760/8883 (64.75%)]		Loss: 0.44388
Training Progress: 	Epoch 78 [6080/8883 (68.35%)]		Loss: 0.43400
Training Progress: 	Epoch 78 [6400/8883 (71.94%)]		Loss: 0.45382
Training Progress: 	Epoch 78 [6720/8883 (75.54%)]		Loss: 0.59120
Training Progress: 	Epoch 78 [7040/8883 (79.14%)]		Loss: 0.61496
Training Progress: 	Epoch 78 [7360/8883 (82.73%)]		Loss: 0.57747
Training Progress: 	Epoch 78 [7680/8883 (86.33%)]		Loss: 0.40599
Training Progress: 	Epoch 78 [8000/8883 (89.93%)]		Loss: 0.54248
Training Progress: 	Epoch 78 [8320/8883 (93.53%)]		Loss: 0.53772
Training Progress: 	Epoch 78 [8640/8883 (97.12%)]		Loss: 0.49413
	Train loss: 0.01271, Accuracy: 7080/8883 (79.00%)
	Validation loss: 0.00024, Accuracy: 1469/1692 (86.00%)
	Test loss: 0.00187, Accuracy: 507/1772 (28.00%)

Training Progress: 	Epoch 79 [0/8883 (0.00%)]		Loss: 0.39316
Training Progress: 	Epoch 79 [320/8883 (3.60%)]		Loss: 0.65577
Training Progress: 	Epoch 79 [640/8883 (7.19%)]		Loss: 0.47224
Training Progress: 	Epoch 79 [960/8883 (10.79%)]		Loss: 0.57234
Training Progress: 	Epoch 79 [1280/8883 (14.39%)]		Loss: 0.45279
Training Progress: 	Epoch 79 [1600/8883 (17.99%)]		Loss: 0.39769
Training Progress: 	Epoch 79 [1920/8883 (21.58%)]		Loss: 0.49275
Training Progress: 	Epoch 79 [2240/8883 (25.18%)]		Loss: 0.50648
Training Progress: 	Epoch 79 [2560/8883 (28.78%)]		Loss: 0.71600
Training Progress: 	Epoch 79 [2880/8883 (32.37%)]		Loss: 0.47073
Training Progress: 	Epoch 79 [3200/8883 (35.97%)]		Loss: 0.42931
Training Progress: 	Epoch 79 [3520/8883 (39.57%)]		Loss: 0.55678
Training Progress: 	Epoch 79 [3840/8883 (43.17%)]		Loss: 0.68147
Training Progress: 	Epoch 79 [4160/8883 (46.76%)]		Loss: 0.35942
Training Progress: 	Epoch 79 [4480/8883 (50.36%)]		Loss: 0.42968
Training Progress: 	Epoch 79 [4800/8883 (53.96%)]		Loss: 0.42090
Training Progress: 	Epoch 79 [5120/8883 (57.55%)]		Loss: 0.57962
Training Progress: 	Epoch 79 [5440/8883 (61.15%)]		Loss: 0.51759
Training Progress: 	Epoch 79 [5760/8883 (64.75%)]		Loss: 0.57520
Training Progress: 	Epoch 79 [6080/8883 (68.35%)]		Loss: 0.31024
Training Progress: 	Epoch 79 [6400/8883 (71.94%)]		Loss: 0.48090
Training Progress: 	Epoch 79 [6720/8883 (75.54%)]		Loss: 0.56198
Training Progress: 	Epoch 79 [7040/8883 (79.14%)]		Loss: 0.88455
Training Progress: 	Epoch 79 [7360/8883 (82.73%)]		Loss: 0.50442
Training Progress: 	Epoch 79 [7680/8883 (86.33%)]		Loss: 0.65477
Training Progress: 	Epoch 79 [8000/8883 (89.93%)]		Loss: 0.54683
Training Progress: 	Epoch 79 [8320/8883 (93.53%)]		Loss: 0.61402
Training Progress: 	Epoch 79 [8640/8883 (97.12%)]		Loss: 0.77084
	Train loss: 0.01359, Accuracy: 6984/8883 (78.00%)
	Validation loss: 0.00028, Accuracy: 1450/1692 (85.00%)
	Test loss: 0.00192, Accuracy: 503/1772 (28.00%)

Training Progress: 	Epoch 80 [0/8883 (0.00%)]		Loss: 0.52183
Training Progress: 	Epoch 80 [320/8883 (3.60%)]		Loss: 0.43906
Training Progress: 	Epoch 80 [640/8883 (7.19%)]		Loss: 0.38688
Training Progress: 	Epoch 80 [960/8883 (10.79%)]		Loss: 0.47611
Training Progress: 	Epoch 80 [1280/8883 (14.39%)]		Loss: 0.50953
Training Progress: 	Epoch 80 [1600/8883 (17.99%)]		Loss: 0.42984
Training Progress: 	Epoch 80 [1920/8883 (21.58%)]		Loss: 0.48030
Training Progress: 	Epoch 80 [2240/8883 (25.18%)]		Loss: 0.59286
Training Progress: 	Epoch 80 [2560/8883 (28.78%)]		Loss: 0.69821
Training Progress: 	Epoch 80 [2880/8883 (32.37%)]		Loss: 0.68055
Training Progress: 	Epoch 80 [3200/8883 (35.97%)]		Loss: 0.44947
Training Progress: 	Epoch 80 [3520/8883 (39.57%)]		Loss: 0.56894
Training Progress: 	Epoch 80 [3840/8883 (43.17%)]		Loss: 0.72765
Training Progress: 	Epoch 80 [4160/8883 (46.76%)]		Loss: 0.47291
Training Progress: 	Epoch 80 [4480/8883 (50.36%)]		Loss: 0.52488
Training Progress: 	Epoch 80 [4800/8883 (53.96%)]		Loss: 0.30381
Training Progress: 	Epoch 80 [5120/8883 (57.55%)]		Loss: 0.65599
Training Progress: 	Epoch 80 [5440/8883 (61.15%)]		Loss: 0.57483
Training Progress: 	Epoch 80 [5760/8883 (64.75%)]		Loss: 0.40000
Training Progress: 	Epoch 80 [6080/8883 (68.35%)]		Loss: 0.55376
Training Progress: 	Epoch 80 [6400/8883 (71.94%)]		Loss: 0.50484
Training Progress: 	Epoch 80 [6720/8883 (75.54%)]		Loss: 1.34109
Training Progress: 	Epoch 80 [7040/8883 (79.14%)]		Loss: 0.62144
Training Progress: 	Epoch 80 [7360/8883 (82.73%)]		Loss: 0.60024
Training Progress: 	Epoch 80 [7680/8883 (86.33%)]		Loss: 0.46148
Training Progress: 	Epoch 80 [8000/8883 (89.93%)]		Loss: 0.43482
Training Progress: 	Epoch 80 [8320/8883 (93.53%)]		Loss: 0.53014
Training Progress: 	Epoch 80 [8640/8883 (97.12%)]		Loss: 0.46703
	Train loss: 0.01377, Accuracy: 6953/8883 (78.00%)
	Validation loss: 0.00028, Accuracy: 1441/1692 (85.00%)
	Test loss: 0.00176, Accuracy: 514/1772 (29.00%)

Training Progress: 	Epoch 81 [0/8883 (0.00%)]		Loss: 0.37426
Training Progress: 	Epoch 81 [320/8883 (3.60%)]		Loss: 0.41683
Training Progress: 	Epoch 81 [640/8883 (7.19%)]		Loss: 0.30932
Training Progress: 	Epoch 81 [960/8883 (10.79%)]		Loss: 0.47623
Training Progress: 	Epoch 81 [1280/8883 (14.39%)]		Loss: 0.41810
Training Progress: 	Epoch 81 [1600/8883 (17.99%)]		Loss: 0.36509
Training Progress: 	Epoch 81 [1920/8883 (21.58%)]		Loss: 0.38027
Training Progress: 	Epoch 81 [2240/8883 (25.18%)]		Loss: 0.44059
Training Progress: 	Epoch 81 [2560/8883 (28.78%)]		Loss: 0.68777
Training Progress: 	Epoch 81 [2880/8883 (32.37%)]		Loss: 0.37549
Training Progress: 	Epoch 81 [3200/8883 (35.97%)]		Loss: 0.54997
Training Progress: 	Epoch 81 [3520/8883 (39.57%)]		Loss: 0.82471
Training Progress: 	Epoch 81 [3840/8883 (43.17%)]		Loss: 0.62217
Training Progress: 	Epoch 81 [4160/8883 (46.76%)]		Loss: 0.38742
Training Progress: 	Epoch 81 [4480/8883 (50.36%)]		Loss: 0.58919
Training Progress: 	Epoch 81 [4800/8883 (53.96%)]		Loss: 0.33897
Training Progress: 	Epoch 81 [5120/8883 (57.55%)]		Loss: 0.68734
Training Progress: 	Epoch 81 [5440/8883 (61.15%)]		Loss: 0.46312
Training Progress: 	Epoch 81 [5760/8883 (64.75%)]		Loss: 0.40595
Training Progress: 	Epoch 81 [6080/8883 (68.35%)]		Loss: 0.38342
Training Progress: 	Epoch 81 [6400/8883 (71.94%)]		Loss: 0.62363
Training Progress: 	Epoch 81 [6720/8883 (75.54%)]		Loss: 0.66861
Training Progress: 	Epoch 81 [7040/8883 (79.14%)]		Loss: 0.76858
Training Progress: 	Epoch 81 [7360/8883 (82.73%)]		Loss: 0.60123
Training Progress: 	Epoch 81 [7680/8883 (86.33%)]		Loss: 0.60076
Training Progress: 	Epoch 81 [8000/8883 (89.93%)]		Loss: 0.55620
Training Progress: 	Epoch 81 [8320/8883 (93.53%)]		Loss: 0.57555
Training Progress: 	Epoch 81 [8640/8883 (97.12%)]		Loss: 0.39101
	Train loss: 0.01273, Accuracy: 7075/8883 (79.00%)
	Validation loss: 0.00022, Accuracy: 1496/1692 (88.00%)
	Test loss: 0.00182, Accuracy: 501/1772 (28.00%)

Training Progress: 	Epoch 82 [0/8883 (0.00%)]		Loss: 0.49210
Training Progress: 	Epoch 82 [320/8883 (3.60%)]		Loss: 0.68878
Training Progress: 	Epoch 82 [640/8883 (7.19%)]		Loss: 0.26552
Training Progress: 	Epoch 82 [960/8883 (10.79%)]		Loss: 0.57158
Training Progress: 	Epoch 82 [1280/8883 (14.39%)]		Loss: 0.60940
Training Progress: 	Epoch 82 [1600/8883 (17.99%)]		Loss: 0.29265
Training Progress: 	Epoch 82 [1920/8883 (21.58%)]		Loss: 0.35387
Training Progress: 	Epoch 82 [2240/8883 (25.18%)]		Loss: 0.57812
Training Progress: 	Epoch 82 [2560/8883 (28.78%)]		Loss: 0.86960
Training Progress: 	Epoch 82 [2880/8883 (32.37%)]		Loss: 0.46789
Training Progress: 	Epoch 82 [3200/8883 (35.97%)]		Loss: 0.39314
Training Progress: 	Epoch 82 [3520/8883 (39.57%)]		Loss: 0.63881
Training Progress: 	Epoch 82 [3840/8883 (43.17%)]		Loss: 0.64229
Training Progress: 	Epoch 82 [4160/8883 (46.76%)]		Loss: 0.36721
Training Progress: 	Epoch 82 [4480/8883 (50.36%)]		Loss: 0.43672
Training Progress: 	Epoch 82 [4800/8883 (53.96%)]		Loss: 0.37509
Training Progress: 	Epoch 82 [5120/8883 (57.55%)]		Loss: 0.43164
Training Progress: 	Epoch 82 [5440/8883 (61.15%)]		Loss: 0.48274
Training Progress: 	Epoch 82 [5760/8883 (64.75%)]		Loss: 0.37574
Training Progress: 	Epoch 82 [6080/8883 (68.35%)]		Loss: 0.40944
Training Progress: 	Epoch 82 [6400/8883 (71.94%)]		Loss: 0.48065
Training Progress: 	Epoch 82 [6720/8883 (75.54%)]		Loss: 0.54005
Training Progress: 	Epoch 82 [7040/8883 (79.14%)]		Loss: 0.66521
Training Progress: 	Epoch 82 [7360/8883 (82.73%)]		Loss: 0.51834
Training Progress: 	Epoch 82 [7680/8883 (86.33%)]		Loss: 0.36296
Training Progress: 	Epoch 82 [8000/8883 (89.93%)]		Loss: 0.56256
Training Progress: 	Epoch 82 [8320/8883 (93.53%)]		Loss: 0.46063
Training Progress: 	Epoch 82 [8640/8883 (97.12%)]		Loss: 0.73022
	Train loss: 0.01280, Accuracy: 7063/8883 (79.00%)
	Validation loss: 0.00020, Accuracy: 1496/1692 (88.00%)
	Test loss: 0.00178, Accuracy: 515/1772 (29.00%)

Training Progress: 	Epoch 83 [0/8883 (0.00%)]		Loss: 0.63909
Training Progress: 	Epoch 83 [320/8883 (3.60%)]		Loss: 0.65443
Training Progress: 	Epoch 83 [640/8883 (7.19%)]		Loss: 0.33582
Training Progress: 	Epoch 83 [960/8883 (10.79%)]		Loss: 0.47644
Training Progress: 	Epoch 83 [1280/8883 (14.39%)]		Loss: 0.62887
Training Progress: 	Epoch 83 [1600/8883 (17.99%)]		Loss: 0.61214
Training Progress: 	Epoch 83 [1920/8883 (21.58%)]		Loss: 0.79453
Training Progress: 	Epoch 83 [2240/8883 (25.18%)]		Loss: 0.45709
Training Progress: 	Epoch 83 [2560/8883 (28.78%)]		Loss: 0.66083
Training Progress: 	Epoch 83 [2880/8883 (32.37%)]		Loss: 0.41459
Training Progress: 	Epoch 83 [3200/8883 (35.97%)]		Loss: 0.42715
Training Progress: 	Epoch 83 [3520/8883 (39.57%)]		Loss: 0.69688
Training Progress: 	Epoch 83 [3840/8883 (43.17%)]		Loss: 0.71613
Training Progress: 	Epoch 83 [4160/8883 (46.76%)]		Loss: 0.32512
Training Progress: 	Epoch 83 [4480/8883 (50.36%)]		Loss: 0.28033
Training Progress: 	Epoch 83 [4800/8883 (53.96%)]		Loss: 0.50194
Training Progress: 	Epoch 83 [5120/8883 (57.55%)]		Loss: 0.56844
Training Progress: 	Epoch 83 [5440/8883 (61.15%)]		Loss: 0.80440
Training Progress: 	Epoch 83 [5760/8883 (64.75%)]		Loss: 0.54596
Training Progress: 	Epoch 83 [6080/8883 (68.35%)]		Loss: 0.46522
Training Progress: 	Epoch 83 [6400/8883 (71.94%)]		Loss: 0.60714
Training Progress: 	Epoch 83 [6720/8883 (75.54%)]		Loss: 0.58976
Training Progress: 	Epoch 83 [7040/8883 (79.14%)]		Loss: 0.64568
Training Progress: 	Epoch 83 [7360/8883 (82.73%)]		Loss: 0.49595
Training Progress: 	Epoch 83 [7680/8883 (86.33%)]		Loss: 0.53845
Training Progress: 	Epoch 83 [8000/8883 (89.93%)]		Loss: 0.40773
Training Progress: 	Epoch 83 [8320/8883 (93.53%)]		Loss: 0.64481
Training Progress: 	Epoch 83 [8640/8883 (97.12%)]		Loss: 0.58328
	Train loss: 0.01238, Accuracy: 7111/8883 (80.00%)
	Validation loss: 0.00020, Accuracy: 1500/1692 (88.00%)
	Test loss: 0.00172, Accuracy: 567/1772 (31.00%)

Training Progress: 	Epoch 84 [0/8883 (0.00%)]		Loss: 0.61259
Training Progress: 	Epoch 84 [320/8883 (3.60%)]		Loss: 0.63460
Training Progress: 	Epoch 84 [640/8883 (7.19%)]		Loss: 0.31248
Training Progress: 	Epoch 84 [960/8883 (10.79%)]		Loss: 0.51822
Training Progress: 	Epoch 84 [1280/8883 (14.39%)]		Loss: 0.45387
Training Progress: 	Epoch 84 [1600/8883 (17.99%)]		Loss: 0.47632
Training Progress: 	Epoch 84 [1920/8883 (21.58%)]		Loss: 0.31251
Training Progress: 	Epoch 84 [2240/8883 (25.18%)]		Loss: 0.38702
Training Progress: 	Epoch 84 [2560/8883 (28.78%)]		Loss: 0.73927
Training Progress: 	Epoch 84 [2880/8883 (32.37%)]		Loss: 0.38680
Training Progress: 	Epoch 84 [3200/8883 (35.97%)]		Loss: 0.75630
Training Progress: 	Epoch 84 [3520/8883 (39.57%)]		Loss: 0.60613
Training Progress: 	Epoch 84 [3840/8883 (43.17%)]		Loss: 0.56461
Training Progress: 	Epoch 84 [4160/8883 (46.76%)]		Loss: 0.31170
Training Progress: 	Epoch 84 [4480/8883 (50.36%)]		Loss: 0.34436
Training Progress: 	Epoch 84 [4800/8883 (53.96%)]		Loss: 0.30709
Training Progress: 	Epoch 84 [5120/8883 (57.55%)]		Loss: 0.50875
Training Progress: 	Epoch 84 [5440/8883 (61.15%)]		Loss: 0.70047
Training Progress: 	Epoch 84 [5760/8883 (64.75%)]		Loss: 0.62136
Training Progress: 	Epoch 84 [6080/8883 (68.35%)]		Loss: 0.37533
Training Progress: 	Epoch 84 [6400/8883 (71.94%)]		Loss: 0.67591
Training Progress: 	Epoch 84 [6720/8883 (75.54%)]		Loss: 0.55282
Training Progress: 	Epoch 84 [7040/8883 (79.14%)]		Loss: 0.64340
Training Progress: 	Epoch 84 [7360/8883 (82.73%)]		Loss: 0.55397
Training Progress: 	Epoch 84 [7680/8883 (86.33%)]		Loss: 0.48611
Training Progress: 	Epoch 84 [8000/8883 (89.93%)]		Loss: 0.37512
Training Progress: 	Epoch 84 [8320/8883 (93.53%)]		Loss: 0.41365
Training Progress: 	Epoch 84 [8640/8883 (97.12%)]		Loss: 0.56385
	Train loss: 0.01289, Accuracy: 7070/8883 (79.00%)
	Validation loss: 0.00023, Accuracy: 1482/1692 (87.00%)
	Test loss: 0.00187, Accuracy: 552/1772 (31.00%)

Training Progress: 	Epoch 85 [0/8883 (0.00%)]		Loss: 0.48015
Training Progress: 	Epoch 85 [320/8883 (3.60%)]		Loss: 0.63260
Training Progress: 	Epoch 85 [640/8883 (7.19%)]		Loss: 0.39504
Training Progress: 	Epoch 85 [960/8883 (10.79%)]		Loss: 0.44003
Training Progress: 	Epoch 85 [1280/8883 (14.39%)]		Loss: 0.61184
Training Progress: 	Epoch 85 [1600/8883 (17.99%)]		Loss: 0.29770
Training Progress: 	Epoch 85 [1920/8883 (21.58%)]		Loss: 0.33146
Training Progress: 	Epoch 85 [2240/8883 (25.18%)]		Loss: 0.40700
Training Progress: 	Epoch 85 [2560/8883 (28.78%)]		Loss: 0.61279
Training Progress: 	Epoch 85 [2880/8883 (32.37%)]		Loss: 0.42339
Training Progress: 	Epoch 85 [3200/8883 (35.97%)]		Loss: 0.46878
Training Progress: 	Epoch 85 [3520/8883 (39.57%)]		Loss: 0.77269
Training Progress: 	Epoch 85 [3840/8883 (43.17%)]		Loss: 0.53683
Training Progress: 	Epoch 85 [4160/8883 (46.76%)]		Loss: 0.32873
Training Progress: 	Epoch 85 [4480/8883 (50.36%)]		Loss: 0.36243
Training Progress: 	Epoch 85 [4800/8883 (53.96%)]		Loss: 0.48631
Training Progress: 	Epoch 85 [5120/8883 (57.55%)]		Loss: 0.45249
Training Progress: 	Epoch 85 [5440/8883 (61.15%)]		Loss: 0.45950
Training Progress: 	Epoch 85 [5760/8883 (64.75%)]		Loss: 0.52496
Training Progress: 	Epoch 85 [6080/8883 (68.35%)]		Loss: 0.47529
Training Progress: 	Epoch 85 [6400/8883 (71.94%)]		Loss: 0.80277
Training Progress: 	Epoch 85 [6720/8883 (75.54%)]		Loss: 0.54872
Training Progress: 	Epoch 85 [7040/8883 (79.14%)]		Loss: 0.58183
Training Progress: 	Epoch 85 [7360/8883 (82.73%)]		Loss: 0.53092
Training Progress: 	Epoch 85 [7680/8883 (86.33%)]		Loss: 0.46036
Training Progress: 	Epoch 85 [8000/8883 (89.93%)]		Loss: 0.41420
Training Progress: 	Epoch 85 [8320/8883 (93.53%)]		Loss: 0.66074
Training Progress: 	Epoch 85 [8640/8883 (97.12%)]		Loss: 0.59990
	Train loss: 0.01295, Accuracy: 7073/8883 (79.00%)
	Validation loss: 0.00026, Accuracy: 1447/1692 (85.00%)
	Test loss: 0.00183, Accuracy: 530/1772 (29.00%)

Training Progress: 	Epoch 86 [0/8883 (0.00%)]		Loss: 0.33464
Training Progress: 	Epoch 86 [320/8883 (3.60%)]		Loss: 0.63496
Training Progress: 	Epoch 86 [640/8883 (7.19%)]		Loss: 0.35788
Training Progress: 	Epoch 86 [960/8883 (10.79%)]		Loss: 0.45194
Training Progress: 	Epoch 86 [1280/8883 (14.39%)]		Loss: 0.58645
Training Progress: 	Epoch 86 [1600/8883 (17.99%)]		Loss: 0.32190
Training Progress: 	Epoch 86 [1920/8883 (21.58%)]		Loss: 0.44932
Training Progress: 	Epoch 86 [2240/8883 (25.18%)]		Loss: 0.44420
Training Progress: 	Epoch 86 [2560/8883 (28.78%)]		Loss: 0.73085
Training Progress: 	Epoch 86 [2880/8883 (32.37%)]		Loss: 0.43558
Training Progress: 	Epoch 86 [3200/8883 (35.97%)]		Loss: 0.38503
Training Progress: 	Epoch 86 [3520/8883 (39.57%)]		Loss: 0.64128
Training Progress: 	Epoch 86 [3840/8883 (43.17%)]		Loss: 0.53204
Training Progress: 	Epoch 86 [4160/8883 (46.76%)]		Loss: 0.35996
Training Progress: 	Epoch 86 [4480/8883 (50.36%)]		Loss: 0.47883
Training Progress: 	Epoch 86 [4800/8883 (53.96%)]		Loss: 0.32079
Training Progress: 	Epoch 86 [5120/8883 (57.55%)]		Loss: 0.50206
Training Progress: 	Epoch 86 [5440/8883 (61.15%)]		Loss: 0.72123
Training Progress: 	Epoch 86 [5760/8883 (64.75%)]		Loss: 0.62676
Training Progress: 	Epoch 86 [6080/8883 (68.35%)]		Loss: 0.26910
Training Progress: 	Epoch 86 [6400/8883 (71.94%)]		Loss: 0.59066
Training Progress: 	Epoch 86 [6720/8883 (75.54%)]		Loss: 0.74065
Training Progress: 	Epoch 86 [7040/8883 (79.14%)]		Loss: 0.62709
Training Progress: 	Epoch 86 [7360/8883 (82.73%)]		Loss: 0.53144
Training Progress: 	Epoch 86 [7680/8883 (86.33%)]		Loss: 0.43828
Training Progress: 	Epoch 86 [8000/8883 (89.93%)]		Loss: 0.47248
Training Progress: 	Epoch 86 [8320/8883 (93.53%)]		Loss: 0.52857
Training Progress: 	Epoch 86 [8640/8883 (97.12%)]		Loss: 0.46049
	Train loss: 0.01237, Accuracy: 7121/8883 (80.00%)
	Validation loss: 0.00020, Accuracy: 1498/1692 (88.00%)
	Test loss: 0.00176, Accuracy: 548/1772 (30.00%)

Training Progress: 	Epoch 87 [0/8883 (0.00%)]		Loss: 0.37986
Training Progress: 	Epoch 87 [320/8883 (3.60%)]		Loss: 0.42978
Training Progress: 	Epoch 87 [640/8883 (7.19%)]		Loss: 0.29723
Training Progress: 	Epoch 87 [960/8883 (10.79%)]		Loss: 0.53403
Training Progress: 	Epoch 87 [1280/8883 (14.39%)]		Loss: 0.46307
Training Progress: 	Epoch 87 [1600/8883 (17.99%)]		Loss: 0.40515
Training Progress: 	Epoch 87 [1920/8883 (21.58%)]		Loss: 0.47176
Training Progress: 	Epoch 87 [2240/8883 (25.18%)]		Loss: 0.43432
Training Progress: 	Epoch 87 [2560/8883 (28.78%)]		Loss: 0.44634
Training Progress: 	Epoch 87 [2880/8883 (32.37%)]		Loss: 0.41906
Training Progress: 	Epoch 87 [3200/8883 (35.97%)]		Loss: 0.45054
Training Progress: 	Epoch 87 [3520/8883 (39.57%)]		Loss: 0.56107
Training Progress: 	Epoch 87 [3840/8883 (43.17%)]		Loss: 0.56728
Training Progress: 	Epoch 87 [4160/8883 (46.76%)]		Loss: 0.34022
Training Progress: 	Epoch 87 [4480/8883 (50.36%)]		Loss: 0.40588
Training Progress: 	Epoch 87 [4800/8883 (53.96%)]		Loss: 0.38833
Training Progress: 	Epoch 87 [5120/8883 (57.55%)]		Loss: 0.77912
Training Progress: 	Epoch 87 [5440/8883 (61.15%)]		Loss: 0.55573
Training Progress: 	Epoch 87 [5760/8883 (64.75%)]		Loss: 0.34818
Training Progress: 	Epoch 87 [6080/8883 (68.35%)]		Loss: 0.42303
Training Progress: 	Epoch 87 [6400/8883 (71.94%)]		Loss: 0.53025
Training Progress: 	Epoch 87 [6720/8883 (75.54%)]		Loss: 0.69107
Training Progress: 	Epoch 87 [7040/8883 (79.14%)]		Loss: 0.71268
Training Progress: 	Epoch 87 [7360/8883 (82.73%)]		Loss: 0.54955
Training Progress: 	Epoch 87 [7680/8883 (86.33%)]		Loss: 0.44012
Training Progress: 	Epoch 87 [8000/8883 (89.93%)]		Loss: 0.47286
Training Progress: 	Epoch 87 [8320/8883 (93.53%)]		Loss: 0.62714
Training Progress: 	Epoch 87 [8640/8883 (97.12%)]		Loss: 0.41120
	Train loss: 0.01313, Accuracy: 7025/8883 (79.00%)
	Validation loss: 0.00024, Accuracy: 1476/1692 (87.00%)
	Test loss: 0.00179, Accuracy: 530/1772 (29.00%)

Training Progress: 	Epoch 88 [0/8883 (0.00%)]		Loss: 0.37710
Training Progress: 	Epoch 88 [320/8883 (3.60%)]		Loss: 0.68983
Training Progress: 	Epoch 88 [640/8883 (7.19%)]		Loss: 0.55286
Training Progress: 	Epoch 88 [960/8883 (10.79%)]		Loss: 0.59715
Training Progress: 	Epoch 88 [1280/8883 (14.39%)]		Loss: 0.66574
Training Progress: 	Epoch 88 [1600/8883 (17.99%)]		Loss: 0.34509
Training Progress: 	Epoch 88 [1920/8883 (21.58%)]		Loss: 0.37539
Training Progress: 	Epoch 88 [2240/8883 (25.18%)]		Loss: 0.47185
Training Progress: 	Epoch 88 [2560/8883 (28.78%)]		Loss: 0.56664
Training Progress: 	Epoch 88 [2880/8883 (32.37%)]		Loss: 0.43445
Training Progress: 	Epoch 88 [3200/8883 (35.97%)]		Loss: 0.48712
Training Progress: 	Epoch 88 [3520/8883 (39.57%)]		Loss: 0.63980
Training Progress: 	Epoch 88 [3840/8883 (43.17%)]		Loss: 0.70204
Training Progress: 	Epoch 88 [4160/8883 (46.76%)]		Loss: 0.54817
Training Progress: 	Epoch 88 [4480/8883 (50.36%)]		Loss: 0.52735
Training Progress: 	Epoch 88 [4800/8883 (53.96%)]		Loss: 0.48022
Training Progress: 	Epoch 88 [5120/8883 (57.55%)]		Loss: 0.55119
Training Progress: 	Epoch 88 [5440/8883 (61.15%)]		Loss: 0.54731
Training Progress: 	Epoch 88 [5760/8883 (64.75%)]		Loss: 0.39713
Training Progress: 	Epoch 88 [6080/8883 (68.35%)]		Loss: 0.37220
Training Progress: 	Epoch 88 [6400/8883 (71.94%)]		Loss: 0.37924
Training Progress: 	Epoch 88 [6720/8883 (75.54%)]		Loss: 0.59577
Training Progress: 	Epoch 88 [7040/8883 (79.14%)]		Loss: 0.55931
Training Progress: 	Epoch 88 [7360/8883 (82.73%)]		Loss: 0.55334
Training Progress: 	Epoch 88 [7680/8883 (86.33%)]		Loss: 0.42321
Training Progress: 	Epoch 88 [8000/8883 (89.93%)]		Loss: 0.58385
Training Progress: 	Epoch 88 [8320/8883 (93.53%)]		Loss: 0.49874
Training Progress: 	Epoch 88 [8640/8883 (97.12%)]		Loss: 0.46532
	Train loss: 0.01184, Accuracy: 7175/8883 (80.00%)
	Validation loss: 0.00018, Accuracy: 1517/1692 (89.00%)
	Test loss: 0.00180, Accuracy: 528/1772 (29.00%)

Training Progress: 	Epoch 89 [0/8883 (0.00%)]		Loss: 0.40179
Training Progress: 	Epoch 89 [320/8883 (3.60%)]		Loss: 0.48405
Training Progress: 	Epoch 89 [640/8883 (7.19%)]		Loss: 0.39828
Training Progress: 	Epoch 89 [960/8883 (10.79%)]		Loss: 0.48164
Training Progress: 	Epoch 89 [1280/8883 (14.39%)]		Loss: 0.69343
Training Progress: 	Epoch 89 [1600/8883 (17.99%)]		Loss: 0.42192
Training Progress: 	Epoch 89 [1920/8883 (21.58%)]		Loss: 0.36917
Training Progress: 	Epoch 89 [2240/8883 (25.18%)]		Loss: 0.54361
Training Progress: 	Epoch 89 [2560/8883 (28.78%)]		Loss: 0.72744
Training Progress: 	Epoch 89 [2880/8883 (32.37%)]		Loss: 0.58360
Training Progress: 	Epoch 89 [3200/8883 (35.97%)]		Loss: 0.40795
Training Progress: 	Epoch 89 [3520/8883 (39.57%)]		Loss: 0.55234
Training Progress: 	Epoch 89 [3840/8883 (43.17%)]		Loss: 0.59761
Training Progress: 	Epoch 89 [4160/8883 (46.76%)]		Loss: 0.31431
Training Progress: 	Epoch 89 [4480/8883 (50.36%)]		Loss: 0.34376
Training Progress: 	Epoch 89 [4800/8883 (53.96%)]		Loss: 0.30780
Training Progress: 	Epoch 89 [5120/8883 (57.55%)]		Loss: 0.57174
Training Progress: 	Epoch 89 [5440/8883 (61.15%)]		Loss: 0.42997
Training Progress: 	Epoch 89 [5760/8883 (64.75%)]		Loss: 0.34395
Training Progress: 	Epoch 89 [6080/8883 (68.35%)]		Loss: 0.56866
Training Progress: 	Epoch 89 [6400/8883 (71.94%)]		Loss: 0.43259
Training Progress: 	Epoch 89 [6720/8883 (75.54%)]		Loss: 0.67220
Training Progress: 	Epoch 89 [7040/8883 (79.14%)]		Loss: 0.56785
Training Progress: 	Epoch 89 [7360/8883 (82.73%)]		Loss: 0.56052
Training Progress: 	Epoch 89 [7680/8883 (86.33%)]		Loss: 0.42260
Training Progress: 	Epoch 89 [8000/8883 (89.93%)]		Loss: 0.55833
Training Progress: 	Epoch 89 [8320/8883 (93.53%)]		Loss: 0.50810
Training Progress: 	Epoch 89 [8640/8883 (97.12%)]		Loss: 0.40867
	Train loss: 0.01248, Accuracy: 7088/8883 (79.00%)
	Validation loss: 0.00020, Accuracy: 1496/1692 (88.00%)
	Test loss: 0.00186, Accuracy: 535/1772 (30.00%)

Training Progress: 	Epoch 90 [0/8883 (0.00%)]		Loss: 0.44281
Training Progress: 	Epoch 90 [320/8883 (3.60%)]		Loss: 0.47556
Training Progress: 	Epoch 90 [640/8883 (7.19%)]		Loss: 0.30332
Training Progress: 	Epoch 90 [960/8883 (10.79%)]		Loss: 0.41694
Training Progress: 	Epoch 90 [1280/8883 (14.39%)]		Loss: 0.48618
Training Progress: 	Epoch 90 [1600/8883 (17.99%)]		Loss: 0.28216
Training Progress: 	Epoch 90 [1920/8883 (21.58%)]		Loss: 0.32597
Training Progress: 	Epoch 90 [2240/8883 (25.18%)]		Loss: 0.44437
Training Progress: 	Epoch 90 [2560/8883 (28.78%)]		Loss: 0.63590
Training Progress: 	Epoch 90 [2880/8883 (32.37%)]		Loss: 0.46719
Training Progress: 	Epoch 90 [3200/8883 (35.97%)]		Loss: 0.45103
Training Progress: 	Epoch 90 [3520/8883 (39.57%)]		Loss: 0.69150
Training Progress: 	Epoch 90 [3840/8883 (43.17%)]		Loss: 0.66396
Training Progress: 	Epoch 90 [4160/8883 (46.76%)]		Loss: 0.29498
Training Progress: 	Epoch 90 [4480/8883 (50.36%)]		Loss: 0.44147
Training Progress: 	Epoch 90 [4800/8883 (53.96%)]		Loss: 0.49334
Training Progress: 	Epoch 90 [5120/8883 (57.55%)]		Loss: 0.56606
Training Progress: 	Epoch 90 [5440/8883 (61.15%)]		Loss: 0.56144
Training Progress: 	Epoch 90 [5760/8883 (64.75%)]		Loss: 0.50198
Training Progress: 	Epoch 90 [6080/8883 (68.35%)]		Loss: 0.43325
Training Progress: 	Epoch 90 [6400/8883 (71.94%)]		Loss: 0.40622
Training Progress: 	Epoch 90 [6720/8883 (75.54%)]		Loss: 0.52137
Training Progress: 	Epoch 90 [7040/8883 (79.14%)]		Loss: 0.60531
Training Progress: 	Epoch 90 [7360/8883 (82.73%)]		Loss: 0.51633
Training Progress: 	Epoch 90 [7680/8883 (86.33%)]		Loss: 0.61947
Training Progress: 	Epoch 90 [8000/8883 (89.93%)]		Loss: 0.36940
Training Progress: 	Epoch 90 [8320/8883 (93.53%)]		Loss: 0.44680
Training Progress: 	Epoch 90 [8640/8883 (97.12%)]		Loss: 0.48458
	Train loss: 0.01202, Accuracy: 7141/8883 (80.00%)
	Validation loss: 0.00019, Accuracy: 1523/1692 (90.00%)
	Test loss: 0.00184, Accuracy: 536/1772 (30.00%)

Training Progress: 	Epoch 91 [0/8883 (0.00%)]		Loss: 0.33518
Training Progress: 	Epoch 91 [320/8883 (3.60%)]		Loss: 0.39456
Training Progress: 	Epoch 91 [640/8883 (7.19%)]		Loss: 0.41812
Training Progress: 	Epoch 91 [960/8883 (10.79%)]		Loss: 0.43972
Training Progress: 	Epoch 91 [1280/8883 (14.39%)]		Loss: 0.38489
Training Progress: 	Epoch 91 [1600/8883 (17.99%)]		Loss: 0.29582
Training Progress: 	Epoch 91 [1920/8883 (21.58%)]		Loss: 0.73792
Training Progress: 	Epoch 91 [2240/8883 (25.18%)]		Loss: 0.46955
Training Progress: 	Epoch 91 [2560/8883 (28.78%)]		Loss: 0.72225
Training Progress: 	Epoch 91 [2880/8883 (32.37%)]		Loss: 0.39769
Training Progress: 	Epoch 91 [3200/8883 (35.97%)]		Loss: 0.48899
Training Progress: 	Epoch 91 [3520/8883 (39.57%)]		Loss: 1.23278
Training Progress: 	Epoch 91 [3840/8883 (43.17%)]		Loss: 0.58531
Training Progress: 	Epoch 91 [4160/8883 (46.76%)]		Loss: 0.45710
Training Progress: 	Epoch 91 [4480/8883 (50.36%)]		Loss: 0.41691
Training Progress: 	Epoch 91 [4800/8883 (53.96%)]		Loss: 0.53102
Training Progress: 	Epoch 91 [5120/8883 (57.55%)]		Loss: 0.51204
Training Progress: 	Epoch 91 [5440/8883 (61.15%)]		Loss: 0.46048
Training Progress: 	Epoch 91 [5760/8883 (64.75%)]		Loss: 0.37051
Training Progress: 	Epoch 91 [6080/8883 (68.35%)]		Loss: 0.33678
Training Progress: 	Epoch 91 [6400/8883 (71.94%)]		Loss: 0.46230
Training Progress: 	Epoch 91 [6720/8883 (75.54%)]		Loss: 0.60143
Training Progress: 	Epoch 91 [7040/8883 (79.14%)]		Loss: 0.76049
Training Progress: 	Epoch 91 [7360/8883 (82.73%)]		Loss: 0.46135
Training Progress: 	Epoch 91 [7680/8883 (86.33%)]		Loss: 0.43765
Training Progress: 	Epoch 91 [8000/8883 (89.93%)]		Loss: 0.39050
Training Progress: 	Epoch 91 [8320/8883 (93.53%)]		Loss: 0.68772
Training Progress: 	Epoch 91 [8640/8883 (97.12%)]		Loss: 0.51185
	Train loss: 0.01212, Accuracy: 7153/8883 (80.00%)
	Validation loss: 0.00021, Accuracy: 1484/1692 (87.00%)
	Test loss: 0.00179, Accuracy: 548/1772 (30.00%)

Training Progress: 	Epoch 92 [0/8883 (0.00%)]		Loss: 0.66044
Training Progress: 	Epoch 92 [320/8883 (3.60%)]		Loss: 0.45694
Training Progress: 	Epoch 92 [640/8883 (7.19%)]		Loss: 0.31796
Training Progress: 	Epoch 92 [960/8883 (10.79%)]		Loss: 0.56385
Training Progress: 	Epoch 92 [1280/8883 (14.39%)]		Loss: 0.39738
Training Progress: 	Epoch 92 [1600/8883 (17.99%)]		Loss: 0.33312
Training Progress: 	Epoch 92 [1920/8883 (21.58%)]		Loss: 0.34701
Training Progress: 	Epoch 92 [2240/8883 (25.18%)]		Loss: 0.46950
Training Progress: 	Epoch 92 [2560/8883 (28.78%)]		Loss: 0.51186
Training Progress: 	Epoch 92 [2880/8883 (32.37%)]		Loss: 0.65372
Training Progress: 	Epoch 92 [3200/8883 (35.97%)]		Loss: 0.52922
Training Progress: 	Epoch 92 [3520/8883 (39.57%)]		Loss: 0.58559
Training Progress: 	Epoch 92 [3840/8883 (43.17%)]		Loss: 0.56359
Training Progress: 	Epoch 92 [4160/8883 (46.76%)]		Loss: 0.30090
Training Progress: 	Epoch 92 [4480/8883 (50.36%)]		Loss: 0.36890
Training Progress: 	Epoch 92 [4800/8883 (53.96%)]		Loss: 0.60523
Training Progress: 	Epoch 92 [5120/8883 (57.55%)]		Loss: 0.44346
Training Progress: 	Epoch 92 [5440/8883 (61.15%)]		Loss: 0.45781
Training Progress: 	Epoch 92 [5760/8883 (64.75%)]		Loss: 0.46568
Training Progress: 	Epoch 92 [6080/8883 (68.35%)]		Loss: 0.36151
Training Progress: 	Epoch 92 [6400/8883 (71.94%)]		Loss: 0.52940
Training Progress: 	Epoch 92 [6720/8883 (75.54%)]		Loss: 0.55776
Training Progress: 	Epoch 92 [7040/8883 (79.14%)]		Loss: 0.75430
Training Progress: 	Epoch 92 [7360/8883 (82.73%)]		Loss: 0.50562
Training Progress: 	Epoch 92 [7680/8883 (86.33%)]		Loss: 0.45191
Training Progress: 	Epoch 92 [8000/8883 (89.93%)]		Loss: 0.38554
Training Progress: 	Epoch 92 [8320/8883 (93.53%)]		Loss: 0.51765
Training Progress: 	Epoch 92 [8640/8883 (97.12%)]		Loss: 0.37672
	Train loss: 0.01229, Accuracy: 7117/8883 (80.00%)
	Validation loss: 0.00020, Accuracy: 1516/1692 (89.00%)
	Test loss: 0.00185, Accuracy: 580/1772 (32.00%)

Training Progress: 	Epoch 93 [0/8883 (0.00%)]		Loss: 0.35502
Training Progress: 	Epoch 93 [320/8883 (3.60%)]		Loss: 0.51351
Training Progress: 	Epoch 93 [640/8883 (7.19%)]		Loss: 0.24764
Training Progress: 	Epoch 93 [960/8883 (10.79%)]		Loss: 0.57722
Training Progress: 	Epoch 93 [1280/8883 (14.39%)]		Loss: 0.44431
Training Progress: 	Epoch 93 [1600/8883 (17.99%)]		Loss: 0.44065
Training Progress: 	Epoch 93 [1920/8883 (21.58%)]		Loss: 0.47803
Training Progress: 	Epoch 93 [2240/8883 (25.18%)]		Loss: 0.52149
Training Progress: 	Epoch 93 [2560/8883 (28.78%)]		Loss: 0.58211
Training Progress: 	Epoch 93 [2880/8883 (32.37%)]		Loss: 0.71318
Training Progress: 	Epoch 93 [3200/8883 (35.97%)]		Loss: 0.51849
Training Progress: 	Epoch 93 [3520/8883 (39.57%)]		Loss: 0.65383
Training Progress: 	Epoch 93 [3840/8883 (43.17%)]		Loss: 0.74965
Training Progress: 	Epoch 93 [4160/8883 (46.76%)]		Loss: 0.29198
Training Progress: 	Epoch 93 [4480/8883 (50.36%)]		Loss: 0.39374
Training Progress: 	Epoch 93 [4800/8883 (53.96%)]		Loss: 0.29610
Training Progress: 	Epoch 93 [5120/8883 (57.55%)]		Loss: 0.56569
Training Progress: 	Epoch 93 [5440/8883 (61.15%)]		Loss: 0.46634
Training Progress: 	Epoch 93 [5760/8883 (64.75%)]		Loss: 0.46008
Training Progress: 	Epoch 93 [6080/8883 (68.35%)]		Loss: 0.46781
Training Progress: 	Epoch 93 [6400/8883 (71.94%)]		Loss: 0.59014
Training Progress: 	Epoch 93 [6720/8883 (75.54%)]		Loss: 0.65833
Training Progress: 	Epoch 93 [7040/8883 (79.14%)]		Loss: 0.68148
Training Progress: 	Epoch 93 [7360/8883 (82.73%)]		Loss: 0.48966
Training Progress: 	Epoch 93 [7680/8883 (86.33%)]		Loss: 0.65595
Training Progress: 	Epoch 93 [8000/8883 (89.93%)]		Loss: 0.44622
Training Progress: 	Epoch 93 [8320/8883 (93.53%)]		Loss: 0.63519
Training Progress: 	Epoch 93 [8640/8883 (97.12%)]		Loss: 0.57992
	Train loss: 0.01272, Accuracy: 7065/8883 (79.00%)
	Validation loss: 0.00018, Accuracy: 1519/1692 (89.00%)
	Test loss: 0.00181, Accuracy: 545/1772 (30.00%)

Training Progress: 	Epoch 94 [0/8883 (0.00%)]		Loss: 0.53865
Training Progress: 	Epoch 94 [320/8883 (3.60%)]		Loss: 0.54411
Training Progress: 	Epoch 94 [640/8883 (7.19%)]		Loss: 0.28837
Training Progress: 	Epoch 94 [960/8883 (10.79%)]		Loss: 0.48527
Training Progress: 	Epoch 94 [1280/8883 (14.39%)]		Loss: 0.62247
Training Progress: 	Epoch 94 [1600/8883 (17.99%)]		Loss: 0.52366
Training Progress: 	Epoch 94 [1920/8883 (21.58%)]		Loss: 0.48889
Training Progress: 	Epoch 94 [2240/8883 (25.18%)]		Loss: 0.64175
Training Progress: 	Epoch 94 [2560/8883 (28.78%)]		Loss: 0.51065
Training Progress: 	Epoch 94 [2880/8883 (32.37%)]		Loss: 0.55009
Training Progress: 	Epoch 94 [3200/8883 (35.97%)]		Loss: 0.43030
Training Progress: 	Epoch 94 [3520/8883 (39.57%)]		Loss: 0.50051
Training Progress: 	Epoch 94 [3840/8883 (43.17%)]		Loss: 0.56061
Training Progress: 	Epoch 94 [4160/8883 (46.76%)]		Loss: 0.28670
Training Progress: 	Epoch 94 [4480/8883 (50.36%)]		Loss: 0.32910
Training Progress: 	Epoch 94 [4800/8883 (53.96%)]		Loss: 0.31242
Training Progress: 	Epoch 94 [5120/8883 (57.55%)]		Loss: 0.64542
Training Progress: 	Epoch 94 [5440/8883 (61.15%)]		Loss: 0.77659
Training Progress: 	Epoch 94 [5760/8883 (64.75%)]		Loss: 0.48485
Training Progress: 	Epoch 94 [6080/8883 (68.35%)]		Loss: 0.34926
Training Progress: 	Epoch 94 [6400/8883 (71.94%)]		Loss: 0.41089
Training Progress: 	Epoch 94 [6720/8883 (75.54%)]		Loss: 0.55352
Training Progress: 	Epoch 94 [7040/8883 (79.14%)]		Loss: 0.53193
Training Progress: 	Epoch 94 [7360/8883 (82.73%)]		Loss: 0.61742
Training Progress: 	Epoch 94 [7680/8883 (86.33%)]		Loss: 0.38793
Training Progress: 	Epoch 94 [8000/8883 (89.93%)]		Loss: 0.46425
Training Progress: 	Epoch 94 [8320/8883 (93.53%)]		Loss: 0.52668
Training Progress: 	Epoch 94 [8640/8883 (97.12%)]		Loss: 0.43755
	Train loss: 0.01180, Accuracy: 7169/8883 (80.00%)
	Validation loss: 0.00017, Accuracy: 1531/1692 (90.00%)
	Test loss: 0.00185, Accuracy: 540/1772 (30.00%)

Training Progress: 	Epoch 95 [0/8883 (0.00%)]		Loss: 0.44359
Training Progress: 	Epoch 95 [320/8883 (3.60%)]		Loss: 0.44988
Training Progress: 	Epoch 95 [640/8883 (7.19%)]		Loss: 0.33709
Training Progress: 	Epoch 95 [960/8883 (10.79%)]		Loss: 0.74123
Training Progress: 	Epoch 95 [1280/8883 (14.39%)]		Loss: 0.67060
Training Progress: 	Epoch 95 [1600/8883 (17.99%)]		Loss: 0.24508
Training Progress: 	Epoch 95 [1920/8883 (21.58%)]		Loss: 0.36990
Training Progress: 	Epoch 95 [2240/8883 (25.18%)]		Loss: 0.56880
Training Progress: 	Epoch 95 [2560/8883 (28.78%)]		Loss: 0.58056
Training Progress: 	Epoch 95 [2880/8883 (32.37%)]		Loss: 0.42866
Training Progress: 	Epoch 95 [3200/8883 (35.97%)]		Loss: 0.53791
Training Progress: 	Epoch 95 [3520/8883 (39.57%)]		Loss: 0.65763
Training Progress: 	Epoch 95 [3840/8883 (43.17%)]		Loss: 0.49634
Training Progress: 	Epoch 95 [4160/8883 (46.76%)]		Loss: 0.41595
Training Progress: 	Epoch 95 [4480/8883 (50.36%)]		Loss: 0.36831
Training Progress: 	Epoch 95 [4800/8883 (53.96%)]		Loss: 0.37055
Training Progress: 	Epoch 95 [5120/8883 (57.55%)]		Loss: 0.42032
Training Progress: 	Epoch 95 [5440/8883 (61.15%)]		Loss: 0.47261
Training Progress: 	Epoch 95 [5760/8883 (64.75%)]		Loss: 0.44403
Training Progress: 	Epoch 95 [6080/8883 (68.35%)]		Loss: 0.30852
Training Progress: 	Epoch 95 [6400/8883 (71.94%)]		Loss: 0.36702
Training Progress: 	Epoch 95 [6720/8883 (75.54%)]		Loss: 0.55175
Training Progress: 	Epoch 95 [7040/8883 (79.14%)]		Loss: 0.73018
Training Progress: 	Epoch 95 [7360/8883 (82.73%)]		Loss: 0.52281
Training Progress: 	Epoch 95 [7680/8883 (86.33%)]		Loss: 0.41298
Training Progress: 	Epoch 95 [8000/8883 (89.93%)]		Loss: 0.53422
Training Progress: 	Epoch 95 [8320/8883 (93.53%)]		Loss: 0.75562
Training Progress: 	Epoch 95 [8640/8883 (97.12%)]		Loss: 0.46573
	Train loss: 0.01290, Accuracy: 7040/8883 (79.00%)
	Validation loss: 0.00024, Accuracy: 1490/1692 (88.00%)
	Test loss: 0.00194, Accuracy: 572/1772 (32.00%)

Training Progress: 	Epoch 96 [0/8883 (0.00%)]		Loss: 0.54918
Training Progress: 	Epoch 96 [320/8883 (3.60%)]		Loss: 0.45855
Training Progress: 	Epoch 96 [640/8883 (7.19%)]		Loss: 0.49604
Training Progress: 	Epoch 96 [960/8883 (10.79%)]		Loss: 0.43742
Training Progress: 	Epoch 96 [1280/8883 (14.39%)]		Loss: 0.49541
Training Progress: 	Epoch 96 [1600/8883 (17.99%)]		Loss: 0.33564
Training Progress: 	Epoch 96 [1920/8883 (21.58%)]		Loss: 0.35621
Training Progress: 	Epoch 96 [2240/8883 (25.18%)]		Loss: 0.41077
Training Progress: 	Epoch 96 [2560/8883 (28.78%)]		Loss: 0.60558
Training Progress: 	Epoch 96 [2880/8883 (32.37%)]		Loss: 0.58724
Training Progress: 	Epoch 96 [3200/8883 (35.97%)]		Loss: 0.43817
Training Progress: 	Epoch 96 [3520/8883 (39.57%)]		Loss: 1.17598
Training Progress: 	Epoch 96 [3840/8883 (43.17%)]		Loss: 0.60180
Training Progress: 	Epoch 96 [4160/8883 (46.76%)]		Loss: 0.35188
Training Progress: 	Epoch 96 [4480/8883 (50.36%)]		Loss: 0.31370
Training Progress: 	Epoch 96 [4800/8883 (53.96%)]		Loss: 0.38154
Training Progress: 	Epoch 96 [5120/8883 (57.55%)]		Loss: 0.56435
Training Progress: 	Epoch 96 [5440/8883 (61.15%)]		Loss: 0.60117
Training Progress: 	Epoch 96 [5760/8883 (64.75%)]		Loss: 0.46350
Training Progress: 	Epoch 96 [6080/8883 (68.35%)]		Loss: 0.47607
Training Progress: 	Epoch 96 [6400/8883 (71.94%)]		Loss: 0.55881
Training Progress: 	Epoch 96 [6720/8883 (75.54%)]		Loss: 0.53950
Training Progress: 	Epoch 96 [7040/8883 (79.14%)]		Loss: 0.61868
Training Progress: 	Epoch 96 [7360/8883 (82.73%)]		Loss: 0.49618
Training Progress: 	Epoch 96 [7680/8883 (86.33%)]		Loss: 0.51480
Training Progress: 	Epoch 96 [8000/8883 (89.93%)]		Loss: 0.48936
Training Progress: 	Epoch 96 [8320/8883 (93.53%)]		Loss: 0.65399
Training Progress: 	Epoch 96 [8640/8883 (97.12%)]		Loss: 0.46574
	Train loss: 0.01199, Accuracy: 7148/8883 (80.00%)
	Validation loss: 0.00019, Accuracy: 1514/1692 (89.00%)
	Test loss: 0.00186, Accuracy: 547/1772 (30.00%)

Training Progress: 	Epoch 97 [0/8883 (0.00%)]		Loss: 0.53802
Training Progress: 	Epoch 97 [320/8883 (3.60%)]		Loss: 0.49319
Training Progress: 	Epoch 97 [640/8883 (7.19%)]		Loss: 0.40207
Training Progress: 	Epoch 97 [960/8883 (10.79%)]		Loss: 0.49138
Training Progress: 	Epoch 97 [1280/8883 (14.39%)]		Loss: 0.40833
Training Progress: 	Epoch 97 [1600/8883 (17.99%)]		Loss: 0.41907
Training Progress: 	Epoch 97 [1920/8883 (21.58%)]		Loss: 0.48393
Training Progress: 	Epoch 97 [2240/8883 (25.18%)]		Loss: 0.54292
Training Progress: 	Epoch 97 [2560/8883 (28.78%)]		Loss: 0.54373
Training Progress: 	Epoch 97 [2880/8883 (32.37%)]		Loss: 0.47988
Training Progress: 	Epoch 97 [3200/8883 (35.97%)]		Loss: 0.54352
Training Progress: 	Epoch 97 [3520/8883 (39.57%)]		Loss: 0.54534
Training Progress: 	Epoch 97 [3840/8883 (43.17%)]		Loss: 0.57222
Training Progress: 	Epoch 97 [4160/8883 (46.76%)]		Loss: 0.32756
Training Progress: 	Epoch 97 [4480/8883 (50.36%)]		Loss: 0.46456
Training Progress: 	Epoch 97 [4800/8883 (53.96%)]		Loss: 0.29476
Training Progress: 	Epoch 97 [5120/8883 (57.55%)]		Loss: 0.53147
Training Progress: 	Epoch 97 [5440/8883 (61.15%)]		Loss: 0.58199
Training Progress: 	Epoch 97 [5760/8883 (64.75%)]		Loss: 0.39321
Training Progress: 	Epoch 97 [6080/8883 (68.35%)]		Loss: 0.36952
Training Progress: 	Epoch 97 [6400/8883 (71.94%)]		Loss: 0.61513
Training Progress: 	Epoch 97 [6720/8883 (75.54%)]		Loss: 0.55561
Training Progress: 	Epoch 97 [7040/8883 (79.14%)]		Loss: 0.55047
Training Progress: 	Epoch 97 [7360/8883 (82.73%)]		Loss: 0.50598
Training Progress: 	Epoch 97 [7680/8883 (86.33%)]		Loss: 0.42053
Training Progress: 	Epoch 97 [8000/8883 (89.93%)]		Loss: 0.40575
Training Progress: 	Epoch 97 [8320/8883 (93.53%)]		Loss: 0.54527
Training Progress: 	Epoch 97 [8640/8883 (97.12%)]		Loss: 0.48666
	Train loss: 0.01262, Accuracy: 7087/8883 (79.00%)
	Validation loss: 0.00020, Accuracy: 1507/1692 (89.00%)
	Test loss: 0.00186, Accuracy: 526/1772 (29.00%)

Training Progress: 	Epoch 98 [0/8883 (0.00%)]		Loss: 0.64730
Training Progress: 	Epoch 98 [320/8883 (3.60%)]		Loss: 0.44264
Training Progress: 	Epoch 98 [640/8883 (7.19%)]		Loss: 0.32012
Training Progress: 	Epoch 98 [960/8883 (10.79%)]		Loss: 0.42719
Training Progress: 	Epoch 98 [1280/8883 (14.39%)]		Loss: 0.49163
Training Progress: 	Epoch 98 [1600/8883 (17.99%)]		Loss: 0.19973
Training Progress: 	Epoch 98 [1920/8883 (21.58%)]		Loss: 0.65309
Training Progress: 	Epoch 98 [2240/8883 (25.18%)]		Loss: 0.38977
Training Progress: 	Epoch 98 [2560/8883 (28.78%)]		Loss: 0.47033
Training Progress: 	Epoch 98 [2880/8883 (32.37%)]		Loss: 0.51944
Training Progress: 	Epoch 98 [3200/8883 (35.97%)]		Loss: 0.40368
Training Progress: 	Epoch 98 [3520/8883 (39.57%)]		Loss: 0.84071
Training Progress: 	Epoch 98 [3840/8883 (43.17%)]		Loss: 0.59548
Training Progress: 	Epoch 98 [4160/8883 (46.76%)]		Loss: 0.43256
Training Progress: 	Epoch 98 [4480/8883 (50.36%)]		Loss: 0.49424
Training Progress: 	Epoch 98 [4800/8883 (53.96%)]		Loss: 0.34638
Training Progress: 	Epoch 98 [5120/8883 (57.55%)]		Loss: 0.53004
Training Progress: 	Epoch 98 [5440/8883 (61.15%)]		Loss: 0.58657
Training Progress: 	Epoch 98 [5760/8883 (64.75%)]		Loss: 0.52748
Training Progress: 	Epoch 98 [6080/8883 (68.35%)]		Loss: 0.31406
Training Progress: 	Epoch 98 [6400/8883 (71.94%)]		Loss: 0.41011
Training Progress: 	Epoch 98 [6720/8883 (75.54%)]		Loss: 0.52513
Training Progress: 	Epoch 98 [7040/8883 (79.14%)]		Loss: 0.65205
Training Progress: 	Epoch 98 [7360/8883 (82.73%)]		Loss: 0.55448
Training Progress: 	Epoch 98 [7680/8883 (86.33%)]		Loss: 0.38551
Training Progress: 	Epoch 98 [8000/8883 (89.93%)]		Loss: 0.78856
Training Progress: 	Epoch 98 [8320/8883 (93.53%)]		Loss: 0.69405
Training Progress: 	Epoch 98 [8640/8883 (97.12%)]		Loss: 0.63106
	Train loss: 0.01222, Accuracy: 7128/8883 (80.00%)
	Validation loss: 0.00020, Accuracy: 1507/1692 (89.00%)
	Test loss: 0.00189, Accuracy: 507/1772 (28.00%)

Training Progress: 	Epoch 99 [0/8883 (0.00%)]		Loss: 0.53079
Training Progress: 	Epoch 99 [320/8883 (3.60%)]		Loss: 0.41264
Training Progress: 	Epoch 99 [640/8883 (7.19%)]		Loss: 0.21787
Training Progress: 	Epoch 99 [960/8883 (10.79%)]		Loss: 0.66778
Training Progress: 	Epoch 99 [1280/8883 (14.39%)]		Loss: 0.39074
Training Progress: 	Epoch 99 [1600/8883 (17.99%)]		Loss: 0.60672
Training Progress: 	Epoch 99 [1920/8883 (21.58%)]		Loss: 0.42335
Training Progress: 	Epoch 99 [2240/8883 (25.18%)]		Loss: 0.38284
Training Progress: 	Epoch 99 [2560/8883 (28.78%)]		Loss: 0.87911
Training Progress: 	Epoch 99 [2880/8883 (32.37%)]		Loss: 0.58526
Training Progress: 	Epoch 99 [3200/8883 (35.97%)]		Loss: 0.71252
Training Progress: 	Epoch 99 [3520/8883 (39.57%)]		Loss: 0.60023
Training Progress: 	Epoch 99 [3840/8883 (43.17%)]		Loss: 0.55345
Training Progress: 	Epoch 99 [4160/8883 (46.76%)]		Loss: 0.35983
Training Progress: 	Epoch 99 [4480/8883 (50.36%)]		Loss: 0.31606
Training Progress: 	Epoch 99 [4800/8883 (53.96%)]		Loss: 0.31579
Training Progress: 	Epoch 99 [5120/8883 (57.55%)]		Loss: 0.46234
Training Progress: 	Epoch 99 [5440/8883 (61.15%)]		Loss: 0.56899
Training Progress: 	Epoch 99 [5760/8883 (64.75%)]		Loss: 0.61984
Training Progress: 	Epoch 99 [6080/8883 (68.35%)]		Loss: 0.50584
Training Progress: 	Epoch 99 [6400/8883 (71.94%)]		Loss: 0.54424
Training Progress: 	Epoch 99 [6720/8883 (75.54%)]		Loss: 0.78682
Training Progress: 	Epoch 99 [7040/8883 (79.14%)]		Loss: 0.68649
Training Progress: 	Epoch 99 [7360/8883 (82.73%)]		Loss: 0.54064
Training Progress: 	Epoch 99 [7680/8883 (86.33%)]		Loss: 0.45871
Training Progress: 	Epoch 99 [8000/8883 (89.93%)]		Loss: 0.46523
Training Progress: 	Epoch 99 [8320/8883 (93.53%)]		Loss: 0.41133
Training Progress: 	Epoch 99 [8640/8883 (97.12%)]		Loss: 0.55486
	Train loss: 0.01215, Accuracy: 7133/8883 (80.00%)
	Validation loss: 0.00021, Accuracy: 1508/1692 (89.00%)
	Test loss: 0.00191, Accuracy: 553/1772 (31.00%)

Training Progress: 	Epoch 100 [0/8883 (0.00%)]		Loss: 0.40253
Training Progress: 	Epoch 100 [320/8883 (3.60%)]		Loss: 0.48576
Training Progress: 	Epoch 100 [640/8883 (7.19%)]		Loss: 0.44232
Training Progress: 	Epoch 100 [960/8883 (10.79%)]		Loss: 0.46440
Training Progress: 	Epoch 100 [1280/8883 (14.39%)]		Loss: 0.60962
Training Progress: 	Epoch 100 [1600/8883 (17.99%)]		Loss: 0.51566
Training Progress: 	Epoch 100 [1920/8883 (21.58%)]		Loss: 0.44068
Training Progress: 	Epoch 100 [2240/8883 (25.18%)]		Loss: 0.39413
Training Progress: 	Epoch 100 [2560/8883 (28.78%)]		Loss: 0.66650
Training Progress: 	Epoch 100 [2880/8883 (32.37%)]		Loss: 0.56403
Training Progress: 	Epoch 100 [3200/8883 (35.97%)]		Loss: 0.39368
Training Progress: 	Epoch 100 [3520/8883 (39.57%)]		Loss: 0.58327
Training Progress: 	Epoch 100 [3840/8883 (43.17%)]		Loss: 0.54501
Training Progress: 	Epoch 100 [4160/8883 (46.76%)]		Loss: 0.31091
Training Progress: 	Epoch 100 [4480/8883 (50.36%)]		Loss: 0.32323
Training Progress: 	Epoch 100 [4800/8883 (53.96%)]		Loss: 0.27965
Training Progress: 	Epoch 100 [5120/8883 (57.55%)]		Loss: 0.55348
Training Progress: 	Epoch 100 [5440/8883 (61.15%)]		Loss: 0.49019
Training Progress: 	Epoch 100 [5760/8883 (64.75%)]		Loss: 0.48660
Training Progress: 	Epoch 100 [6080/8883 (68.35%)]		Loss: 0.29398
Training Progress: 	Epoch 100 [6400/8883 (71.94%)]		Loss: 0.47375
Training Progress: 	Epoch 100 [6720/8883 (75.54%)]		Loss: 0.51455
Training Progress: 	Epoch 100 [7040/8883 (79.14%)]		Loss: 0.67799
Training Progress: 	Epoch 100 [7360/8883 (82.73%)]		Loss: 0.49282
Training Progress: 	Epoch 100 [7680/8883 (86.33%)]		Loss: 0.49785
Training Progress: 	Epoch 100 [8000/8883 (89.93%)]		Loss: 0.44614
Training Progress: 	Epoch 100 [8320/8883 (93.53%)]		Loss: 0.43390
Training Progress: 	Epoch 100 [8640/8883 (97.12%)]		Loss: 0.46836
	Train loss: 0.01211, Accuracy: 7138/8883 (80.00%)
	Validation loss: 0.00020, Accuracy: 1511/1692 (89.00%)
	Test loss: 0.00188, Accuracy: 525/1772 (29.00%)

Best validation accuracy:
0.9048463356973995
Best test accuracy:
0.327313769751693










